#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
event_comparison.py  (å…¨é¢æ”¹è‰¯ç‰ˆ)

ç›®çš„:
- åŒä¸€è©¦åˆã‚’æ‰±ã†è¤‡æ•°ã®é…ä¿¡è€…CSVã§ã€ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡ºâ†’ä¼¼ãƒˆãƒ”ãƒƒã‚¯çµ±åˆâ†’æ™‚ç³»åˆ—(%)
- ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãƒˆãƒ”ãƒƒã‚¯ã®æ™‚ç³»åˆ—ã‹ã‚‰ãƒ”ãƒ¼ã‚¯ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆè¤‡æ•°ï¼‰ã‚’æŠ½å‡º
- ã‚¹ãƒˆãƒªãƒ¼ãƒ é–“ã§ã€Œèªå½™é¡ä¼¼(Jaccard) Ã— æ™‚é–“ãšã‚Œã€æ¡ä»¶ã§å…±é€šã‚¤ãƒ™ãƒ³ãƒˆã¨ã—ã¦ç…§åˆ
- å„å…±é€šã‚¤ãƒ™ãƒ³ãƒˆã®é…ä¿¡è€…é–“ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹å·®ã‚’ Jensenâ€“Shannonè·é›¢ã§å®šé‡åŒ–
- ã™ã¹ã¦ã®CSVå‡ºåŠ›ã«å¯¾å¿œã™ã‚‹å¯è¦–åŒ–PNGã‚’è‡ªå‹•ç”Ÿæˆï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã€è·é›¢è¡Œåˆ—ã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€æ™‚ç³»åˆ—ï¼‰

ä½¿ã„æ–¹ä¾‹ (PowerShell):
  # ãƒ•ã‚©ãƒ«ãƒ€æŒ‡å®šï¼ˆæ¨å¥¨ï¼‰
  python event_comparison.py `
    --folder "data/football" `
    --pattern "*.csv" `
    --time-bins 300 `
    --peak-pad 1 `
    --n-events 5 `
    --topk 200 `
    --time-match-th 60 `
    --jaccard-th 0.5 `
    --word-match-th 0.4 `
    --save-json

  # ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã‚„æ—¥æœ¬èªãƒ‘ã‚¹ã¯PowerShellå¤‰æ•°çµŒç”±ãŒå®‰å…¨ï¼‰
  $files = Get-ChildItem -Path "data/football" -Filter *.csv | ForEach-Object { $_.FullName }
  python event_comparison.py --files $files --time-bins 300 --n-events 5 --save-json

å‡ºåŠ›:
- output/event_comparison_results.csv        â€¦ å„å…±é€šã‚¤ãƒ™ãƒ³ãƒˆã®è·é›¢çµ±è¨ˆ
- output/event_comparison_results.png        â€¦ ä¸Šã®è·é›¢è¡Œåˆ—ã®å¯è¦–åŒ–
- output/event_eventmap.csv                  â€¦ å…±é€šã‚¤ãƒ™ãƒ³ãƒˆÃ—é…ä¿¡è€…ã®ã€Œæœ‰ç„¡ã€ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— (çŸ©å½¢è¡¨)
- output/event_eventmap.png                  â€¦ â†‘ã®å¯è¦–åŒ–
- output/event_comments.json                 â€¦ ï¼ˆ--save-jsonæ™‚ï¼‰å„ã‚¤ãƒ™ãƒ³ãƒˆÃ—é…ä¿¡è€…ã®æŠ½å‡ºã‚³ãƒ¡ãƒ³ãƒˆ
- output/wordclouds/event_<EID>/WC_<basename>.png   â€¦ ã‚¤ãƒ™ãƒ³ãƒˆã”ã¨ã®ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
- output/timelines/<basename>_timeline.png         â€¦ å„é…ä¿¡è€…ã®Top-10(çµ±åˆ)æ™‚ç³»åˆ—
"""

import argparse
import os, re, json
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Optional

# ===== è¿½åŠ ãƒ©ã‚¤ãƒ–ãƒ©ãƒª =====
# è¨€èªæ¤œå‡ºãƒ©ã‚¤ãƒ–ãƒ©ãƒª langdetect ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ç’°å¢ƒã«å¯¾å¿œã™ã‚‹ãŸã‚ã€
# try-import ã‚’ç”¨ã„ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ç”¨æ„ã™ã‚‹ã€‚
try:
    from langdetect import detect, DetectorFactory  # type: ignore
    DetectorFactory.seed = 42
    _LANGDETECT_AVAILABLE = True
except ImportError:
    _LANGDETECT_AVAILABLE = False
    # ãƒ€ãƒŸãƒ¼é–¢æ•°ã‚’å®šç¾©
    def detect(text: str) -> str:
        return "unk"
    class DetectorFactory:
        seed = None


import numpy as np
import pandas as pd

# ===== Matplotlibï¼ˆæç”»&æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆï¼‰ =====
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from matplotlib import font_manager, rcParams
import unicodedata  # for converting emoji characters to names in charts

# æ—¥æœ¬èªã¨çµµæ–‡å­—ãƒ•ã‚©ãƒ³ãƒˆã®å€™è£œã‚’è¨­å®š
JP_FONT_CANDIDATES = [
    "Meiryo", "Yu Gothic", "Yu Gothic UI", "MS Gothic",
    "Hiragino Sans", "Hiragino Kaku Gothic ProN",
    "Noto Sans CJK JP", "IPAGothic",
]
EMOJI_FONT_CANDIDATES = [
    "Noto Color Emoji",   # Linux
    "Segoe UI Emoji",     # Windows
    "Apple Color Emoji",  # macOS
    "Twemoji Mozilla",    # Fallback
]

_available = {f.name for f in font_manager.fontManager.ttflist}
_used_jp_font = None
for _name in JP_FONT_CANDIDATES:
    if _name in _available:
        _used_jp_font = _name
        break

# å„ªå…ˆã™ã‚‹çµµæ–‡å­—ãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’å–å¾—
_emoji_font_path: Optional[str] = None
for _ename in EMOJI_FONT_CANDIDATES:
    try:
        _emoji_font_path = font_manager.findfont(_ename, fallback_to_default=False)
        # `findfont` may return a path even if font is not found; ensure name actually matches
        if _ename in _available:
            break
        # If not, still accept the found path
        break
    except Exception:
        continue
# è¿½åŠ : ã‚«ãƒ©ãƒ¼ãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ç›´æ¥NotoColorEmoji.ttfã‚’æŒ‡å®šã™ã‚‹
if not _emoji_font_path or ("Color" not in os.path.basename(_emoji_font_path)):
    possible_color_paths = [
        "/usr/share/fonts/truetype/noto/NotoColorEmoji.ttf",
        "/usr/share/fonts/truetype/noto/NotoColorEmoji-Regular.ttf",
        "/usr/share/fonts/truetype/noto/NotoColorEmoji-Regular.otf",
    ]
    for _p in possible_color_paths:
        if os.path.exists(_p):
            _emoji_font_path = _p
            break

# Set Matplotlib global font family as list: first Japanese font if available, then sans-serif
families: List[str] = []
if _used_jp_font:
    families.append(_used_jp_font)
# Do not set emoji font globally because color emojis may not render properly in all contexts; instead use per-plot
families.append("sans-serif")
rcParams["font.family"] = families
rcParams["axes.unicode_minus"] = False
print(f"[Matplotlib] Using fonts: {', '.join(families)} (emoji font path: {_emoji_font_path})")

# ---- Label helpers ----
def _abbr_stream_name(path: str, maxlen: int = 12) -> str:
    """Return a short alias from a CSV path (basename without extension, truncated)."""
    base = os.path.basename(path)
    name = base.replace('.csv', '')
    # common noise cleanup
    for noise in ["_chat_log", "chat_log", "_log", "log"]:
        name = name.replace(noise, "")
    return (name if len(name) <= maxlen else (name[:maxlen-1] + "â€¦"))

# ===== Topicé–¢é€£ =====
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from bertopic.representation import MaximalMarginalRelevance
from sklearn.feature_extraction.text import CountVectorizer
from umap import UMAP
from hdbscan import HDBSCAN

# ===== å‡ºåŠ›å…ˆ =====
OUT_DIR = "output"
os.makedirs(OUT_DIR, exist_ok=True)

# ===== ãã®ã»ã‹ =====
# åˆ©ç”¨ã™ã‚‹SentenceTransformerãƒ¢ãƒ‡ãƒ«ã€‚å¤šè¨€èªå¯¾å¿œæ€§èƒ½ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã¸å¤‰æ›´ã—ã€
# ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã®ç²¾åº¦å‘ä¸Šã‚’å›³ã‚‹
EMB_NAME = "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens"

# ç”¨èªæ­£è¦åŒ–ï¼ˆå„è¨€èªã®é¡ç¾©èªã‚’å…±é€šèªã«å¤‰æ›ï¼‰
# ã‚¹ãƒãƒ¼ãƒ„å®Ÿæ³ã§é »å‡ºã™ã‚‹å¾—ç‚¹ã‚„ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³ãªã©ã€å›½ã”ã¨ã®è¡¨ç¾ã‚’è‹±èªãƒ™ãƒ¼ã‚¹ã®ã‚­ãƒ¼ã«çµ±åˆã™ã‚‹ã€‚
# ã“ã“ã§ã¯ä¸»è¦ãªä¾‹ã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãŠã‚Šã€å¿…è¦ã«å¿œã˜ã¦æ‹¡å¼µã—ã¦ãã ã•ã„ã€‚
TERM_MAP: Dict[str, str] = {
    # ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³
    "ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³": "home_run",
    "æœ¬å¡æ‰“": "home_run",
    "ã»ãƒ¼ã‚€ã‚‰ã‚“": "home_run",
    "homerun": "home_run",
    "home run": "home_run",
    "homer": "home_run",
    "hr": "home_run",
    "jonron": "home_run",
    # å¾—ç‚¹ãƒ»ã‚´ãƒ¼ãƒ«
    "å¾—ç‚¹": "score",
    "å¾—ç‚¹æ™‚": "score",
    "ã‚´ãƒ¼ãƒ«": "score",
    "goal": "score",
    "score": "score",
    "puntuaciÃ³n": "score",
    "gol": "score",
    # ã‚¢ã‚¦ãƒˆ
    "ã‚¢ã‚¦ãƒˆ": "out",
    "out": "out",
    # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯
    "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯": "strike",
    "strike": "strike",
    # ãƒœãƒ¼ãƒ«
    "ãƒœãƒ¼ãƒ«": "ball",
    "ball": "ball",
    # ãƒ•ã‚©ã‚¢ãƒœãƒ¼ãƒ«
    "ãƒ•ã‚©ã‚¢ãƒœãƒ¼ãƒ«": "walk",
    "walk": "walk",
    # ãƒ•ã‚¡ã‚¦ãƒ«
    "ãƒ•ã‚¡ã‚¦ãƒ«": "foul",
    "foul": "foul",
    # é€€å ´
    "é€€å ´": "ejection",
    "é€€å ´å‡¦åˆ†": "ejection",
    "ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰": "ejection",
    "èµ¤ç´™": "ejection",
    "ejected": "ejection",
    "expulsado": "ejection",
    "expulsion": "ejection",
    "red card": "ejection",
    "tarjeta roja": "ejection",
    "cartÃ£o vermelho": "ejection",
    # å¯©åˆ¤
    "å¯©åˆ¤": "umpire",
    "umpire": "umpire",
    # æ€ªæˆ‘
    "æ€ªæˆ‘": "injury",
    "injury": "injury",
    "hurt": "injury",
    # å¿œæ´ãƒ»æ­“å£°
    "å¿œæ´": "cheer",
    "cheer": "cheer",
    "boo": "boo",
    "æ‹æ‰‹": "clap",
    # ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰ï¼ˆè­¦å‘Šï¼‰
    "ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰": "warning",
    "yellow card": "warning",
    "è­¦å‘Š": "warning",
    "tarjeta amarilla": "warning",
    "cartÃ£o amarelo": "warning",
    "amarilla": "warning",
    # äº¤ä»£ãƒ»é¸æ‰‹äº¤ä»£
    "äº¤ä»£": "substitution",
    "é¸æ‰‹äº¤ä»£": "substitution",
    "substitution": "substitution",
    "substituiÃ§Ã£o": "substitution",
    "æ›¿ãˆ": "substitution",
    # ã‚´ãƒ¼ãƒ«ã«é–¢ã™ã‚‹è¿½åŠ è¡¨ç¾
    "ã‚´ãƒ¼ãƒ«ï¼": "score",
    "ã‚´ãƒ¼ãƒ«ã ": "score",
    "goooooal": "score",
    "goool": "score",
    "ã‚´ãƒ¼ãƒ«": "score",
    "goal": "score",
    "gol": "score",
    "golaÃ§o": "score",
    "gole": "score",
    "goal!": "score",
    "goooool": "score",
    # å¾—ç‚¹ã«é–¢ã™ã‚‹åˆ¥è¡¨ç¾
    "å¾—ç‚¹!": "score",
    "scored": "score",
    "goal!": "score",
    "goals": "score",
    # ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆPKï¼‰
    "ãƒšãƒŠãƒ«ãƒ†ã‚£": "penalty",
    "penal": "penalty",
    "penalty": "penalty",
    "pÃªnalti": "penalty",
    "penalti": "penalty",
    "pk": "penalty",
    # ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯
    "ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯": "free_kick",
    "free kick": "free_kick",
    "tiro libre": "free_kick",
    "tiro livre": "free_kick",
    "tiro de falta": "free_kick",
    # ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯
    "ã‚³ãƒ¼ãƒŠãƒ¼": "corner",
    "ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯": "corner",
    "corner": "corner",
    "escanteio": "corner",
    "esquina": "corner",
    # ã‚ªãƒ•ã‚µã‚¤ãƒ‰
    "ã‚ªãƒ•ã‚µã‚¤ãƒ‰": "offside",
    "offside": "offside",
    "impedimento": "offside",
    "fuera de juego": "offside",
    # ãƒ•ã‚¡ã‚¦ãƒ«
    "ãƒ•ã‚¡ã‚¦ãƒ«": "foul",
    "foul": "foul",
    "falta": "foul",
    # å¯©åˆ¤ï¼ãƒ¬ãƒ•ã‚§ãƒªãƒ¼
    "å¯©åˆ¤": "umpire",
    "ãƒ¬ãƒ•ã‚§ãƒªãƒ¼": "umpire",
    "referee": "umpire",
    "Ã¡rbitro": "umpire",
    # ãã®ä»–
    # ...
}

def normalize_term(word: str) -> str:
    """ç”¨èªãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã‚’ç”¨ã„ã¦å˜èªã‚’æ­£è¦åŒ–ã™ã‚‹ã€‚å°æ–‡å­—åŒ–ã—ã¦ä¸€è‡´ã•ã›ã‚‹ã€‚"""
    if not isinstance(word, str):
        return word
    w = word.lower()
    # è¾æ›¸å†…ã«ã‚­ãƒ¼ãŒã‚ã‚Œã°ãã®å€¤ã‚’è¿”ã—ã€ãªã‘ã‚Œã°å…ƒã®å°æ–‡å­—ã‚’è¿”ã™
    return TERM_MAP.get(w, w)

# -------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ / å‰å‡¦ç†
# -------------------------
def segment_text(text: str) -> str:
    """
    æ—¥æœ¬èªã¨ãã®ä»–ã®æ–‡å­—ãŒæ··åœ¨ã™ã‚‹æ–‡å­—åˆ—ã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥ã™ã‚‹ã€‚

    æ—¥æœ¬èªã«ã¯é€šå¸¸ã‚¹ãƒšãƒ¼ã‚¹ãŒå…¥ã£ã¦ã„ãªã„ãŸã‚ã€
    ã‚³ãƒ¡ãƒ³ãƒˆå…¨ä½“ãŒ1ãƒˆãƒ¼ã‚¯ãƒ³ã«ãªã£ã¦ã—ã¾ã†å•é¡Œã‚’é¿ã‘ã‚‹ãŸã‚ã®ç°¡æ˜“æ‰‹æ³•ã§ã™ã€‚
    æ¼¢å­—ãƒ»ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãŒé€£ç¶šã™ã‚‹éƒ¨åˆ†ã¨ã€ãã‚Œä»¥å¤–ï¼ˆãƒ©ãƒ†ãƒ³æ–‡å­—ã‚„æ•°å­—ãªã©ï¼‰
    ã®å¢ƒç•Œã§ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥ã—ã¾ã™ã€‚
    ã“ã‚Œã¯å³å¯†ãªå½¢æ…‹ç´ è§£æã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²ã‚’æ”¹å–„ã—ã¾ã™ã€‚
    """
    result = []
    prev_jp: Optional[bool] = None
    for ch in text:
        # æ—¥æœ¬èªï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ï¼‰ã¨ãã‚Œä»¥å¤–ã®å¢ƒç•Œã‚’æ¤œå‡º
        is_jp = (
            ('\u3040' <= ch <= '\u30ff')  # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠ
            or ('\u4e00' <= ch <= '\u9fff')  # æ¼¢å­—
        )
        if prev_jp is None:
            result.append(ch)
        else:
            if is_jp != prev_jp:
                # å¢ƒç•Œã§ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥
                result.append(' ')
            result.append(ch)
        prev_jp = is_jp
    return ''.join(result)
def preprocess_text(text: str) -> str:
    """
    ã‚³ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ã‚’è¡Œã†ã€‚

    - URL, ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³, ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’é™¤å»
    - çµµæ–‡å­—ã®ä¸€éƒ¨ã‚’è‹±å˜èªã«ç½®ãæ›ãˆ
    - è¨˜å·ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›ã—ã€è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã«
    - æ—¥æœ¬èªã¨ãã®ä»–ã®æ–‡å­—ã®é–“ã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥ï¼ˆç°¡æ˜“åˆ†å‰²ï¼‰
    - å°æ–‡å­—åŒ–
    """
    if not isinstance(text, str):
        return ""
    # URLã‚„ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãªã©ã®é™¤å»
    text = re.sub(r"http\S+|www\.\S+", " ", text)
    text = re.sub(r"@\w+", " ", text)
    text = re.sub(r"#\w+", " ", text)
    # ä¸€éƒ¨çµµæ–‡å­—ã‚’å˜èªã«å¤‰æ›
    text = (
        text.replace("ğŸ˜‚", " laugh ")
            .replace("ğŸ˜­", " cry ")
            .replace("ğŸ‘", " clap ")
            .replace("ğŸ”¥", " fire ")
    )
    # è¨˜å·ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«
    text = re.sub(r"[^\w\s\u4E00-\u9FFF\u3040-\u30FF\uAC00-\uD7AF]", " ", text)
    # æ—¥æœ¬èªã¨ãã®ä»–ã®æ–‡å­—åˆ—å¢ƒç•Œã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥
    text = segment_text(text)
    # è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’ä¸€ã¤ã«ã€å‰å¾Œã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤ã—å°æ–‡å­—åŒ–
    text = re.sub(r"\s+", " ", text).strip().lower()
    return text

# -------------------------
# è¨€èªãƒ»çµµæ–‡å­—é–¢é€£ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -------------------------
def detect_lang_safe(text: str) -> str:
    """langdetect ã®ä¾‹å¤–ã‚’æ¡ã‚Šã¤ã¶ã—ã¦è¨€èªã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™"""
    try:
        return detect(text)
    except Exception:
        return "unk"

def is_emoji(char: str) -> bool:
    """ç°¡æ˜“çš„ãªçµµæ–‡å­—åˆ¤å®šã€‚Unicode ã®çµµæ–‡å­—ãƒ–ãƒ­ãƒƒã‚¯ã«å±ã™ã‚‹ã‹ã§åˆ¤å®š"""
    cp = ord(char)
    # çµµæ–‡å­—ã¯å¤šæ§˜ã ãŒã€ä»¥ä¸‹ã®ç¯„å›²ã‚’ã‚«ãƒãƒ¼ã™ã‚‹
    return (
        0x1F600 <= cp <= 0x1F64F  # emoticons
        or 0x1F300 <= cp <= 0x1F5FF  # symbols & pictographs
        or 0x1F680 <= cp <= 0x1F6FF  # transport & map symbols
        or 0x2600 <= cp <= 0x26FF    # miscellaneous symbols
        or 0x2700 <= cp <= 0x27BF    # dingbats
        or 0x1F1E6 <= cp <= 0x1F1FF  # flags
    )

def compute_language_distribution(texts: List[str]) -> Dict[str, int]:
    """ã‚³ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã‹ã‚‰è¨€èªåˆ¥ä»¶æ•°ã‚’æ•°ãˆã‚‹"""
    counter = defaultdict(int)
    for txt in texts:
        lang = detect_lang_safe(txt)
        counter[lang] += 1
    return counter

def compute_emoji_ratio(texts: List[str]) -> float:
    """ã‚³ãƒ¡ãƒ³ãƒˆå†…ã®çµµæ–‡å­—æ¯”ç‡ã‚’è¨ˆç®—ï¼ˆæ–‡å­—æ•°ã§ã¯ãªãçµµæ–‡å­—æ•°/å˜èªæ•°ï¼‰"""
    total_tokens = 0
    emoji_count = 0
    for txt in texts:
        tokens = txt.split()
        total_tokens += len(tokens)
        for ch in txt:
            if is_emoji(ch):
                emoji_count += 1
    return float(emoji_count) / max(total_tokens, 1)

def js_distance_distribution(counter_a: Dict[str, int], counter_b: Dict[str, int]) -> float:
    """è¨€èªåˆ†å¸ƒã‚„ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã® Jensenâ€“Shannon è·é›¢ã‚’è¨ˆç®—"""
    keys = set(counter_a.keys()) | set(counter_b.keys())
    pa = np.array([counter_a.get(k, 0) for k in keys], dtype=float)
    pb = np.array([counter_b.get(k, 0) for k in keys], dtype=float)
    return js_distance(pa, pb)

# -------------------------
# æ„Ÿæƒ…ãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆç°¡æ˜“ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰
# -------------------------
EN_POS = set([
    "good","great","amazing","awesome","nice","love","wow","goal","score","win","gg","goat","clutch",
])
EN_NEG = set([
    "bad","terrible","worst","lose","miss","wtf","hate","boo","noob","trash","rigged",
])
JA_POS = set(["ã™ã”ã„","æœ€é«˜","ä¸Šæ‰‹ã„","ã†ã¾ã„","å‹ã¡","å‹ã£ãŸ","ãƒŠã‚¤ã‚¹","ç¥","ã‚„ã£ãŸ","ã„ã„ã­","è‰","GG"])
JA_NEG = set(["æœ€æ‚ª","ã²ã©ã„","ä¸‹æ‰‹","è² ã‘","è² ã‘ãŸ","ãƒ€ãƒ¡","å«Œã„","ãã","ã‚¯ã‚½","ãƒ–ãƒ¼ã‚¤ãƒ³ã‚°"])

def _count_emoji(text: str) -> int:
    return sum(1 for ch in text if is_emoji(ch))

def compute_sentiment_metrics(texts: List[str]) -> Dict[str, float]:
    """æ¥µæ€§/è¦šé†’åº¦ã®ç°¡æ˜“æŒ‡æ¨™ã‚’è¿”ã™ã€‚
    polarity: [-1,1]ï¼ˆãƒã‚¸-ãƒã‚¬ï¼‰
    pos_ratio/neg_ratio: èªå½™æ¯”ç‡
    arousal: æ„Ÿæƒ…å–šèµ·ã®ä»£ç†ï¼ˆ! ã¨ çµµæ–‡å­—å¯†åº¦ï¼‰
    """
    pos = neg = 0
    tokens_total = 0
    exclam = 0
    emoji_ct = 0
    for txt in texts:
        if not isinstance(txt, str):
            continue
        t = txt.lower()
        tokens = t.split()
        tokens_total += len(tokens)
        exclam += t.count("!")
        emoji_ct += _count_emoji(txt)
        # ç°¡æ˜“æ¥µæ€§
        for w in tokens:
            if w in EN_POS:
                pos += 1
            if w in EN_NEG:
                neg += 1
        # æ—¥æœ¬èªã¯ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§ãªã„ã“ã¨ãŒå¤šã„ã®ã§æ–‡å­—åˆ—åŒ…å«ã§åˆ¤å®š
        for w in JA_POS:
            if w in txt:
                pos += 1
        for w in JA_NEG:
            if w in txt:
                neg += 1
    s_total = max(pos + neg, 1)
    polarity = float((pos - neg) / s_total)
    pos_ratio = float(pos / max(tokens_total, 1))
    neg_ratio = float(neg / max(tokens_total, 1))
    arousal = float((exclam + emoji_ct) / max(tokens_total, 1))
    return {
        "polarity": polarity,
        "pos_ratio": pos_ratio,
        "neg_ratio": neg_ratio,
        "arousal": arousal,
    }

def compute_style_profile(texts: List[str]) -> Dict[str, float]:
    letters = 0
    upper = 0
    tokens = 0
    uniq = set()
    exclam = ques = 0
    urls = mentions = 0
    for txt in texts:
        if not isinstance(txt, str):
            continue
        t = txt
        tokens_list = t.split()
        tokens += len(tokens_list)
        for w in tokens_list:
            uniq.add(w)
            if w.startswith("http://") or w.startswith("https://"):
                urls += 1
            if w.startswith("@"):
                mentions += 1
        exclam += t.count("!")
        ques += t.count("?")
        letters += sum(1 for ch in t if ch.isalpha())
        upper += sum(1 for ch in t if ch.isupper())
    avg_len = float(tokens / max(len(texts), 1))
    unique_ratio = float(len(uniq) / max(tokens, 1))
    upper_ratio = float(upper / max(letters, 1))
    exclam_ratio = float(exclam / max(tokens, 1))
    ques_ratio = float(ques / max(tokens, 1))
    url_ratio = float(urls / max(tokens, 1))
    mention_ratio = float(mentions / max(tokens, 1))
    return {
        "avg_len": avg_len,
        "unique_ratio": unique_ratio,
        "upper_ratio": upper_ratio,
        "exclam_ratio": exclam_ratio,
        "ques_ratio": ques_ratio,
        "url_ratio": url_ratio,
        "mention_ratio": mention_ratio,
    }

def style_distance(p: Dict[str, float], q: Dict[str, float]) -> float:
    """ã‚¹ã‚¿ã‚¤ãƒ«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é–“ã®å¹³å‡çµ¶å¯¾å·®"""
    keys = sorted(set(p.keys()) & set(q.keys()))
    if not keys:
        return 0.0
    diffs = [abs(float(p[k]) - float(q[k])) for k in keys]
    return float(np.mean(diffs))

def read_csv_any(path: str) -> pd.DataFrame:
    encodings = ["utf-8", "utf-8-sig", "cp932", "iso-8859-1"]
    for enc in encodings:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            pass
    return pd.read_csv(path, engine="python", on_bad_lines="skip")

def parse_country_from_filename(path: str) -> str:
    name = os.path.basename(path)
    m = re.findall(r"(japan|japanese|jpn|india|indian|dominican|usa|korea|korean|mexico|taiwan|china|chinese|france)", name.lower())
    if m:
        return m[-1].capitalize()
    if re.search(r"[\u3040-\u30ff\u4e00-\u9faf]", name):
        return "Japan"
    return "Unknown"

# -------------------------
# ãƒˆãƒ”ãƒƒã‚¯çµ±åˆãƒ»æ™‚ç³»åˆ—ãƒ»ãƒ”ãƒ¼ã‚¯
# -------------------------
def build_topic_model(embedding_model: SentenceTransformer) -> BERTopic:
    # ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã®ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
    # CountVectorizer ã®ç‰¹å¾´æ•°ã‚’å¢—ã‚„ã—ã€å˜ä¸€å‡ºç¾èªã‚‚å¯¾è±¡ã«å«ã‚ã‚‹
    vectorizer_model = CountVectorizer(token_pattern=r"(?u)\b\w+\b", max_features=6000, min_df=1)
    # UMAP ã®æ¬¡å…ƒæ•°ã¨è¿‘å‚æ•°ã‚’å¢—ã‚„ã—ã€é«˜æ¬¡å…ƒåŸ‹ã‚è¾¼ã¿ã‚’ã‚ˆã‚Šè©³ç´°ã«è¡¨ç¾ã™ã‚‹
    umap_model = UMAP(n_components=10, n_neighbors=30, min_dist=0.00, metric="cosine", random_state=42)
    # HDBSCAN ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚ºã¨ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å°ã•ãè¨­å®šã—ã€å°ã•ãªã‚¤ãƒ™ãƒ³ãƒˆã‚‚æ¤œå‡ºã—ã‚„ã™ãã™ã‚‹
    hdbscan_model = HDBSCAN(min_cluster_size=10, min_samples=2, metric="euclidean",
                            cluster_selection_method="eom", prediction_data=True, core_dist_n_jobs=1)
    representation_model = MaximalMarginalRelevance(diversity=0.5)
    # ã‚¹ãƒãƒ¼ãƒ„å®Ÿæ³ã«é–¢é€£ã™ã‚‹ã‚ˆã‚Šå¤šæ§˜ãªç¨®ãƒˆãƒ”ãƒƒã‚¯ã‚’å«ã‚ã‚‹
    seed_topic_list = [
        ["pitch", "strike", "ball", "fastball", "slider", "pitching"],
        ["bat", "homer", "home run", "slugger", "batting"],
        ["defense", "catch", "outfield", "infield", "double play"],
        ["umpire", "referee", "call", "review", "challenge"],
        ["injury", "hurt", "rehab", "out"],
        ["weather", "rain", "delay"],
        ["cheer", "chant", "song", "boo", "applause", "clap", "support"],
        ["strategy", "tactics", "lineup", "substitution", "change"],
        # ã‚µãƒƒã‚«ãƒ¼ãƒ»é‡çƒç­‰å…±é€š: å¾—ç‚¹ã«é–¢ã™ã‚‹ç¨®
        ["goal", "score", "goalkeeper", "penalty", "shoot", "free kick", "corner", "offside"],
        # ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰ãƒ»ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰ãƒ»è­¦å‘Š
        ["yellow card", "warning", "foul", "penalty", "fine"],
        ["red card", "ejection", "sent off", "expulsion"],
        # äº¤ä»£ãƒ»é¸æ‰‹äº¤ä»£
        ["substitution", "sub", "change player", "replace", "äº¤ä»£"],
    ]
    return BERTopic(
        embedding_model=embedding_model,
        vectorizer_model=vectorizer_model,
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        representation_model=representation_model,
        calculate_probabilities=False,
        seed_topic_list=seed_topic_list,
        # ã‚ˆã‚Šå°ã•ãªãƒˆãƒ”ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’è¨±å®¹ã™ã‚‹ã“ã¨ã§ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºã‚’ç´°åˆ†åŒ–
        min_topic_size=5,
        nr_topics=None,
        verbose=False,
    )

def merge_topics(words_by_tid: Dict[int, List[Tuple[str, float]]], threshold: float) -> List[List[int]]:
    tids = [t for t in words_by_tid.keys() if t != -1]
    # å„ãƒˆãƒ”ãƒƒã‚¯ã®ä¸Šä½èªã‚»ãƒƒãƒˆã‚’æ­£è¦åŒ–ï¼ˆç”¨èªãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰ã—ã¦ã‹ã‚‰æ¯”è¼ƒ
    sets = {}
    for t in tids:
        raw_words = [w for w, _ in words_by_tid[t][:10] if isinstance(w, str) and w.strip()]
        normalized = {normalize_term(w) for w in raw_words}
        sets[t] = normalized
    parent: Dict[int, int] = {t: t for t in tids}
    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x
    def union(x, y):
        rx, ry = find(x), find(y)
        if rx != ry: parent[ry] = rx
    for i, ti in enumerate(tids):
        for tj in tids[i+1:]:
            sa, sb = sets[ti], sets[tj]
            if not sa and not sb: 
                continue
            jac = len(sa & sb) / (len(sa | sb) + 1e-12)
            if jac >= threshold: union(ti, tj)
    groups = defaultdict(list)
    for t in tids: groups[find(t)].append(t)
    return list(groups.values())

def compute_topics_over_time(topic_model: BERTopic, docs: List[str], topics: List[int], timestamps: List[pd.Timestamp], nr_bins: int) -> pd.DataFrame:
    return topic_model.topics_over_time(docs=docs, topics=topics, timestamps=timestamps, nr_bins=nr_bins, datetime_format=None)

def build_relative_time_bins(timestamps: pd.Series, nr_bins: int) -> pd.IntervalIndex:
    tmin, tmax = timestamps.min(), timestamps.max()
    edges = pd.date_range(start=tmin, end=tmax, periods=nr_bins + 1)
    return pd.IntervalIndex.from_breaks(edges, closed="left")

def smooth_series(y: np.ndarray, k: int = 3) -> np.ndarray:
    """ç§»å‹•å¹³å‡ã§å¹³æ»‘åŒ–ï¼ˆç«¯ã¯åå°„paddingï¼‰"""
    if k <= 1 or len(y) == 0:
        return y
    pad = k // 2
    ypad = np.pad(y, (pad, pad), mode="reflect")
    ker = np.ones(k) / k
    return np.convolve(ypad, ker, mode="valid")

def local_peaks(y: np.ndarray, n_keep: int = 3) -> List[int]:
    """å˜ç´”ãªå±€æ‰€æœ€å¤§ï¼ˆéš£ã‚ˆã‚Šå¤§ãã„ or åŒç­‰ï¼‰ã‚’æŠ½å‡ºã—ã¦ä¸Šä½n_keep"""
    if len(y) == 0:
        return []
    idxs = []
    for i in range(len(y)):
        l = y[i-1] if i-1 >= 0 else -np.inf
        r = y[i+1] if i+1 < len(y) else -np.inf
        if y[i] >= l and y[i] >= r:
            idxs.append(i)
    # å¼·ã„ãƒ”ãƒ¼ã‚¯é †
    idxs.sort(key=lambda i: y[i], reverse=True)
    return idxs[:n_keep]

# -------------------------
# ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
# -------------------------
class StreamData:
    def __init__(self, file_path: str, country: str, df_valid: pd.DataFrame,
                 topics_valid: List[int], groups: List[List[int]],
                 gid_label: Dict[int, str], group_timeseries: pd.DataFrame,
                 nr_bins: int, group_top_words: Dict[int, List[str]]):
        self.file_path = file_path
        self.country = country
        self.df_valid = df_valid
        self.topics_valid = topics_valid
        self.groups = groups
        self.gid_label = gid_label
        self.group_timeseries = group_timeseries
        self.nr_bins = nr_bins
        self.group_top_words = group_top_words  # {group_id: [str,...]}
        # è¨€èªåˆ—ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ä¿æŒ
        self.languages = df_valid.get("lang") if "lang" in df_valid.columns else None

# -------------------------
# ã‚¹ãƒˆãƒªãƒ¼ãƒ 1æœ¬ã®å‡¦ç†
# -------------------------
def process_stream(csv_file: str, embedding_model: SentenceTransformer,
                   jaccard_th: float, nr_bins: int, topk_plot: int = 10) -> Optional[StreamData]:
    df = read_csv_any(csv_file)
    if df.empty or "message" not in df.columns:
        print(f"Skipping {csv_file}: no message column")
        return None

    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— æ­£è¦åŒ–ï¼ˆTZæ··åœ¨å¯¾ç­–ï¼šã™ã¹ã¦tz-naiveã¸ï¼‰
    if "timestamp" in df.columns and not df["timestamp"].isnull().all():
        ts = pd.to_datetime(df["timestamp"], errors="coerce", utc=True)
        df["timestamp"] = ts.dt.tz_localize(None)
        df = df.dropna(subset=["timestamp"]).copy()
    else:
        df["timestamp"] = pd.date_range(start="2024-01-01", periods=len(df), freq="5S")

    # å‰å‡¦ç†
    df = df.dropna(subset=["message"]).copy()
    df["message_clean"] = df["message"].astype(str).apply(preprocess_text)
    df = df[df["message_clean"].str.len() > 0].copy()
    if df.empty:
        print(f"Skipping {csv_file}: no usable comments")
        return None
    # è¨€èªæ¤œå‡ºï¼ˆå¾Œã§ã‚¹ã‚¿ã‚¤ãƒ«æ¯”è¼ƒã«åˆ©ç”¨ï¼‰
    df["lang"] = df["message_clean"].apply(detect_lang_safe)
    texts = df["message_clean"].tolist()

    # åŸ‹ã‚è¾¼ã¿ & BERTopic
    emb = embedding_model.encode(texts, batch_size=64, show_progress_bar=False, normalize_embeddings=True)
    topic_model = build_topic_model(embedding_model)
    topics, _ = topic_model.fit_transform(texts, embeddings=emb)

    valid_idx = [i for i, t in enumerate(topics) if t != -1]
    if len(valid_idx) < 10:
        print(f"Skipping {csv_file}: too few valid topics")
        return None
    df_valid = df.iloc[valid_idx].reset_index(drop=True)
    topics_valid = [topics[i] for i in valid_idx]

    # ä¸Šä½èª
    topic_info = topic_model.get_topic_info()
    valid_tids = sorted([int(t) for t in topic_info["Topic"].tolist() if int(t) != -1])
    words_by_tid: Dict[int, List[Tuple[str, float]]] = {}
    for tid in valid_tids:
        items = topic_model.get_topic(tid) or []
        items = [(str(w), float(s)) for w, s in items if isinstance(w, str) and str(w).strip()]
        words_by_tid[tid] = items

    # ä¼¼ãƒˆãƒ”ãƒƒã‚¯çµ±åˆ
    groups = merge_topics(words_by_tid, threshold=jaccard_th)
    gid_label, group_top_words = {}, {}
    for gid, members in enumerate(groups):
        # ãƒˆãƒ”ãƒƒã‚¯çµ±åˆå¾Œã®ä»£è¡¨èªã‚’ä½œæˆï¼šé¡ç¾©èªãƒãƒƒãƒ”ãƒ³ã‚°ã§æ­£è¦åŒ–ã—ã€ä¸Šä½ã‚¹ã‚³ã‚¢é †ã«ã‚«ã‚¦ãƒ³ãƒˆ
        counter = Counter()
        for t in members:
            for w, s in words_by_tid.get(t, [])[:10]:
                if isinstance(w, str) and w.strip():
                    norm = normalize_term(w)
                    counter[norm] += float(s)
        tops = [w for w, _ in counter.most_common(4)]
        group_top_words[gid] = tops
        gid_label[gid] = "ãƒ»".join(tops) if tops else f"group_{gid}"

    # æ™‚ç³»åˆ—ï¼ˆçµ±åˆï¼‰
    tot = compute_topics_over_time(topic_model, df_valid["message_clean"].tolist(),
                                   topics_valid, df_valid["timestamp"].tolist(), nr_bins)
    # raw topic -> group
    raw2g = {}
    for gid, members in enumerate(groups):
        for t in members: raw2g[t] = gid
    tot["Group"] = tot["Topic"].map(raw2g).astype(int)

    sums = tot.groupby("Timestamp")["Frequency"].sum().rename("total")
    df_g = (tot.groupby(["Group", "Timestamp"], as_index=False)["Frequency"].sum()
              .merge(sums, left_on="Timestamp", right_index=True, how="left"))
    df_g["Percentage"] = 100.0 * df_g["Frequency"] / df_g["total"].clip(lower=1)
    df_g = df_g.drop(columns=["total"])

    # å¯è¦–åŒ–: å„é…ä¿¡è€…ã®Top-10æ™‚ç³»åˆ—
    plot_top_groups(df_g, gid_label,
                    out_png=os.path.join(OUT_DIR, "timelines", f"{os.path.basename(csv_file).replace('.csv','')}_timeline.png"),
                    title=f"Topics Over Time (Top-{topk_plot}) : {os.path.basename(csv_file)} [{parse_country_from_filename(csv_file)}]",
                    top_k=topk_plot)

    return StreamData(
        file_path=os.path.basename(csv_file),
        country=parse_country_from_filename(csv_file),
        df_valid=df_valid,
        topics_valid=topics_valid,
        groups=groups,
        gid_label=gid_label,
        group_timeseries=df_g,
        nr_bins=nr_bins,
        group_top_words=group_top_words,
    )

def plot_top_groups(df_g: pd.DataFrame, labels: Dict[int, str], out_png: str, title: str, top_k: int = 10):
    os.makedirs(os.path.dirname(out_png), exist_ok=True)
    order = (df_g.groupby("Group")["Frequency"].sum().sort_values(ascending=False))
    top_groups = order.index.tolist()[:top_k]
    plt.figure(figsize=(12,6))
    for gid in top_groups:
        d = df_g[df_g["Group"] == gid].sort_values("Timestamp")
        if d.empty: continue
        label = labels.get(gid, f"G{gid}")
        if len(label) > 40: label = label[:37] + "..."
        plt.plot(d["Timestamp"], d["Percentage"], marker=".", linewidth=1.2, label=label)
    plt.title(title); plt.xlabel("Time"); plt.ylabel("Percentage of comments")
    plt.legend(ncol=2, fontsize=9, frameon=False); plt.xticks(rotation=45)
    plt.tight_layout(); plt.savefig(out_png, dpi=220); plt.close()
    print(f"Saved timeline: {out_png}")

# -------------------------
# ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡ºãƒ»ç…§åˆãƒ»ãƒ†ã‚­ã‚¹ãƒˆè·é›¢
# -------------------------
def detect_events(stream: StreamData, n_events: int = 5, focus_top: Optional[int] = None) -> List[Dict[str, object]]:
    """
    å„ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆãƒˆãƒ”ãƒƒã‚¯çµ±åˆï¼‰ã®æ™‚ç³»åˆ—ã‹ã‚‰ãƒ”ãƒ¼ã‚¯ã‚¤ãƒ™ãƒ³ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚

    Parameters
    ----------
    stream : StreamData
        1ã¤ã®é…ä¿¡ã®è§£æçµæœã€‚
    n_events : int
        å„ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æŠ½å‡ºã™ã‚‹ãƒ”ãƒ¼ã‚¯ã®æœ€å¤§æ•°ã€‚
    focus_top : int or None
        ãƒˆãƒ”ãƒƒã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç·å‡ºç¾å›æ•°ã®å¤šã„é †ã«é™å®šã™ã‚‹æ•°ã€‚
        ä¾‹: 10 ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å¤šã„ä¸Šä½10ã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿ã‹ã‚‰ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’è¡Œã†ã€‚
        None ã®å ´åˆã¯ã™ã¹ã¦ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å¯¾è±¡ã¨ã™ã‚‹ã€‚

    Returns
    -------
    List[Dict[str, object]]
        æ¤œå‡ºã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆã€‚
    """
    events: List[Dict[str, object]] = []
    # å¯¾è±¡ã‚°ãƒ«ãƒ¼ãƒ—ã®æ±ºå®š
    if focus_top is not None and focus_top > 0:
        sums = stream.group_timeseries.groupby("Group")["Frequency"].sum()
        groups_to_use = sums.sort_values(ascending=False).head(focus_top).index.tolist()
    else:
        groups_to_use = sorted(stream.group_timeseries["Group"].unique())
    # Binning å†ç¾
    bins = build_relative_time_bins(stream.df_valid["timestamp"], stream.nr_bins)
    for gid in groups_to_use:
        gdf = stream.group_timeseries[stream.group_timeseries["Group"] == gid]
        if gdf.empty:
            continue
        # Bin assignment
        bin_edges = list(zip(bins.left, bins.right))
        counts = np.zeros(len(bins), dtype=float)
        for _, r in gdf.iterrows():
            ts = r["Timestamp"]
            b = None
            for bi, (lft, rgt) in enumerate(bin_edges):
                if ts >= lft and ts < rgt:
                    b = bi
                    break
            if b is None:
                centers = np.array([iv.left.value for iv in bins], dtype=np.int64)
                b = int(np.argmin(np.abs(centers - int(ts.value))))
            counts[b] += float(r["Frequency"])
        # å¹³æ»‘åŒ–â†’ãƒ”ãƒ¼ã‚¯æŠ½å‡º
        y = smooth_series(counts, k=5)
        peak_idx = local_peaks(y, n_keep=n_events)
        for b in peak_idx:
            events.append({
                "group_id": int(gid),
                "bin_id": int(b),
                "peak_time": bins[b].left,
                "top_words": stream.group_top_words.get(gid, []),
                "label": stream.gid_label.get(gid, f"group_{gid}")
            })
    return events

def match_events_across_streams(
    events_by_stream: Dict[str, List[Dict[str, object]]],
    word_th: float,
    time_th: int,
    embed_th: Optional[float] = None,
) -> Dict[Tuple[str, int], int]:
    """
    å¤šæ•°ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒšã‚¢ãƒ¯ã‚¤ã‚ºã§æ¯”è¼ƒã—ã€é¡ä¼¼ã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã‚’åŒä¸€IDã«çµ±åˆã™ã‚‹ã€‚

    æ¯”è¼ƒã«ã¯ä»¥ä¸‹ã®æ¡ä»¶ã‚’ä½¿ç”¨ã™ã‚‹ï¼š
      1. Top-wordé›†åˆã®Jaccardé¡ä¼¼åº¦ãŒ `word_th` ä»¥ä¸Šã€‚
      2. ç™ºç”ŸbinãŒ `time_th` ä»¥ä¸‹ã®å·®ã§è¿‘æ¥ã—ã¦ã„ã‚‹ã€‚
      3. ã‚¤ãƒ™ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå¹³å‡ã‚³ãƒ¡ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãŒ `embed_th` ä»¥ä¸Š
         ï¼ˆ`embed_th` ãŒ None ã®å ´åˆã¯ã“ã®æ¡ä»¶ã‚’ç„¡è¦–ï¼‰ã€‚

    ã“ã‚Œã«ã‚ˆã‚Šå˜ç´”ãªJaccardã¨æ™‚é–“ã ã‘ã§ãªãã€èªå¥ãŒç•°ãªã‚‹è¨€èªã§ã‚‚å†…å®¹ãŒè¿‘ã„å ´åˆã‚’æ‹¾ãˆã‚‹ã€‚

    Parameters
    ----------
    events_by_stream : dict
        ã‚¹ãƒˆãƒªãƒ¼ãƒ ã”ã¨ã«æŠ½å‡ºã—ãŸã‚¤ãƒ™ãƒ³ãƒˆã‚’æ ¼ç´ã—ãŸè¾æ›¸ã€‚
        å„ã‚¤ãƒ™ãƒ³ãƒˆdictã«ã¯ 'top_words', 'bin_id', 'embedding' (optional) ãªã©ãŒå«ã¾ã‚Œã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
    word_th : float
        ãƒˆãƒ”ãƒƒã‚¯ã®ä¸Šä½èªé›†åˆã§è¨ˆç®—ã—ãŸJaccardé¡ä¼¼åº¦ã®é–¾å€¤ã€‚
    time_th : int
        ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿbinã®å·®ã®è¨±å®¹ç¯„å›²ã€‚
    embed_th : float or None
        ã‚¤ãƒ™ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦é–¾å€¤ã€‚Noneã®ã¨ãã¯ãƒã‚§ãƒƒã‚¯ã—ãªã„ã€‚

    Returns
    -------
    dict
        (stream_key, event_index) -> unified_event_id ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    """
    items = []
    # Flatten events with their stream key and index
    for key, evts in events_by_stream.items():
        for i, evt in enumerate(evts):
            items.append((key, i, evt))
    # DSU setup
    parent: Dict[Tuple[str,int], Tuple[str,int]] = {(k,i):(k,i) for k,i,_ in items}
    def find(x: Tuple[str,int]) -> Tuple[str,int]:
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x
    def union(x: Tuple[str,int], y: Tuple[str,int]) -> None:
        rx, ry = find(x), find(y)
        if rx != ry:
            parent[ry] = rx
    # Pairwise comparisons
    for a in range(len(items)):
        ka, ia, ea = items[a]
        for b in range(a+1, len(items)):
            kb, ib, eb = items[b]
            # åŒä¸€ã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ã‚¤ãƒ™ãƒ³ãƒˆã¯ãƒãƒ¼ã‚¸ã—ãªã„
            if ka == kb:
                continue
            # Jaccard on top words
            # å„ã‚¤ãƒ™ãƒ³ãƒˆã® top_words ã‚’æ­£è¦åŒ–ï¼ˆç”¨èªãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰ã—ã¦Jaccardé¡ä¼¼åº¦ã‚’è¨ˆç®—
            sa_raw = ea.get("top_words", [])
            sb_raw = eb.get("top_words", [])
            sa = {normalize_term(w) for w in sa_raw if isinstance(w, str) and w.strip()}
            sb = {normalize_term(w) for w in sb_raw if isinstance(w, str) and w.strip()}
            if not sa and not sb:
                continue
            # Jaccard similarity of normalized sets
            jacc = len(sa & sb) / (len(sa | sb) + 1e-12)
            if jacc < word_th:
                continue
            # Time proximity
            if abs(int(ea.get("bin_id", -1)) - int(eb.get("bin_id", -1))) > time_th:
                continue
            # Embedding similarity (optional)
            if embed_th is not None:
                emb_a = ea.get("embedding")
                emb_b = eb.get("embedding")
                # ã©ã¡ã‚‰ã‹æ¬ å¦‚ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
                if emb_a is None or emb_b is None:
                    continue
                # æ­£è¦åŒ–æ¸ˆã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
                num = float(np.dot(emb_a, emb_b))
                # æ—¢ã«normalize_embeddings=Trueã§ç”Ÿæˆã—ã¦ã„ã‚‹ã®ã§normã¯â‰ˆ1
                if num < embed_th:
                    continue
            # All conditions satisfied â†’ union
            union((ka, ia), (kb, ib))
    # Assign unified IDs
    root2id: Dict[Tuple[str,int], int] = {}
    event_map: Dict[Tuple[str,int], int] = {}
    nxt = 0
    for k,i,_ in items:
        r = find((k,i))
        if r not in root2id:
            root2id[r] = nxt
            nxt += 1
        event_map[(k,i)] = root2id[r]
    return event_map

def extract_event_comments(stream: StreamData, event: Dict[str, object], peak_pad: int) -> Tuple[List[str], List[str]]:
    """
    ã‚¤ãƒ™ãƒ³ãƒˆã«è©²å½“ã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆã¨ãã®è¨€èªãƒªã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚
    æˆ»ã‚Šå€¤ã¯ (ã‚³ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ, è¨€èªã®ãƒªã‚¹ãƒˆ)
    """
    gid, bin_id = event["group_id"], int(event["bin_id"])
    bins = build_relative_time_bins(stream.df_valid["timestamp"], stream.nr_bins)
    raw2g: Dict[int, int] = {}
    for g_id, members in enumerate(stream.groups):
        for t in members:
            raw2g[t] = g_id
    low, high = max(0, bin_id - peak_pad), min(stream.nr_bins - 1, bin_id + peak_pad)
    comments: List[str] = []
    langs: List[str] = []
    # 1:1å¯¾å¿œã®ãŸã‚ topics_valid ã¯ df_valid ã¨åŒã˜é †
    for i, row in stream.df_valid.iterrows():
        topic_id = stream.topics_valid[i]
        if topic_id == -1 or raw2g.get(topic_id, -1) != gid:
            continue
        ts = row["timestamp"]
        b = None
        for bi, iv in enumerate(bins):
            if ts >= iv.left and ts < iv.right:
                b = bi
                break
        if b is None:
            centers = np.array([iv.left.value for iv in bins], dtype=np.int64)
            b = int(np.argmin(np.abs(centers - int(ts.value))))
        if low <= b <= high:
            comments.append(row["message_clean"])
            # è¨€èªåˆ—ãŒã‚ã‚Œã°å–å¾—
            lang = row.get("lang") if isinstance(row, pd.Series) else None
            if lang is None and stream.languages is not None and i < len(stream.languages):
                lang = stream.languages.iloc[i]
            langs.append(lang if isinstance(lang, str) else "unk")
    return comments, langs

def js_distance(p: np.ndarray, q: np.ndarray) -> float:
    p = p.astype(float); q = q.astype(float)
    p = p / (p.sum() + 1e-12); q = q / (q.sum() + 1e-12)
    m = 0.5 * (p + q)
    kl_pm = np.sum(np.where(p > 0, p * np.log((p + 1e-12) / (m + 1e-12)), 0.0))
    kl_qm = np.sum(np.where(q > 0, q * np.log((q + 1e-12) / (m + 1e-12)), 0.0))
    return float(np.sqrt(0.5 * (kl_pm + kl_qm)))

def compute_lexical_distance(comments_a: List[str], comments_b: List[str], top_n: int = 1000) -> float:
    """
    ã‚³ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆåŒå£«ã®èªå½™åˆ†å¸ƒå·®ï¼ˆJensenâ€“Shannonè·é›¢ï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    ãƒˆãƒ¼ã‚¯ãƒ³ã¯ç”¨èªãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã§æ­£è¦åŒ–ã—ã¦ã‹ã‚‰é »åº¦ã‚’æ•°ãˆã‚‹ãŸã‚ã€
    è¨€èªã‚„è¡¨è¨˜æºã‚ŒãŒç•°ãªã£ã¦ã‚‚é¡ç¾©èªã¨ã—ã¦æ‰±ã†ã“ã¨ãŒã§ãã‚‹ã€‚
    """
    ca, cb = Counter(), Counter()
    for txt in comments_a:
        for w in txt.split():
            norm = normalize_term(w)
            ca[norm] += 1
    for txt in comments_b:
        for w in txt.split():
            norm = normalize_term(w)
            cb[norm] += 1
    combined = ca + cb
    # ä¸Šä½å˜èªã‚’æœ€å¤§ top_n ã¾ã§
    vocab = [w for w, _ in combined.most_common(top_n)]
    if not vocab:
        return 0.0
    va = np.array([ca.get(w, 0) for w in vocab], dtype=float)
    vb = np.array([cb.get(w, 0) for w in vocab], dtype=float)
    return js_distance(va, vb)

# -------------------------
# å¯è¦–åŒ–: ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ»è·é›¢è¡Œåˆ—ãƒ»ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
# -------------------------
def save_csv_and_png_heatmap(df: pd.DataFrame, out_csv: str, out_png: str, title: str):
    os.makedirs(os.path.dirname(out_csv), exist_ok=True)
    # å¸¸ã«å®Œå…¨ãªè¡¨ã‚’CSVã«ä¿å­˜
    df.to_csv(out_csv, index=True, encoding="utf-8-sig")
    # å¯è¦–åŒ–ã¯æ•°å€¤åˆ—ã®ã¿ã«é™å®šã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’é˜²ã
    df_plot = df.select_dtypes(include=[np.number])
    if df_plot is None or df_plot.empty:
        print(f"[WARN] Heatmap skipped for {out_png}: no numeric data to plot")
        return
    # å›³ã‚µã‚¤ã‚ºèª¿æ•´ï¼ˆè«–æ–‡å‘ã‘ã«ã‚„ã‚„åºƒã‚ï¼‰
    fig_width = max(8, 0.6 * len(df_plot.columns) + 3)
    fig_height = max(8, 0.5 * len(df_plot.index) + 3)
    plt.figure(figsize=(fig_width, fig_height))
    vals = df_plot.values.astype(float)
    # presenceï¼ˆ0/1ï¼‰ã®è¦‹ã‚„ã™ã„é…è‰²ã«è‡ªå‹•èª¿æ•´
    unique_vals = np.unique(vals)
    is_binary = set(unique_vals.tolist()) <= {0.0, 1.0}
    if is_binary:
        im = plt.imshow(vals, aspect="auto", cmap="Greens", vmin=0, vmax=1)
    else:
        vmin = float(np.nanmin(vals)) if np.isfinite(vals).any() else 0.0
        vmax = float(np.nanmax(vals)) if np.isfinite(vals).any() else 1.0
        if vmin == vmax:
            vmin, vmax = 0.0, 1.0
        im = plt.imshow(vals, aspect="auto", cmap="viridis", vmin=vmin, vmax=vmax)
    plt.title(title)
    plt.xlabel("Streams")
    plt.ylabel("Events")
    # xè»¸ãƒ©ãƒ™ãƒ«: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã¾ãŸã¯åˆ—åï¼‰ã‚’æ”¹è¡Œä»˜ãã«ã—ã¦é•·ã•ã‚’èª¿æ•´
    x_labels = []
    for c in df_plot.columns:
        s = os.path.basename(str(c))
        if len(s) > 12:
            s = '\n'.join([s[i:i+12] for i in range(0, len(s), 12)])
        x_labels.append(s)
    plt.xticks(range(len(df_plot.columns)), x_labels, rotation=40, ha="right", fontsize=8)
    # yè»¸ãƒ©ãƒ™ãƒ«: ã‚¤ãƒ™ãƒ³ãƒˆãƒ©ãƒ™ãƒ«ãŒé•·ã„å ´åˆã¯æ”¹è¡Œã‚’æŒ¿å…¥ï¼ˆå…ƒã®indexã‚’ãã®ã¾ã¾åˆ©ç”¨ï¼‰
    y_labels = []
    for idx_label in df_plot.index:
        s = str(idx_label)
        if len(s) > 20:
            s = '\n'.join([s[i:i+20] for i in range(0, len(s), 20)])
        y_labels.append(s)
    plt.yticks(range(len(df_plot.index)), y_labels, fontsize=9)
    cb = plt.colorbar(im); cb.set_label("Similarity" if not is_binary else "Presence")
    # ã‚»ãƒ«æ³¨é‡ˆï¼ˆå°ã•ã‚ã®è¡Œåˆ—ã®ã¿ï¼‰
    total_cells = vals.shape[0] * vals.shape[1]
    if total_cells <= 400:
        for i in range(vals.shape[0]):
            for j in range(vals.shape[1]):
                v = vals[i, j]
                if is_binary:
                    if v >= 0.5:
                        plt.text(j, i, "1", ha="center", va="center", color="white", fontsize=9, fontweight="bold")
                else:
                    txt_color = "white" if v > (np.nanmin(vals) + np.nanmax(vals)) / 2 else "black"
                    try:
                        plt.text(j, i, f"{v:.2f}", ha="center", va="center", color=txt_color, fontsize=8)
                    except Exception:
                        pass
    # ã‚°ãƒªãƒƒãƒ‰ç·š
    plt.grid(which='both', color='lightgray', linestyle='-', linewidth=0.3)
    plt.gca().set_xticks(np.arange(-0.5, vals.shape[1], 1), minor=True)
    plt.gca().set_yticks(np.arange(-0.5, vals.shape[0], 1), minor=True)
    plt.grid(which='minor', color='lightgray', linestyle='-', linewidth=0.3)
    plt.gca().tick_params(which='minor', bottom=False, left=False)
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig(out_png, dpi=220)
    plt.close()
    print(f"Saved heatmap: {out_png}")

def _save_matched_summary_table(presence_df: pd.DataFrame, matched_df: pd.DataFrame, out_png: str, top_k: int = 15) -> None:
    """ä¸€è‡´ã‚¤ãƒ™ãƒ³ãƒˆã®æ¦‚è¦ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ PNG ã§ä¿å­˜ã€‚
    åˆ—: Time, Best Pair, Similarity, Label
    """
    try:
        if presence_df is None or presence_df.empty or matched_df is None or matched_df.empty:
            return
        # presence ã® time_label/row_label ã¨ matched ã® (group_id,bin_id) ã‚’çªåˆ
        lef = presence_df[["group_id","bin_id","time_label","row_label"]].drop_duplicates()
        rig = matched_df.copy()
        # best_pair/best_sim ãŒç„¡ã„æ—§ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œ
        if "best_pair" not in rig.columns:
            # best ãƒšã‚¢ã¯ (lex) ãŒæœ€å°ã®ãƒšã‚¢ã‹ã‚‰æ¨å®š
            lex_cols = [c for c in rig.columns if c.endswith("(lex)")]
            if lex_cols:
                best_idx = np.argmin(rig[lex_cols].values, axis=1)
                rig["best_pair"] = [lex_cols[i].replace(" (lex)", "") for i in best_idx]
                rig["best_sim"] = 1.0 - np.take_along_axis(rig[lex_cols].values, best_idx.reshape(-1,1), axis=1).ravel()
        m = pd.merge(rig, lef, on=["group_id","bin_id"], how="left")
        # è¡¨ç¤ºç”¨åˆ—ã«æ•´å½¢
        view = m[["time_label","best_pair","best_sim","label"]].copy()
        view = view.rename(columns={"time_label":"Time","best_pair":"Best Pair","best_sim":"Similarity","label":"Topic"})
        # ä¸¦ã¹æ›¿ãˆï¼ˆSimilarity é™é †ã€Time æ˜‡é †ï¼‰
        if "Similarity" in view.columns:
            view = view.sort_values(["Similarity","Time"], ascending=[False, True])
        else:
            view = view.sort_values(["Time"], ascending=True)
        # ä¸Šä½ã®ã¿
        if len(view) > top_k:
            view = view.head(top_k)
        # æ•°å€¤ã®ä¸¸ã‚
        if "Similarity" in view.columns:
            view["Similarity"] = view["Similarity"].map(lambda x: f"{x:.2f}")
        # ä¿å­˜
        save_df_as_table_png(view, out_png, title="Matched Events Summary (Top)")
    except Exception as e:
        print(f"[WARN] failed to save matched summary: {e}")

def save_png_distance_matrix(mat: pd.DataFrame, out_csv: str, out_png: str, title: str):
    os.makedirs(os.path.dirname(out_csv), exist_ok=True)
    mat.to_csv(out_csv, index=True, encoding="utf-8-sig")
    plt.figure(figsize=(max(6, 0.35*len(mat.columns)+3), max(6, 0.35*len(mat.index)+3)))
    plt.imshow(mat.values, vmin=0, vmax=1)
    plt.title(title); plt.xlabel("Stream"); plt.ylabel("Stream")
    ticks = range(len(mat.columns))
    labels = [os.path.basename(c) for c in mat.columns]
    plt.xticks(ticks, labels, rotation=45, ha="right", fontsize=9)
    plt.yticks(ticks, labels, fontsize=9)
    cb = plt.colorbar(); cb.set_label("Jensenâ€“Shannon distance")
    plt.tight_layout(); plt.savefig(out_png, dpi=220); plt.close()
    print(f"Saved distance matrix: {out_png}")

def save_emoji_timeline_heatmap(df: pd.DataFrame, out_csv: str, out_png: str, title: str):
    """Save an emoji timeline heatmap (time x emoji) per stream.
    df: index=time label (e.g., HH:MM), columns=emoji char, values=counts.
    """
    os.makedirs(os.path.dirname(out_csv), exist_ok=True)
    df.to_csv(out_csv, index=True, encoding="utf-8-sig")
    if df.empty:
        print(f"[WARN] Emoji timeline empty: {out_png}")
        return
    plt.figure(figsize=(max(8, 0.5*len(df.columns)+3), max(6, 0.35*len(df.index)+2)))
    vals = df.values.astype(float)
    im = plt.imshow(vals, aspect="auto", cmap="magma")
    plt.title(title)
    plt.xlabel("Emoji")
    plt.ylabel("Time")
    plt.xticks(range(len(df.columns)), list(df.columns), rotation=0, fontsize=12)
    plt.yticks(range(len(df.index)), list(df.index), fontsize=9)
    cb = plt.colorbar(im); cb.set_label("Count")
    plt.tight_layout(); plt.savefig(out_png, dpi=200); plt.close()
    print(f"Saved emoji timeline: {out_png}")

def make_wordcloud(texts: List[str], out_png: str):
    from wordcloud import WordCloud
    os.makedirs(os.path.dirname(out_png), exist_ok=True)
    # textsãŒç©ºã€ã‚‚ã—ãã¯æ–‡å­—åˆ—ãŒãªã„å ´åˆã¯ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã—ãªã„
    if not texts:
        print(f"[WARN] Wordcloud skipped for {out_png}: no texts")
        return
    txt = " ".join(texts).strip()
    # ãƒˆãƒ¼ã‚¯ãƒ³ãŒ1å€‹æœªæº€ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
    tokens = txt.split()
    if len(tokens) == 0:
        print(f"[WARN] Wordcloud skipped for {out_png}: no tokens to plot")
        return
    # WordCloud ç”¨ãƒ•ã‚©ãƒ³ãƒˆã‚’è‡ªå‹•é¸æŠã€‚æ—¥æœ¬èªã‚’å«ã‚€å ´åˆã¯ä½¿ç”¨ä¸­ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹ã€‚
    font_path = None
    try:
        if _used_jp_font:
            from matplotlib import font_manager as _fm
            try:
                font_path = _fm.findfont(_used_jp_font, fontext="ttf")
            except Exception:
                font_path = None
    except Exception:
        font_path = None
    # WordCloudç”Ÿæˆ
    wc = WordCloud(
        width=1200,
        height=800,
        background_color="white",
        font_path=font_path if font_path else None,
        collocations=False,
    )
    try:
        wc.generate(txt)
        wc.to_file(out_png)
        print(f"Saved wordcloud: {out_png}")
    except Exception as e:
        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆã¯è­¦å‘Šã®ã¿å‡ºåŠ›
        print(f"[WARN] wordcloud failed for {out_png}: {e}")

# -------------------------
# å¼•æ•°å‡¦ç†
# -------------------------
def parse_args():
    p = argparse.ArgumentParser(description="Compare events across multiple streams for the same match.")
    # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ« or ãƒ•ã‚©ãƒ«ãƒ€ï¼‹ãƒ‘ã‚¿ãƒ¼ãƒ³
    p.add_argument("--files", nargs="+", help="åˆ†æã™ã‚‹CSVã‚’è¤‡æ•°æŒ‡å®šï¼ˆã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼‰")
    p.add_argument("--folder", type=str, help="CSVãŒå…¥ã£ã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€")
    p.add_argument("--pattern", type=str, default="*.csv", help="ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹: *.csvï¼‰")
    # æ—¢å­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    p.add_argument("--time-bins", type=int, default=300)
    p.add_argument("--peak-pad", type=int, default=1)
    p.add_argument("--jaccard-th", type=float, default=0.6)
    p.add_argument("--word-match-th", type=float, default=0.4)
    p.add_argument("--time-match-th", type=int, default=1)
    # cross-lingual embedding similarity threshold for event matching
    p.add_argument("--embedding-match-th", type=float, default=None,
                   help="åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦é–¾å€¤ï¼ˆNoneãªã‚‰ãƒã‚§ãƒƒã‚¯ã—ãªã„ï¼‰")
    p.add_argument("--n-events", type=int, default=5)
    p.add_argument("--topk", type=int, default=10, help="æ™‚ç³»åˆ—æç”»ã®ä¸Šä½ã‚°ãƒ«ãƒ¼ãƒ—æ•°")
    p.add_argument("--save-json", action="store_true")
    # å¯è¦–åŒ–ã®åˆ¶é™: é¡ä¼¼åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã«è¡¨ç¤ºã™ã‚‹ä¸€è‡´ã‚¤ãƒ™ãƒ³ãƒˆã®ä¸Šä½ä»¶æ•°ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆé‡ã®å¤šã„é †ï¼‰
    p.add_argument("--top-matched", type=int, default=5,
                   help="matched_event_presence.png ã«è¡¨ç¤ºã™ã‚‹ä¸Šä½ã‚¤ãƒ™ãƒ³ãƒˆæ•°ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆç·æ•°ã®å¤šã„é †ã€0ã§åˆ¶é™ãªã—ï¼‰")
    # ä¸Šä½ã‚°ãƒ«ãƒ¼ãƒ—ã«é™å®šã™ã‚‹æ•°ï¼ˆä¾‹: 10â†’ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãŒå¤šã„ä¸Šä½10ã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿ï¼‰
    p.add_argument("--focus-top", type=int, default=10, help="ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’è¡Œã†å¯¾è±¡ã‚°ãƒ«ãƒ¼ãƒ—æ•°ï¼ˆNoneã®å ´åˆã¯å…¨ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰")
    # Emoji timeline å¯è¦–åŒ–è¨­å®š
    p.add_argument("--emoji-topk", type=int, default=10, help="å„é…ä¿¡ã®çµµæ–‡å­—ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«è¡¨ç¤ºã™ã‚‹ä¸Šä½çµµæ–‡å­—æ•°")
    args = p.parse_args()

    # --folder/--pattern ã‚’ --files ã«å±•é–‹
    if (not args.files) and args.folder:
        from pathlib import Path
        files = sorted(str(p) for p in Path(args.folder).glob(args.pattern))
        if not files:
            raise SystemExit(f"No files matched: {args.folder}/{args.pattern}")
        args.files = files
    if not args.files:
        p.error("either --files or --folder must be provided")
    # ãƒ•ãƒ«ãƒ‘ã‚¹åŒ–
    args.files = [os.path.abspath(f) for f in args.files]
    return args

# -------------------------
# ãƒ¡ã‚¤ãƒ³
# -------------------------
def main():
    args = parse_args()
    embedding_model = SentenceTransformer(EMB_NAME)

    # ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†
    streams: Dict[str, StreamData] = {}
    for csv_file in args.files:
        if not os.path.exists(csv_file):
            print(f"File not found: {csv_file}"); continue
        sd = process_stream(csv_file, embedding_model, args.jaccard_th, args.time_bins, topk_plot=args.topk)
        if sd: streams[csv_file] = sd
    if len(streams) < 2:
        print("Need at least two valid streams to compare events."); return

    # å„ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡ºï¼ˆã‚³ãƒ¡ãƒ³ãƒˆãŒå¤šã„ä¸Šä½ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å„ªå…ˆï¼‰
    events_by_stream: Dict[str, List[Dict[str, object]]] = {}
    for key, sd in streams.items():
        events_by_stream[key] = detect_events(sd, n_events=args.n_events, focus_top=args.focus_top)

    # å„ã‚¤ãƒ™ãƒ³ãƒˆã«ã‚³ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä»˜ä¸ã™ã‚‹
    # ã¾ãšã¯ã‚¤ãƒ™ãƒ³ãƒˆã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã€å¹³å‡åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—ï¼ˆnormalize_embeddings=Trueã§ã‚ã‚‹ãŸã‚å¹³å‡å¾Œã‚‚å˜ä½é•·ã«å†æ­£è¦åŒ–ï¼‰
    for stream_key, evts in events_by_stream.items():
        for evt in evts:
            try:
                comments, _langs = extract_event_comments(streams[stream_key], evt, args.peak_pad)
                if comments:
                    vecs = embedding_model.encode(comments, batch_size=32, show_progress_bar=False, normalize_embeddings=True)
                    # 2D array (n_comments x dim)
                    # å¹³å‡ã—ãŸå¾Œã€å†æ­£è¦åŒ–
                    mean_vec = np.mean(vecs, axis=0)
                    norm = np.linalg.norm(mean_vec) + 1e-12
                    mean_vec = mean_vec / norm
                else:
                    # ã‚³ãƒ¡ãƒ³ãƒˆãŒãªã„å ´åˆã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
                    dim = embedding_model.get_sentence_embedding_dimension()
                    mean_vec = np.zeros(dim, dtype=float)
                evt["embedding"] = mean_vec
            except Exception:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯embeddingã‚’Noneã«
                evt["embedding"] = None
    # å…±é€šã‚¤ãƒ™ãƒ³ãƒˆç…§åˆ
    event_map = match_events_across_streams(
        events_by_stream,
        args.word_match_th,
        args.time_match_th,
        embed_th=args.embedding_match_th,
    )
    if not event_map:
        print("ä¸€è‡´ã™ã‚‹å…±é€šã‚¤ãƒ™ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚é–¾å€¤ï¼ˆ--time-match-th, --word-match-th, --jaccard-thï¼‰ã‚’èª¿æ•´ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    # å…±é€šã‚¤ãƒ™ãƒ³ãƒˆIDã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°
    events_by_id: Dict[int, Dict[str, object]] = defaultdict(lambda: {"streams": {}})
    for stream_key, evts in events_by_stream.items():
        for i, evt in enumerate(evts):
            eid = event_map[(stream_key, i)]
            events_by_id[eid]["label"] = evt["label"]
            events_by_id[eid]["bin_id"] = evt["bin_id"]
            events_by_id[eid]["streams"][stream_key] = evt

    # å„å…±é€šã‚¤ãƒ™ãƒ³ãƒˆã®ã‚³ãƒ¡ãƒ³ãƒˆåé›† & è·é›¢è¨ˆç®—
    results = []
    raw_data = {}
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”¨ï¼šã‚¤ãƒ™ãƒ³ãƒˆÃ—é…ä¿¡è€… è¡Œåˆ—
    stream_names = [os.path.basename(k) for k in streams.keys()]
    event_presence = []
    for eid, info in events_by_id.items():
        # åŒã˜ã‚¤ãƒ™ãƒ³ãƒˆã«å«ã¾ã‚Œã‚‹é…ä¿¡è€…ãŒ1ã¤ã—ã‹ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        # ã€ŒåŒã˜æ™‚é–“å¸¯ãƒ»åŒã˜ãƒˆãƒ”ãƒƒã‚¯ã§ç››ã‚Šä¸ŠãŒã£ãŸã€ã‚‚ã®ã®ã¿æ¯”è¼ƒå¯¾è±¡ã¨ã™ã‚‹ãŸã‚
        if len(info["streams"]) < 2:
            continue
        label = info["label"]; bin_id = int(info["bin_id"])
        # ã‚³ãƒ¡ãƒ³ãƒˆåé›†
        streams_comments: Dict[str, List[str]] = {}
        streams_langs: Dict[str, List[str]] = {}
        row_presence = {}
        for stream_key in streams.keys():
            if stream_key in info["streams"]:
                comments, langs = extract_event_comments(streams[stream_key], info["streams"][stream_key], args.peak_pad)
                streams_comments[stream_key] = comments
                streams_langs[stream_key] = langs
                row_presence[os.path.basename(stream_key)] = 1
            else:
                streams_comments[stream_key] = []
                streams_langs[stream_key] = []
                row_presence[os.path.basename(stream_key)] = 0

        # è·é›¢è¡Œåˆ—ï¼ˆèªå½™ãƒ»è¨€èªãƒ»çµµæ–‡å­—ã®ã‚¹ã‚¿ã‚¤ãƒ«å·®ï¼‰
        keys = list(streams_comments.keys())
        n = len(keys)
        dmat = np.zeros((n, n), dtype=float)
        lang_mat = np.zeros((n, n), dtype=float)
        emoji_mat = np.zeros((n, n), dtype=float)
        for i in range(n):
            for j in range(n):
                if i == j:
                    dmat[i, j] = 0.0
                    lang_mat[i, j] = 0.0
                    emoji_mat[i, j] = 0.0
                elif i < j:
                    # èªå½™å·®
                    d = compute_lexical_distance(streams_comments[keys[i]], streams_comments[keys[j]])
                    dmat[i, j] = d; dmat[j, i] = d
                    # è¨€èªåˆ†å¸ƒå·®
                    lang_dist_a = compute_language_distribution(streams_langs[keys[i]])
                    lang_dist_b = compute_language_distribution(streams_langs[keys[j]])
                    ld = js_distance_distribution(lang_dist_a, lang_dist_b)
                    lang_mat[i, j] = ld; lang_mat[j, i] = ld
                    # çµµæ–‡å­—æ¯”ç‡å·®
                    er_a = compute_emoji_ratio(streams_comments[keys[i]])
                    er_b = compute_emoji_ratio(streams_comments[keys[j]])
                    emoji_diff = abs(er_a - er_b)
                    emoji_mat[i, j] = emoji_diff; emoji_mat[j, i] = emoji_diff

        # å¹³å‡è·é›¢ãªã©ã‚’çµæœã«
        tril = dmat[np.tril_indices(n, k=-1)]
        avg_dist = float(np.mean(tril)) if tril.size else 0.0
        tril_lang = lang_mat[np.tril_indices(n, k=-1)]
        avg_lang_dist = float(np.mean(tril_lang)) if tril_lang.size else 0.0
        tril_emoji = emoji_mat[np.tril_indices(n, k=-1)]
        avg_emoji_diff = float(np.mean(tril_emoji)) if tril_emoji.size else 0.0
        result = {
            "event_id": int(eid),
            "label": label,
            "peak_bin": int(bin_id),
            "avg_js_distance": avg_dist,
            "avg_language_distance": avg_lang_dist,
            "avg_emoji_difference": avg_emoji_diff,
        }
        # å„ãƒšã‚¢ã”ã¨ã®å€¤ã‚’è¿½åŠ 
        # äº‹å‰ã«å„é…ä¿¡ã®æ„Ÿæƒ…ãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ç‰¹å¾´ã‚’è¨ˆç®—
        sentiments = {k: compute_sentiment_metrics(streams_comments[k]) for k in keys}
        styles = {k: compute_style_profile(streams_comments[k]) for k in keys}
        for i in range(n):
            for j in range(i+1, n):
                name = f"{os.path.basename(keys[i])} vs {os.path.basename(keys[j])}"
                result[f"{name} (lex)"] = float(dmat[i, j])
                result[f"{name} (lang)"] = float(lang_mat[i, j])
                result[f"{name} (emoji)"] = float(emoji_mat[i, j])
        results.append(result)

        # JSONç”¨
        if args.save_json:
            raw_data[eid] = {os.path.basename(k): v for k, v in streams_comments.items()}

        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰å‡ºåŠ›
        evt_dir = os.path.join(OUT_DIR, "wordclouds", f"event_{eid}")
        for stream_key, texts in streams_comments.items():
            out_wc = os.path.join(evt_dir, f"WC_{os.path.basename(stream_key).replace('.csv','')}.png")
            try:
                make_wordcloud(texts, out_wc)
            except Exception as e:
                print(f"[WARN] wordcloud failed for {stream_key}: {e}")

        # ã‚¤ãƒ™ãƒ³ãƒˆÃ—é…ä¿¡è€… presence è¡Œ
        event_presence.append({"event_id": int(eid), **row_presence})

    # === å‡ºåŠ›: ã‚¤ãƒ™ãƒ³ãƒˆæ¯”è¼ƒçµæœ ===
    # ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã«ç©ºã®çµæœã‚’é˜²ã
    if results:
        results_df = pd.DataFrame(results).sort_values(["avg_js_distance", "event_id"])
    else:
        results_df = pd.DataFrame(results)
    out_csv = os.path.join(OUT_DIR, "event_comparison_results.csv")
    results_df.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"Saved: {out_csv}")

    # å…±é€šã‚¤ãƒ™ãƒ³ãƒˆ presence ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    if event_presence:
        eventmap_df = pd.DataFrame(event_presence)
        # event_idåˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®š
        if "event_id" in eventmap_df.columns:
            eventmap_df = eventmap_df.set_index("event_id").reindex(sorted(eventmap_df["event_id"].unique()))
        hm_csv = os.path.join(OUT_DIR, "event_eventmap.csv")
        hm_png = os.path.join(OUT_DIR, "event_eventmap.png")
        save_csv_and_png_heatmap(eventmap_df, hm_csv, hm_png, title="Shared Events Presence (1=matched)")
    else:
        print("[INFO] No matched events to create event presence heatmap.")

    # ãƒšã‚¢è·é›¢ã®å¹³å‡ â†’ å¹³å‡è·é›¢è¡Œåˆ—ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆã”ã¨å¹³å‡ã—ã¦ã‚‚ã‚ˆã„ãŒã€ã“ã“ã¯å…¨ã‚¤ãƒ™ãƒ³ãƒˆå¹³å‡ï¼‰
    # å…¨ã‚¤ãƒ™ãƒ³ãƒˆã®è·é›¢ã‚’åˆç®—ã—ã¦å¹³å‡
    # æ§‹ç¯‰: ã‚¹ãƒˆãƒªãƒ¼ãƒ é † keys ã«åˆã‚ã›ã‚‹
    stream_keys = list(streams.keys())
    n = len(stream_keys)
    acc = np.zeros((n, n), dtype=float); cnt = np.zeros((n, n), dtype=int)
    for eid, info in events_by_id.items():
        # åŒã˜ã‚¤ãƒ™ãƒ³ãƒˆã«å«ã¾ã‚Œã‚‹é…ä¿¡è€…ãŒ2ã¤æœªæº€ãªã‚‰ã‚°ãƒ­ãƒ¼ãƒãƒ«è·é›¢ã«ã¯åŠ ç®—ã—ãªã„
        if len(info["streams"]) < 2:
            continue
        # å„ã‚¤ãƒ™ãƒ³ãƒˆã§è·é›¢ã‚’è¨ˆç®—
        comments: Dict[str, List[str]] = {}
        for k in stream_keys:
            if k in info["streams"]:
                cmt, _langs = extract_event_comments(streams[k], info["streams"][k], args.peak_pad)
                comments[k] = cmt
            else:
                comments[k] = []
        for i in range(n):
            for j in range(i+1, n):
                d = compute_lexical_distance(comments[stream_keys[i]], comments[stream_keys[j]])
                acc[i, j] += d; acc[j, i] += d
                cnt[i, j] += 1; cnt[j, i] += 1
    with np.errstate(divide='ignore', invalid='ignore'):
        avg = np.where(cnt>0, acc/np.maximum(cnt,1), 0.0)
    names = [os.path.basename(k) for k in stream_keys]
    dist_df = pd.DataFrame(avg, index=names, columns=names)
    dist_csv = os.path.join(OUT_DIR, "event_comparison_distance_matrix.csv")
    dist_png = os.path.join(OUT_DIR, "event_comparison_results.png")
    save_png_distance_matrix(dist_df, dist_csv, dist_png, title="Average JS Distance across Shared Events")

    # JSONä¿å­˜
    if args.save_json:
        with open(os.path.join(OUT_DIR, "event_comments.json"), "w", encoding="utf-8") as f:
            json.dump(raw_data, f, ensure_ascii=False, indent=2)
    print(f"Saved: {os.path.join(OUT_DIR, 'event_comments.json')}")

    print("All done")

    # === æ–°æ©Ÿèƒ½: åŒæ™‚é–“å¸¯ãƒ»åŒãƒˆãƒ”ãƒƒã‚¯ã§ç››ã‚Šä¸ŠãŒã£ãŸã‚¤ãƒ™ãƒ³ãƒˆé–“ã®æ¯”è¼ƒ ===
    # è¤‡æ•°é…ä¿¡è€…ãŒåŒã˜ã‚°ãƒ«ãƒ¼ãƒ—IDã¨bin ID (ãƒ”ãƒ¼ã‚¯æ™‚é–“å¸¯) ã§ç››ã‚Šä¸ŠãŒã£ã¦ã„ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã€
    # èªå½™ãƒ»è¨€èªãƒ»çµµæ–‡å­—ã®é•ã„ã‚’å®šé‡åŒ–ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚
    print("Matching events on exact group & bin across streams ...")
    # (group_id, bin_id) -> {stream_key: event_dict}
    events_by_group_bin: Dict[Tuple[int, int], Dict[str, Dict[str, object]]] = defaultdict(dict)
    for stream_key, evts in events_by_stream.items():
        for evt in evts:
            key = (int(evt.get("group_id", -1)), int(evt.get("bin_id", -1)))
            events_by_group_bin[key][stream_key] = evt
    matched_results = []
    matched_presence = []  # è¨­å®š: group-binã”ã¨ã®presenceï¼ˆæ™‚é–“å¸¯ãƒ»ãƒ©ãƒ™ãƒ«ãƒ»é¡ä¼¼åº¦ã®æ³¨é‡ˆä»˜ãï¼‰
    matched_similarity = []  # æ–°è¦: å‚ç…§é…ä¿¡ã«å¯¾ã™ã‚‹å„é…ä¿¡ã®é¡ä¼¼åº¦ï¼ˆ1-JSï¼‰, è¡Œæ³¨é‡ˆä»˜ã
    matched_meta = []  # è¡Œãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: gid,bin,participants,total_comments
    # ä¸€è‡´ã—ãŸã‚¤ãƒ™ãƒ³ãƒˆã®ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®è¾æ›¸
    matched_comments: Dict[str, Dict[str, List[str]]] = {}
    # è©³ç´°æƒ…å ±ã®ä¿å­˜ç”¨: å„ãƒãƒƒãƒã‚¤ãƒ™ãƒ³ãƒˆÃ—é…ä¿¡è€…ã”ã¨ã®è¡Œã‚’è“„ç©
    matched_details_all: List[Dict[str, object]] = []
    for (gid, bin_id), evts_dict in events_by_group_bin.items():
        # 2ã¤ä»¥ä¸Šã®é…ä¿¡è€…ãŒè©²å½“ã™ã‚‹å ´åˆã®ã¿æ¯”è¼ƒ
        if len(evts_dict) < 2:
            continue
        # ã‚°ãƒ«ãƒ¼ãƒ—ãƒ©ãƒ™ãƒ«ã¯ä»£è¡¨ã®ã‚‚ã®ã‚’æ¡ç”¨
        label = list(evts_dict.values())[0].get("label", f"group_{gid}")
        # ã‚³ãƒ¡ãƒ³ãƒˆãƒ»è¨€èªã‚’åé›†
        comments_by_stream: Dict[str, List[str]] = {}
        langs_by_stream: Dict[str, List[str]] = {}
        presence_row = {"group_id": gid, "bin_id": bin_id, "label": label}
        for stream_key in streams.keys():
            if stream_key in evts_dict:
                comments, langs = extract_event_comments(streams[stream_key], evts_dict[stream_key], args.peak_pad)
                comments_by_stream[stream_key] = comments
                langs_by_stream[stream_key] = langs
                presence_row[os.path.basename(stream_key)] = 1
            else:
                comments_by_stream[stream_key] = []
                langs_by_stream[stream_key] = []
                presence_row[os.path.basename(stream_key)] = 0
        # è·é›¢è¡Œåˆ—ã‚’è¨ˆç®—
        keys = list(streams.keys())
        n = len(keys)
        dmat = np.zeros((n, n), dtype=float)
        lang_mat = np.zeros((n, n), dtype=float)
        emoji_mat = np.zeros((n, n), dtype=float)
        for i in range(n):
            for j in range(i+1, n):
                d = compute_lexical_distance(comments_by_stream[keys[i]], comments_by_stream[keys[j]])
                dmat[i, j] = d; dmat[j, i] = d
                ld = js_distance_distribution(
                    compute_language_distribution(langs_by_stream[keys[i]]),
                    compute_language_distribution(langs_by_stream[keys[j]])
                )
                lang_mat[i, j] = ld; lang_mat[j, i] = ld
                ediff = abs(
                    compute_emoji_ratio(comments_by_stream[keys[i]]) - compute_emoji_ratio(comments_by_stream[keys[j]])
                )
                emoji_mat[i, j] = ediff; emoji_mat[j, i] = ediff
        # æ„Ÿæƒ…ãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ç‰¹å¾´ï¼ˆãƒšã‚¢è·é›¢ç®—å‡ºç”¨ï¼‰
        sentiments = {k: compute_sentiment_metrics(comments_by_stream.get(k, [])) for k in keys}
        styles = {k: compute_style_profile(comments_by_stream.get(k, [])) for k in keys}
        # === è¿½åŠ : ã“ã®ä¸€è‡´ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆåŒä¸€groupÃ—binï¼‰ã®"æ™‚é–“å¸¯"ã¨"å„é…ä¿¡ã®ãƒ©ãƒ™ãƒ«(ä¸Šä½èª)"ã€"é¡ä¼¼åº¦(1-JSè·é›¢)"ã‚’æ³¨é‡ˆã¨ã—ã¦ä½œæˆ ===
        # å‚åŠ é…ä¿¡ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆãŒå­˜åœ¨ã™ã‚‹ï¼presence=1ï¼‰ã®æŠ½å‡º
        present_streams_basename: List[str] = []
        present_streams_keys: List[str] = []
        for sk in streams.keys():
            if sk in evts_dict:
                present_streams_keys.append(sk)
                present_streams_basename.append(os.path.basename(sk))
        # æ™‚é–“å¸¯ï¼ˆçµ¶å¯¾æ™‚åˆ»ã®ä¸­å¿ƒ: ns since epochï¼‰ã®ä»£è¡¨å€¤ï¼ˆä¸­å¤®å€¤ï¼‰ã‚’ç®—å‡º
        center_ns: List[int] = []
        stream_label_map: Dict[str, str] = {}
        stream_words_map: Dict[str, str] = {}
        for sk in present_streams_keys:
            evt_info = evts_dict[sk]
            stream_obj = streams[sk]
            bins = build_relative_time_bins(stream_obj.df_valid["timestamp"], stream_obj.nr_bins)
            b_local = int(evt_info.get("bin_id", -1))
            if 0 <= b_local < len(bins):
                interval = bins[b_local]
                center_ts = interval.left + (interval.right - interval.left)/2
                try:
                    center_ns.append(int(pd.Timestamp(center_ts).value))
                except Exception:
                    pass
            # å„é…ä¿¡ã®ãƒ©ãƒ™ãƒ«ï¼ˆçŸ­ç¸®ç‰ˆ: ä¸Šä½èª 2ã€œ3 èª, ãªã‘ã‚Œã°å…ƒãƒ©ãƒ™ãƒ«ã‚’2èªã«ï¼‰
            gid_local = int(evt_info.get("group_id", -1))
            top_words_local = stream_obj.group_top_words.get(gid_local, [])[:3]
            if top_words_local:
                short = ",".join(top_words_local)
            else:
                raw_label = str(evt_info.get("label", ""))
                toks = [normalize_term(w) for w in re.split(r"[\sãƒ»,ï¼Œã€‚!ï¼?ï¼Ÿ]+", raw_label) if w]
                toks = [t for t in toks if len(t) > 1][:2]
                short = ",".join(toks) if toks else "topic"
            stream_label_map[os.path.basename(sk)] = short
            stream_words_map[os.path.basename(sk)] = short
        time_label = ""
        if center_ns:
            cen = pd.to_datetime(int(np.median(center_ns)))
            try:
                time_label = pd.Timestamp(cen).strftime("%H:%M")
            except Exception:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ†ä¸¸ã‚
                minutes = int(round((pd.Timestamp(cen).value/1e9) / 60.0))
                time_label = f"{minutes:02d}:00"
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºã®èªå½™é¡ä¼¼åº¦(1-JSè·é›¢)ã‚’è¨ˆç®—ã—ã€æœ€è‰¯ãƒšã‚¢ã‚’æŠ½å‡º
        pair_sims: List[Tuple[str, str, float]] = []
        key_index = {k: i for i, k in enumerate(keys)}
        for i in range(len(present_streams_keys)):
            for j in range(i+1, len(present_streams_keys)):
                ki = key_index[present_streams_keys[i]]
                kj = key_index[present_streams_keys[j]]
                sim = float(max(0.0, min(1.0, 1.0 - dmat[ki, kj])))
                pair_sims.append((os.path.basename(present_streams_keys[i]), os.path.basename(present_streams_keys[j]), sim))
        best_pair_str = ""
        if pair_sims:
            # é¡ä¼¼åº¦ã®é«˜ã„é †ã«
            pair_sims.sort(key=lambda x: x[2], reverse=True)
            a, b, s = pair_sims[0]
            # ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
            la = stream_label_map.get(a, "")
            lb = stream_label_map.get(b, "")
            best_pair_str = f"{a}:{la} vs {b}:{lb}, sim={s:.2f}"
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºé¡ä¼¼åº¦ï¼ˆ1âˆ’JSï¼‰ã‚’åˆ—ã«å±•é–‹ï¼ˆç°¡æ½”ãªåˆ—å: "A vs B"ï¼‰
        sim_row: Dict[str, object] = {"group_id": gid, "bin_id": bin_id}
        for i in range(n):
            for j in range(i+1, n):
                base_i = os.path.basename(keys[i]).replace('.csv','')
                base_j = os.path.basename(keys[j]).replace('.csv','')
                name = f"{base_i} vs {base_j}"
                sim_val = float(max(0.0, min(1.0, 1.0 - dmat[i, j])))
                sim_row[name] = sim_val
        # å¾Œæ®µã®æ³¨é‡ˆåˆ—ã¯ presence_row ã¨æƒãˆã‚‹
        # å¹³å‡ã‚’è¨ˆç®—
        tril = dmat[np.tril_indices(n, k=-1)]
        avg_lex = float(np.mean(tril)) if tril.size else 0.0
        tril_lang = lang_mat[np.tril_indices(n, k=-1)]
        avg_lang = float(np.mean(tril_lang)) if tril_lang.size else 0.0
        tril_emoji = emoji_mat[np.tril_indices(n, k=-1)]
        avg_emoji = float(np.mean(tril_emoji)) if tril_emoji.size else 0.0
        result: Dict[str, object] = {
            "group_id": gid,
            "bin_id": bin_id,
            "label": label,
            "avg_js_distance": avg_lex,
            "avg_language_distance": avg_lang,
            "avg_emoji_difference": avg_emoji,
        }
        # å„ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§ã®è©²å½“ã‚¤ãƒ™ãƒ³ãƒˆã® bin_id ã¨ label ã‚’è¨˜éŒ²ã™ã‚‹
        # å­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºç™½ã«ã™ã‚‹
        for sk in streams.keys():
            base = os.path.basename(sk)
            if sk in evts_dict:
                evt_info = evts_dict[sk]
                # ãã®ã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã§ã®binã‚„ãƒ©ãƒ™ãƒ«ã‚’ä¿å­˜
                result[f"{base}_bin"] = int(evt_info.get("bin_id", -1))
                result[f"{base}_label"] = str(evt_info.get("label", ""))
            else:
                result[f"{base}_bin"] = ""
                result[f"{base}_label"] = ""
        # å„ãƒšã‚¢ã®è©³ç´°ã‚’è¿½åŠ 
        # ãƒšã‚¢ã”ã¨ã®è©³ç´°ã‚’è¿½åŠ ï¼ˆlex/lang/emojiã«åŠ ãˆ sentiment/styleï¼‰
        pair_rows = []
        for i in range(n):
            for j in range(i+1, n):
                name = f"{os.path.basename(keys[i])} vs {os.path.basename(keys[j])}"
                lex = float(dmat[i, j])
                langd = float(lang_mat[i, j])
                emj = float(emoji_mat[i, j])
                result[f"{name} (lex)"] = lex
                result[f"{name} (lang)"] = langd
                result[f"{name} (emoji)"] = emj
                # sentiment/style è·é›¢ï¼ˆå¹³å‡çµ¶å¯¾å·®ï¼‰
                sdist = style_distance(sentiments[keys[i]], sentiments[keys[j]])
                tdist = style_distance(styles[keys[i]], styles[keys[j]])
                result[f"{name} (sentiment)"] = float(sdist)
                result[f"{name} (style)"] = float(tdist)
                pair_rows.append({
                    "group_id": gid,
                    "bin_id": bin_id,
                    "pair": name,
                    "lex_js": lex,
                    "lang_js": langd,
                    "emoji_diff": emj,
                    "sentiment_dist": float(sdist),
                    "style_dist": float(tdist),
                })
        matched_results.append(result)
        # æ³¨é‡ˆæƒ…å ±ã‚’presence_rowã«è¿½åŠ ï¼ˆå¯è¦–åŒ–æ™‚ã®è¡Œãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨ï¼‰
        if time_label:
            presence_row["time_label"] = time_label
        # å„é…ä¿¡ã®ãƒ©ãƒ™ãƒ«ã‚‚è¡Œã«å«ã‚ã¦ãŠãï¼ˆCSVã«æ®‹ã™ãŸã‚ï¼‰
        for base, lab in stream_label_map.items():
            presence_row[f"{base}_label"] = lab
        if time_label and best_pair_str:
            presence_row["row_label"] = f"{time_label}, {best_pair_str}"
        else:
            # çŸ­ã„ãƒ©ãƒ™ãƒ«ï¼ˆå‚åŠ é…ä¿¡ã®ä¸Šä½èªã‹ã‚‰ä»£è¡¨ã‚’æ§‹æˆï¼‰
            short_all = []
            for v in stream_words_map.values():
                for w in str(v).split(","):
                    if w and w not in short_all:
                        short_all.append(w)
            short_join = ",".join(short_all[:3]) if short_all else str(label)
            presence_row["row_label"] = f"{time_label}, {short_join}" if time_label else short_join
        matched_presence.append(presence_row)
        # similarity è¡Œã«ã‚‚æ³¨é‡ˆã‚’ä»˜ä¸
        if time_label:
            sim_row["time_label"] = time_label
        sim_row["row_label"] = presence_row["row_label"]
        matched_similarity.append(sim_row)
        # è¡Œãƒ¡ã‚¿: å‚åŠ è€…æ•°ã¨ã‚³ãƒ¡ãƒ³ãƒˆç·æ•°
        participants = int(sum(1 for sk in streams.keys() if sk in evts_dict))
        total_comments = int(sum(len(comments_by_stream.get(sk, [])) for sk in streams.keys()))
        matched_meta.append({
            "group_id": gid,
            "bin_id": bin_id,
            "participants": participants,
            "total_comments": total_comments,
            "time_label": time_label,
            "row_label": presence_row.get("row_label", ""),
        })

        # ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹ã‚’ä¿å­˜ï¼ˆç©ºã®é…åˆ—ã‚‚å«ã‚€ï¼‰
        evt_key = f"gid{gid}_bin{bin_id}"
        matched_comments[evt_key] = {}
        for sk in streams.keys():
            # ä¿å­˜ã™ã‚‹éš›ã¯ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã‚’ã‚­ãƒ¼ã¨ã™ã‚‹
            matched_comments[evt_key][os.path.basename(sk)] = comments_by_stream.get(sk, [])

        # ã‚¤ãƒ™ãƒ³ãƒˆå…¨ä½“ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä½¿ã£ã¦ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ
        # å„ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’çµåˆã—ã¦1ã¤ã®ãƒªã‚¹ãƒˆã«
        aggregated_comments: List[str] = []
        for _sk, comms in comments_by_stream.items():
            aggregated_comments.extend(comms)
        # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        all_wc_dir = os.path.join(OUT_DIR, "wordclouds_matched", f"gid{gid}_bin{bin_id}")
        os.makedirs(all_wc_dir, exist_ok=True)
        wc_path = os.path.join(all_wc_dir, f"WC_ALL.png")
        try:
            make_wordcloud(aggregated_comments, wc_path)
        except Exception as e:
            print(f"[WARN] wordcloud failed for aggregated event gid{gid}_bin{bin_id}: {e}")

        # --- è©³ç´°æƒ…å ±ã‚’åé›†: å„é…ä¿¡è€…ã”ã¨ã®binç¯„å›²ã‚„ãƒ©ãƒ™ãƒ«ãªã© ---
        # event_id ã‚’gidã¨bin_idã«åŸºã¥ã„ã¦ç”Ÿæˆ
        event_identifier = f"gid{gid}_bin{bin_id}"
        for sk in streams.keys():
            base_name = os.path.basename(sk)
            if sk in evts_dict:
                evt_info = evts_dict[sk]
                stream_obj = streams[sk]
                # binå¢ƒç•Œã‚’å–å¾—
                bins = build_relative_time_bins(stream_obj.df_valid["timestamp"], stream_obj.nr_bins)
                b = int(evt_info.get("bin_id", -1))
                if 0 <= b < len(bins):
                    interval = bins[b]
                    t0 = stream_obj.df_valid["timestamp"].min()
                    # ç›¸å¯¾ç§’
                    bin_start_sec = int((interval.left - t0).total_seconds())
                    bin_end_sec = int((interval.right - t0).total_seconds())
                else:
                    bin_start_sec = None
                    bin_end_sec = None
                gid_local = int(evt_info.get("group_id", -1))
                label_local = str(evt_info.get("label", ""))
                top_words_local = stream_obj.group_top_words.get(gid_local, [])[:5]
                matched_details_all.append({
                    "event_id": event_identifier,
                    "stream": base_name,
                    "bin_id": b,
                    "bin_start_sec": bin_start_sec if bin_start_sec is not None else "",
                    "bin_end_sec": bin_end_sec if bin_end_sec is not None else "",
                    "label": label_local,
                    "top_words": " ".join(top_words_local),
                })
            else:
                # ã“ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«ã¯è©²å½“ã‚¤ãƒ™ãƒ³ãƒˆãŒå­˜åœ¨ã—ãªã„
                matched_details_all.append({
                    "event_id": event_identifier,
                    "stream": base_name,
                    "bin_id": "",
                    "bin_start_sec": "",
                    "bin_end_sec": "",
                    "label": "",
                    "top_words": "",
                })
    # ä¿å­˜
    if matched_results:
        matched_df = pd.DataFrame(matched_results)
        out_csv2 = os.path.join(OUT_DIR, "matched_event_comparison_results.csv")
        matched_df.to_csv(out_csv2, index=False, encoding="utf-8-sig")
        print(f"Saved matched events results: {out_csv2}")
        # presence CSV ã¯ä¿å­˜ï¼ˆå¾“æ¥ã® 0/1 æƒ…å ±ï¼‰
        presence_df = pd.DataFrame(matched_presence)
        # presence å´ã‚‚åŒã˜ä¸Šä½ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã—ã¦ã€è¦‹ãŸç›®ã¨CSVã®æ•´åˆã‚’å–ã‚‹
        try:
            top_n = int(getattr(args, "top_matched", 5) or 0)
        except Exception:
            top_n = 5
        if top_n and not presence_df.empty and 'group_id' in presence_df.columns and 'bin_id' in presence_df.columns:
            meta_df2 = pd.DataFrame(matched_meta)
            order_keys = (meta_df2.sort_values(["total_comments","participants"], ascending=[False, False])
                                   [["group_id","bin_id"]].apply(tuple, axis=1).tolist()[:top_n])
            keep_idx = set(order_keys)
            presence_df = presence_df[presence_df.apply(lambda r: (r.get("group_id"), r.get("bin_id")) in keep_idx, axis=1)]
        # ä¿å­˜ç”¨CSVã¯å…ƒã®å½¢ã§å‡ºåŠ›
        pres_csv = os.path.join(OUT_DIR, "matched_event_presence.csv")
        presence_df.to_csv(pres_csv, index=False, encoding="utf-8-sig")
        # å¯è¦–åŒ–ã¯ã€Œãƒšã‚¢ãƒ¯ã‚¤ã‚ºé¡ä¼¼åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆ1âˆ’JSï¼‰ã€ã‚’ä½¿ç”¨
        sim_df = pd.DataFrame(matched_similarity)
        meta_df = pd.DataFrame(matched_meta)
        # ä¸Šä½ãƒ•ã‚£ãƒ«ã‚¿: ã‚³ãƒ¡ãƒ³ãƒˆç·æ•°ã®å¤šã„é †ã« --top-matched ä»¶ã¾ã§
        try:
            top_n = int(getattr(args, "top_matched", 5) or 0)
        except Exception:
            top_n = 5
        if top_n and not meta_df.empty:
            order_keys = (meta_df.sort_values(["total_comments","participants"], ascending=[False, False])
                                 [["group_id","bin_id"]].apply(tuple, axis=1).tolist()[:top_n])
            keep_idx = set(order_keys)
            # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
            sim_df = sim_df[sim_df.apply(lambda r: (r.get("group_id"), r.get("bin_id")) in keep_idx, axis=1)]
            meta_df = meta_df[meta_df.apply(lambda r: (r.get("group_id"), r.get("bin_id")) in keep_idx, axis=1)]
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ãƒ©ãƒ™ãƒ«ã‚’è¨­å®š
        if "row_label" in sim_df.columns:
            sim_df_for_plot = sim_df.set_index("row_label")
        else:
            sim_df_for_plot = sim_df
        sim_df_for_plot = sim_df_for_plot.drop(columns=["group_id","bin_id","time_label","row_label"], errors="ignore")
        # åˆ—ã‚’å¹³å‡é¡ä¼¼åº¦ã®é«˜ã„é †ã«ä¸¦ã¹æ›¿ãˆ
        if not sim_df_for_plot.empty:
            col_order = sim_df_for_plot.mean(axis=0).sort_values(ascending=False).index.tolist()
            sim_df_for_plot = sim_df_for_plot[col_order]
        # ãƒ—ãƒ­ãƒƒãƒˆç”¨CSVã¨PNGã®å‡ºåŠ›å…ˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã¯äº’æ›ã®ãŸã‚æ®ãˆç½®ãï¼‰
        pres_csv_plot = os.path.join(OUT_DIR, "matched_event_presence_plot.csv")
        pres_png = os.path.join(OUT_DIR, "matched_event_presence.png")
        save_csv_and_png_heatmap(
            sim_df_for_plot,
            pres_csv_plot,
            pres_png,
            title="Pairwise Topic Similarity for Matched Events (1âˆ’JS)"
        )
        # ä¸€è‡´ã‚¤ãƒ™ãƒ³ãƒˆã®ã‚µãƒãƒªãƒ¼ï¼ˆè¦‹ã‚„ã™ã„ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰ã‚‚ä¿å­˜
        try:
            summary_png = os.path.join(OUT_DIR, "matched_event_summary.png")
            _save_matched_summary_table(presence_df, matched_df, summary_png, top_k=20)
            print(f"Saved matched events summary: {summary_png}")
        except Exception as e:
            print(f"[WARN] failed to save matched events summary: {e}")
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæŒ‡æ¨™ã‚’CSV/PNGã§ä¿å­˜
        try:
            if pair_rows:
                pair_df = pd.DataFrame(pair_rows)
                pair_dir = os.path.join(OUT_DIR, "matched_event_pairs")
                os.makedirs(pair_dir, exist_ok=True)
                pair_csv = os.path.join(pair_dir, f"gid{gid}_bin{bin_id}_pairs.csv")
                pair_df.to_csv(pair_csv, index=False, encoding="utf-8-sig")
                # ç°¡æ˜“ãƒãƒ¼å›³: å„è·é›¢ã®å¹³å‡
                agg = pair_df[["lex_js","lang_js","emoji_diff","sentiment_dist","style_dist"]].mean()
                plt.figure(figsize=(6,4))
                plt.bar(agg.index, agg.values)
                plt.xticks(rotation=45, ha="right")
                plt.title(f"gid{gid}_bin{bin_id} pairwise metrics (avg)")
                plt.tight_layout()
                plt.savefig(os.path.join(pair_dir, f"gid{gid}_bin{bin_id}_pairs.png"), dpi=200)
                plt.close()
        except Exception as e:
            print(f"[WARN] failed to save pairwise metrics: {e}")

        # JSONã«ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹ã‚’ä¿å­˜
        comments_json_path = os.path.join(OUT_DIR, "matched_event_comments.json")
        try:
            with open(comments_json_path, "w", encoding="utf-8") as f:
                json.dump(matched_comments, f, ensure_ascii=False, indent=2)
            print(f"Saved matched event comments: {comments_json_path}")
        except Exception as e:
            print(f"[WARN] failed to save matched event comments: {e}")
        # è©³ç´°CSVã‚‚ä¿å­˜
        if matched_details_all:
            details_df = pd.DataFrame(matched_details_all)
            details_csv = os.path.join(OUT_DIR, "matched_event_details.csv")
            details_df.to_csv(details_csv, index=False, encoding="utf-8-sig")
            print(f"Saved matched event details: {details_csv}")
            # PNGã¨ã—ã¦ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¿å­˜
            details_png = os.path.join(OUT_DIR, "matched_event_details.png")
            try:
                save_df_as_table_png(details_df, details_png, title="Matched Event Details")
                print(f"Saved matched event details PNG: {details_png}")
            except Exception as e:
                print(f"[WARN] Failed to save matched event details PNG: {e}")
    else:
        print("[INFO] No matched events on exact group & bin across streams.")

    # === æ–°æ©Ÿèƒ½: é¡ä¼¼ãƒˆãƒ”ãƒƒã‚¯ Ã— æ™‚é–“ãŒè¿‘ã„ã‚¤ãƒ™ãƒ³ãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ é–“ã§ç…§åˆã—æ¯”è¼ƒ ===
    # `match_events_across_streams` ã‚’åˆ©ç”¨ã—ã¦ã€Jaccard é¡ä¼¼åº¦ã¨æ™‚é–“å·®ã«åŸºã¥ãã‚¤ãƒ™ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    print("Matching events across streams by topic similarity and time ...")
    similar_event_map = match_events_across_streams(events_by_stream, args.word_match_th, args.time_match_th)
    similar_results = []
    similar_presence = []
    similar_comments: Dict[int, Dict[str, List[str]]] = {}
    # é¡ä¼¼ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°æƒ…å ±ã®ä¿å­˜ç”¨ãƒªã‚¹ãƒˆ
    similar_details_all: List[Dict[str, object]] = []
    # Build mapping from event_id to list of (stream_key, event)
    events_by_sim_id: Dict[int, Dict[str, Dict[str, object]]] = defaultdict(dict)
    for stream_key, evts in events_by_stream.items():
        for i, evt in enumerate(evts):
            eid = similar_event_map.get((stream_key, i))
            if eid is None:
                continue
            events_by_sim_id[eid][stream_key] = evt
    for sim_id, evts_dict in events_by_sim_id.items():
        # skip if less than two streams participate
        if len(evts_dict) < 2:
            continue
        # Determine representative label (concatenate top words across streams)
        # Use first stream's label as base
        labels = []
        for evt in evts_dict.values():
            if evt.get("label"):
                labels.append(evt["label"])
        label = labels[0] if labels else f"event_{sim_id}"
        # Collect comments and languages per stream
        comments_by_stream: Dict[str, List[str]] = {}
        langs_by_stream: Dict[str, List[str]] = {}
        presence_row: Dict[str, int] = {}
        for sk in streams.keys():
            if sk in evts_dict:
                comments, langs = extract_event_comments(streams[sk], evts_dict[sk], args.peak_pad)
                comments_by_stream[sk] = comments
                langs_by_stream[sk] = langs
                presence_row[os.path.basename(sk)] = 1
            else:
                comments_by_stream[sk] = []
                langs_by_stream[sk] = []
                presence_row[os.path.basename(sk)] = 0
        # Compute distance matrices
        keys = list(streams.keys())
        n = len(keys)
        dmat = np.zeros((n, n), dtype=float)
        lang_mat = np.zeros((n, n), dtype=float)
        emoji_mat = np.zeros((n, n), dtype=float)
        for i in range(n):
            for j in range(i+1, n):
                d = compute_lexical_distance(comments_by_stream[keys[i]], comments_by_stream[keys[j]])
                dmat[i, j] = d; dmat[j, i] = d
                ld = js_distance_distribution(
                    compute_language_distribution(langs_by_stream[keys[i]]),
                    compute_language_distribution(langs_by_stream[keys[j]])
                )
                lang_mat[i, j] = ld; lang_mat[j, i] = ld
                ediff = abs(
                    compute_emoji_ratio(comments_by_stream[keys[i]]) - compute_emoji_ratio(comments_by_stream[keys[j]])
                )
                emoji_mat[i, j] = ediff; emoji_mat[j, i] = ediff
        # Compute averages
        tril = dmat[np.tril_indices(n, k=-1)]
        avg_lex = float(np.mean(tril)) if tril.size else 0.0
        tril_lang = lang_mat[np.tril_indices(n, k=-1)]
        avg_lang = float(np.mean(tril_lang)) if tril_lang.size else 0.0
        tril_emoji = emoji_mat[np.tril_indices(n, k=-1)]
        avg_emoji = float(np.mean(tril_emoji)) if tril_emoji.size else 0.0
        result = {
            "sim_event_id": sim_id,
            "label": label,
            "avg_js_distance": avg_lex,
            "avg_language_distance": avg_lang,
            "avg_emoji_difference": avg_emoji,
        }
        for i in range(n):
            for j in range(i+1, n):
                name = f"{os.path.basename(keys[i])} vs {os.path.basename(keys[j])}"
                result[f"{name} (lex)"] = float(dmat[i, j])
                result[f"{name} (lang)"] = float(lang_mat[i, j])
                result[f"{name} (emoji)"] = float(emoji_mat[i, j])
        similar_results.append(result)
        similar_presence.append({"sim_event_id": sim_id, "label": label, **presence_row})
        # Save comments
        similar_comments[sim_id] = {os.path.basename(sk): comments_by_stream[sk] for sk in streams.keys()}
        # Generate aggregated wordcloud for this similar event
        aggregated_comments: List[str] = []
        for comms in comments_by_stream.values():
            aggregated_comments.extend(comms)
        all_wc_dir = os.path.join(OUT_DIR, "wordclouds_similar", f"event_{sim_id}")
        os.makedirs(all_wc_dir, exist_ok=True)
        wc_path = os.path.join(all_wc_dir, f"WC_ALL.png")
        try:
            make_wordcloud(aggregated_comments, wc_path)
        except Exception as e:
            print(f"[WARN] wordcloud failed for similar event {sim_id}: {e}")
        # Also per-stream wordclouds
        for sk, comms in comments_by_stream.items():
            wc_stream_path = os.path.join(all_wc_dir, f"WC_{os.path.basename(sk).replace('.csv','')}.png")
            try:
                make_wordcloud(comms, wc_stream_path)
            except Exception as e:
                print(f"[WARN] wordcloud failed for similar event {sim_id}, stream {sk}: {e}")

        # --- è©³ç´°æƒ…å ±ã‚’åé›†: å„é…ä¿¡è€…ã”ã¨ã®binç¯„å›²ã‚„ãƒ©ãƒ™ãƒ«ãªã© ---
        for sk in streams.keys():
            base_name = os.path.basename(sk)
            if sk in evts_dict:
                evt_info = evts_dict[sk]
                stream_obj = streams[sk]
                bins = build_relative_time_bins(stream_obj.df_valid["timestamp"], stream_obj.nr_bins)
                b = int(evt_info.get("bin_id", -1))
                if 0 <= b < len(bins):
                    interval = bins[b]
                    t0 = stream_obj.df_valid["timestamp"].min()
                    bin_start_sec = int((interval.left - t0).total_seconds())
                    bin_end_sec = int((interval.right - t0).total_seconds())
                else:
                    bin_start_sec = None
                    bin_end_sec = None
                gid_local = int(evt_info.get("group_id", -1))
                label_local = str(evt_info.get("label", ""))
                top_words_local = stream_obj.group_top_words.get(gid_local, [])[:5]
                similar_details_all.append({
                    "sim_event_id": sim_id,
                    "stream": base_name,
                    "bin_id": b,
                    "bin_start_sec": bin_start_sec if bin_start_sec is not None else "",
                    "bin_end_sec": bin_end_sec if bin_end_sec is not None else "",
                    "label": label_local,
                    "top_words": " ".join(top_words_local),
                })
            else:
                similar_details_all.append({
                    "sim_event_id": sim_id,
                    "stream": base_name,
                    "bin_id": "",
                    "bin_start_sec": "",
                    "bin_end_sec": "",
                    "label": "",
                    "top_words": "",
                })
    # Save similar event results
    if similar_results:
        similar_df = pd.DataFrame(similar_results)
        csv_path = os.path.join(OUT_DIR, "similar_event_comparison_results.csv")
        similar_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        print(f"Saved similar events results: {csv_path}")
        # Save presence and heatmap
        pres_df = pd.DataFrame(similar_presence)
        pres_csv = os.path.join(OUT_DIR, "similar_event_presence.csv")
        pres_df.to_csv(pres_csv, index=False, encoding="utf-8-sig")
        # For heatmap, set index to label
        pres_df_plot = pres_df.set_index("label")
        pres_df_plot = pres_df_plot.drop(columns=["sim_event_id", "label"], errors="ignore")
        pres_csv_plot = os.path.join(OUT_DIR, "similar_event_presence_plot.csv")
        pres_png = os.path.join(OUT_DIR, "similar_event_presence.png")
        save_csv_and_png_heatmap(pres_df_plot, pres_csv_plot, pres_png, title="Similar Events Presence (1=present)")
        # Save comments JSON
        comments_json = os.path.join(OUT_DIR, "similar_event_comments.json")
        with open(comments_json, "w", encoding="utf-8") as f:
            json.dump(similar_comments, f, ensure_ascii=False, indent=2)
        print(f"Saved similar event comments: {comments_json}")
        # é¡ä¼¼ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°CSVã‚’ä¿å­˜
    if similar_details_all:
            sim_details_df = pd.DataFrame(similar_details_all)
            sim_details_csv = os.path.join(OUT_DIR, "similar_event_details.csv")
            sim_details_df.to_csv(sim_details_csv, index=False, encoding="utf-8-sig")
            print(f"Saved similar event details: {sim_details_csv}")
            # PNGã¨ã—ã¦ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¿å­˜
            sim_details_png = os.path.join(OUT_DIR, "similar_event_details.png")
            try:
                save_df_as_table_png(sim_details_df, sim_details_png, title="Similar Event Details")
                print(f"Saved similar event details PNG: {sim_details_png}")
            except Exception as e:
                print(f"[WARN] Failed to save similar event details PNG: {e}")
    else:
        print("[INFO] No similar events matched across streams under current thresholds.")

    # === Emojiãƒ©ãƒ³ã‚­ãƒ³ã‚°: å„CSVã”ã¨ã«çµµæ–‡å­—ã®å‡ºç¾é »åº¦ã‚’é›†è¨ˆã—ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¨æ£’ã‚°ãƒ©ãƒ•ã‚’å‡ºåŠ› ===
    # message_clean ã§ã¯çµµæ–‡å­—ãŒé™¤å»ã•ã‚Œã¦ã—ã¾ã†ãŸã‚ã€å…ƒã® message åˆ—ã‹ã‚‰çµµæ–‡å­—ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    try:
        emoji_dir = os.path.join(OUT_DIR, "emoji_rankings")
        os.makedirs(emoji_dir, exist_ok=True)
        rankings_rows: List[Dict[str, object]] = []
        for stream_key, sd in streams.items():
            # stream_key ã¯CSVã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚å…ƒã®ã‚³ãƒ¡ãƒ³ãƒˆå…¨ä½“ã‚’èª­ã¿å‡ºã—ã¦çµµæ–‡å­—ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹
            msgs: List[str] = []
            # ã¾ãšå…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿å‡ºã™
            try:
                full_df = read_csv_any(stream_key)
                if "message" in full_df.columns:
                    msgs = full_df["message"].astype(str).tolist()
            except Exception:
                # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãŒèª­ã‚ãªã„å ´åˆã¯ã€df_valid ã® message åˆ—ã§ä»£ç”¨
                try:
                    msgs = sd.df_valid.get("message", pd.Series([], dtype=str)).astype(str).tolist()
                except Exception:
                    msgs = []
            # çµµæ–‡å­—ã®å‡ºç¾æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            emoji_counter: Dict[str, int] = defaultdict(int)
            for txt in msgs:
                if not isinstance(txt, str):
                    continue
                for ch in txt:
                    if is_emoji(ch):
                        emoji_counter[ch] += 1
            # ä¸Šä½10ä»¶ã®çµµæ–‡å­—ã¨é »åº¦ã‚’å–å¾—
            top_emojis = sorted(emoji_counter.items(), key=lambda x: x[1], reverse=True)[:10]
            # æ£’ã‚°ãƒ©ãƒ•ã‚ã‚‹ã„ã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä»˜ãå›³ã‚’ä½œæˆ
            plt.figure(figsize=(8, 4))
            if top_emojis:
                emojis, freq = zip(*top_emojis)
                plt.bar(range(len(emojis)), freq)
                # Use actual emojis as labels and supply emoji-supporting font if available
                emoji_font_prop = None
                try:
                    if _emoji_font_path:
                        emoji_font_prop = font_manager.FontProperties(fname=_emoji_font_path)
                except Exception:
                    emoji_font_prop = None
                plt.xticks(range(len(emojis)), list(emojis), fontsize=14, rotation=0, fontproperties=emoji_font_prop)
                plt.xlabel("Emoji")
                plt.ylabel("Frequency")
                plt.title(f"Top Emojis in {os.path.basename(stream_key)}")
            else:
                # 1ã¤ã‚‚çµµæ–‡å­—ãŒãªã‹ã£ãŸå ´åˆã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                plt.text(0.5, 0.5, "No emojis found", ha='center', va='center', fontsize=14)
                plt.axis('off')
                plt.title(f"Top Emojis in {os.path.basename(stream_key)}")
            plt.tight_layout()
            chart_fname = f"emoji_ranking_{os.path.basename(stream_key).replace('.csv','')}.png"
            chart_path = os.path.join(emoji_dir, chart_fname)
            plt.savefig(chart_path, dpi=200)
            plt.close()
            # Print saved chart and build ranking row
            print(f"Saved emoji ranking chart: {chart_path}")
            rank_data: Dict[str, object] = {"stream": os.path.basename(stream_key)}
            for idx, (emo, cnt) in enumerate(top_emojis, start=1):
                rank_data[f"emoji_{idx}"] = emo
                rank_data[f"freq_{idx}"] = cnt
            rankings_rows.append(rank_data)
        # After iterating all streams, save rankings CSV
        if rankings_rows:
            emoji_csv = os.path.join(emoji_dir, "emoji_rankings.csv")
            pd.DataFrame(rankings_rows).to_csv(emoji_csv, index=False, encoding="utf-8-sig")
            print(f"Saved emoji rankings CSV: {emoji_csv}")
    except Exception as e:
        print(f"[WARN] Failed to compute emoji rankings: {e}")

    # === Emoji ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³: å„é…ä¿¡ã”ã¨ã«æ™‚é–“å¸¯Ã—çµµæ–‡å­—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’å‡ºåŠ› ===
    try:
        topk_emoji = int(getattr(args, "emoji_topk", 10) or 10)
        et_dir = os.path.join(OUT_DIR, "emoji_timelines")
        os.makedirs(et_dir, exist_ok=True)
        for stream_key, sd in streams.items():
            # å…ƒã® message ã¨ timestamp ã‚’èª­ã¿è¾¼ã¿
            try:
                df_full = read_csv_any(stream_key)
            except Exception:
                df_full = sd.df_valid.copy()
            if "timestamp" not in df_full.columns or "message" not in df_full.columns:
                continue
            ts = pd.to_datetime(df_full["timestamp"], errors="coerce", utc=True).dt.tz_localize(None)
            msgs = df_full["message"].astype(str).tolist()
            # æ™‚é–“binï¼ˆ5åˆ†å˜ä½ç›¸å½“: nr_bins ã§åˆ†ã‘ãŸBE RTopicã®binsã¨ã¯åˆ¥ã«ã€å˜ç´”åŒ–ã— 20åŒºåˆ†ï¼‰
            # ã“ã“ã§ã¯ã€ã‚¹ãƒˆãƒªãƒ¼ãƒ å…¨ä½“ã‚’ sd.nr_bins ã¨åŒã˜ãƒ“ãƒ³ã«åˆ†å‰²ã—ã¦æ•´åˆ
            bins = build_relative_time_bins(ts.dropna(), sd.nr_bins)
            # å„binÃ—emojiã®ã‚«ã‚¦ãƒ³ãƒˆ
            # ã¾ãšå…¨emojiã®ç·æ•°ã§ä¸Šä½ topk ã‚’é¸ã¶
            total_emoji_counter: Dict[str, int] = defaultdict(int)
            for txt in msgs:
                for ch in str(txt):
                    if is_emoji(ch):
                        total_emoji_counter[ch] += 1
            top_emojis = [e for e, _ in sorted(total_emoji_counter.items(), key=lambda x: x[1], reverse=True)[:topk_emoji]]
            if not top_emojis:
                continue
            # binå‰²å½“ã¨ã‚«ã‚¦ãƒ³ãƒˆ
            # DataFrameã«ã¾ã¨ã‚ã¦é«˜é€ŸåŒ–
            df_tmp = pd.DataFrame({"timestamp": ts, "message": msgs}).dropna(subset=["timestamp"]).reset_index(drop=True)
            # å„è¡Œã‹ã‚‰å€™è£œã®çµµæ–‡å­—ã®ã¿æŠ½å‡º
            def _extract_emojis(s: str) -> List[str]:
                return [ch for ch in str(s) if ch in top_emojis and is_emoji(ch)]
            df_tmp["emojis"] = df_tmp["message"].apply(_extract_emojis)
            if df_tmp["emojis"].map(len).sum() == 0:
                continue
            # å„è¡Œã®bin id ã‚’æ±ºå®š
            def _find_bin(t: pd.Timestamp) -> int:
                for i, iv in enumerate(bins):
                    if t >= iv.left and t < iv.right:
                        return i
                centers = np.array([iv.left.value for iv in bins], dtype=np.int64)
                return int(np.argmin(np.abs(centers - int(t.value))))
            df_tmp["bin_id"] = df_tmp["timestamp"].apply(_find_bin)
            # æ™‚é–“ãƒ©ãƒ™ãƒ«ï¼ˆHH:MM ä¸­å¤®ï¼‰ã‚’ç”¨æ„
            time_labels: Dict[int, str] = {}
            for i, iv in enumerate(bins):
                center_ts = iv.left + (iv.right - iv.left)/2
                try:
                    time_labels[i] = pd.Timestamp(center_ts).strftime("%H:%M")
                except Exception:
                    time_labels[i] = str(i)
            # é›†è¨ˆ: binÃ—emoji
            rows = []
            for _, r in df_tmp.iterrows():
                if not r["emojis"]:
                    continue
                b = int(r["bin_id"])
                for ch in r["emojis"]:
                    rows.append({"bin_id": b, "emoji": ch, "cnt": 1})
            if not rows:
                continue
            edf = pd.DataFrame(rows)
            pivot = edf.pivot_table(index="bin_id", columns="emoji", values="cnt", aggfunc="sum", fill_value=0)
            pivot = pivot.reindex(range(len(bins)), fill_value=0)
            pivot.index = [time_labels.get(i, str(i)) for i in pivot.index]
            # å‡ºåŠ›
            base = os.path.basename(stream_key).replace('.csv','')
            out_csv_t = os.path.join(et_dir, f"emoji_timeline_{base}.csv")
            out_png_t = os.path.join(et_dir, f"emoji_timeline_{base}.png")
            save_emoji_timeline_heatmap(pivot, out_csv_t, out_png_t, title=f"Emoji Timeline: {base}")
    except Exception as e:
        print(f"[WARN] Failed to build emoji timelines: {e}")

def save_df_as_table_png(df: pd.DataFrame, out_png: str, title: str = "") -> None:
    """Save a DataFrame as a table PNG. Adjust figure size based on number of rows and columns."""
    if df is None or df.empty:
        # nothing to save
        return
    # Determine figure size heuristically
    n_rows, n_cols = df.shape
    # base sizes with caps
    width = min(20, 1 + 0.6 * n_cols)
    height = min(20, 1 + 0.3 * n_rows)
    fig, ax = plt.subplots(figsize=(width, height))
    ax.axis('off')
    # build table
    # Convert values to strings to avoid potential formatting issues
    cell_text = [[str(x) for x in row] for row in df.values]
    table = ax.table(cellText=cell_text, colLabels=df.columns, loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(8)
    table.scale(1, 1.5)
    # Optionally set title
    if title:
        plt.title(title)
    plt.tight_layout()
    # Save
    plt.savefig(out_png, dpi=200)
    plt.close(fig)

if __name__ == "__main__":
    main()
