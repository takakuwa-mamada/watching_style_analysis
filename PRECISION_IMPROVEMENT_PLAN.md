# ğŸ“Š Event Comparison ç²¾åº¦å‘ä¸Šè¨ˆç”» (Paper Quality: 6/10 â†’ 8-9/10)

**ä½œæˆæ—¥**: 2025å¹´11æœˆ17æ—¥  
**ç›®æ¨™**: 2é€±é–“ä»¥å†…ã«å­¦ä¼šè«–æ–‡ãƒ¬ãƒ™ãƒ«ã®åˆ†æç²¾åº¦ã‚’é”æˆ  
**ç¾çŠ¶**: Paper Quality Assessment = 6/10 (ACCEPTABLE, but needs improvement)  
**ç›®æ¨™**: Paper Quality Assessment = 8-9/10 (PUBLICATION READY)

---

## ğŸ¯ Executive Summary

### ç¾åœ¨ã®å•é¡Œç‚¹åˆ†æ

#### 1. **çµ±è¨ˆçš„å•é¡Œ**
```
Topic Matching Analysis:
  - topic_jaccard = 0: 8/10 (80.0%) â† å•é¡Œ!
  - topic_jaccard > 0: 2/10 (20.0%)
  - Average topic_jaccard: 0.101 â† ä½ã™ãã‚‹!

Similarity Distribution:
  - Low (<0.5): 7/10 (70.0%) â† å¤šã™ãã‚‹!
  - High (>=0.7): 1/10 (10.0%) â† å°‘ãªã™ãã‚‹!
```

#### 2. **æ–¹æ³•è«–çš„å•é¡Œ**
- âŒ **N-gram extraction**: TF-IDFä¾å­˜ã§æ–‡è„ˆç„¡è¦–
- âŒ **BERTopic**: å°è¦æ¨¡ã‚¤ãƒ™ãƒ³ãƒˆ(6-84ã‚³ãƒ¡ãƒ³ãƒˆ)ã«éå‰°é©åˆ
- âŒ **Embedding threshold**: 0.7ã¯é«˜ã™ãã‚‹å¯èƒ½æ€§
- âŒ **Time binning**: 100 bins â†’ ãƒ‡ãƒ¼ã‚¿åˆ†æ•£ã€ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºä½ä¸‹

#### 3. **ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œ**
- âš ï¸ "kkkkkk", "www", "laugh laugh" ãªã©ãƒã‚¤ã‚ºãƒˆãƒ”ãƒƒã‚¯ãŒæ”¯é…çš„
- âš ï¸ å®Ÿè³ªçš„ãªè©¦åˆã‚¤ãƒ™ãƒ³ãƒˆ(ã‚´ãƒ¼ãƒ«ã€ãƒ•ã‚¡ã‚¦ãƒ«ç­‰)ã®æ¤œå‡ºä¸è¶³
- âš ï¸ ã‚¹ãƒˆãƒªãƒ¼ãƒ é–“ã®æ™‚åˆ»åŒæœŸç²¾åº¦æœªæ¤œè¨¼

---

## ğŸ“‹ Phase 1: å³åŠ¹æ€§ã®ã‚ã‚‹æ”¹å–„ (Week 1: Nov 17-24)

### 1.1 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– (Priority: ğŸ”´ CRITICAL)

**ç›®æ¨™**: Topic matchingç‡ã‚’20% â†’ 60%ä»¥ä¸Šã«å‘ä¸Š

#### A. Time Binning ã®æœ€é©åŒ–
```python
# ç¾çŠ¶
--time-bins 100  # â†’ ã‚¤ãƒ™ãƒ³ãƒˆãŒç´°åˆ†åŒ–ã•ã‚Œã™ã

# æ”¹å–„æ¡ˆ (Grid Searchå®Ÿæ–½)
time_bins_candidates = [20, 30, 50, 75, 100]
optimal_bins = find_optimal_bins(
    metric='f1_score',  # Precision vs Recall ã®ãƒãƒ©ãƒ³ã‚¹
    ground_truth='manual_annotation.csv'  # æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¿…è¦
)
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] `scripts/optimize_time_bins.py` ä½œæˆ
- [ ] æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: 3è©¦åˆÃ—10ã‚¤ãƒ™ãƒ³ãƒˆ = 30 ground truth events
- [ ] Grid Searchå®Ÿè¡Œ (5åˆ†Ã—5ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ = 25åˆ†)
- [ ] æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’READMEã«è¨˜è¼‰

**æœŸå¾…åŠ¹æœ**: Topic matching +20-30%

---

#### B. Embedding Threshold ã®èª¿æ•´
```python
# ç¾çŠ¶
--embedding-match-th 0.7  # å³ã—ã™ãã‚‹?

# æ”¹å–„æ¡ˆ (ROC Curveåˆ†æ)
thresholds = [0.50, 0.55, 0.60, 0.65, 0.70, 0.75]
optimal_th = find_optimal_threshold(
    metric='f1_score',
    ground_truth='manual_annotation.csv'
)
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] `scripts/optimize_embedding_threshold.py` ä½œæˆ
- [ ] ROC Curve, Precision-Recall Curveç”Ÿæˆ
- [ ] æœ€é©é–¾å€¤ã®ç†è«–çš„æ­£å½“åŒ– (è«–æ–‡ã§èª¬æ˜)

**æœŸå¾…åŠ¹æœ**: False Negativeå‰Šæ¸›ã€Recall +10-15%

---

#### C. Topic Jaccard Threshold ã®ç·©å’Œ
```python
# ç¾çŠ¶
--jaccard-th 0.6  # BERTopicãƒˆãƒ”ãƒƒã‚¯ã®é¡ä¼¼åº¦

# æ”¹å–„æ¡ˆ (æ®µéšçš„ç·©å’Œ)
jaccard_th_candidates = [0.3, 0.4, 0.5, 0.6]
# æ³¨: ãƒˆãƒ”ãƒƒã‚¯ãŒç•°ãªã£ã¦ã‚‚ã€embeddingé¡ä¼¼åº¦ãŒé«˜ã‘ã‚Œã°ãƒãƒƒãƒè¨±å¯
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] Jaccardé–¾å€¤ã®å½±éŸ¿åˆ†æ
- [ ] Embeddingå„ªå…ˆã®é‡ã¿ä»˜ã‘æ¤œè¨¼

**æœŸå¾…åŠ¹æœ**: Topic matching +15-20%

---

### 1.2 ãƒã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¼·åŒ– (Priority: ğŸŸ  HIGH)

**ç›®æ¨™**: æ„å‘³ã®ãªã„ãƒˆãƒ”ãƒƒã‚¯("kkk", "www")ã‚’æ’é™¤

#### A. ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰æ‹¡å¼µ
```python
# è¿½åŠ ã™ã¹ãã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
NOISE_PATTERNS = [
    r'^k{3,}$',           # kkkkkk
    r'^w{3,}$',           # wwwwww
    r'^laugh( laugh)*$',  # laugh laugh laugh
    r'^lol( lol)*$',      # lol lol lol
    r'^clap( clap)*$',    # clap clap clap
    r'^[0-9]+$',          # æ•°å­—ã®ã¿
    r'^[!]+$',            # æ„Ÿå˜†ç¬¦ã®ã¿
    r'^emoji_\w+$',       # Emojiå˜ä½“
]
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] `utils/noise_filter.py` ä½œæˆ
- [ ] event_comparison.py ã«çµ±åˆ
- [ ] ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰å¾Œã®æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

**æœŸå¾…åŠ¹æœ**: Topic quality +30-40%, Event coherenceå‘ä¸Š

---

#### B. æœ€å°ã‚³ãƒ¡ãƒ³ãƒˆæ•°é–¾å€¤
```python
# ç¾çŠ¶: 1ã‚³ãƒ¡ãƒ³ãƒˆã§ã‚‚ã‚¤ãƒ™ãƒ³ãƒˆåŒ–
MIN_COMMENTS_PER_EVENT = 5  # çµ±è¨ˆçš„ã«æœ‰æ„ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º

# ç†è«–çš„æ ¹æ‹ 
# - ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãŒå°‘ãªã„ã¨ãƒˆãƒ”ãƒƒã‚¯ãŒä¸å®‰å®š
# - ãƒã‚¤ã‚º(ãƒœãƒƒãƒˆã€ã‚¹ãƒ‘ãƒ )ã®å½±éŸ¿ã‚’å—ã‘ã‚„ã™ã„
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºå¾Œã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¿½åŠ 
- [ ] æœ€å°ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å½±éŸ¿åˆ†æ

**æœŸå¾…åŠ¹æœ**: False Positiveå‰Šæ¸› -20-30%

---

### 1.3 è©•ä¾¡æŒ‡æ¨™ã®è¿½åŠ  (Priority: ğŸŸ¡ MEDIUM)

**ç›®æ¨™**: Paper Quality Assessmentã®å®¢è¦³æ€§å‘ä¸Š

#### A. Ground Truth ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
```csv
# manual_annotation.csv (ä¾‹)
stream,timestamp,event_type,description,importance
Ja_abema,3413,goal,Japan scores first goal,HIGH
Bra,11037,celebration,Brazil fans celebrate,MEDIUM
UK,5447,controversy,Referee decision disputed,HIGH
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ (3è©¦åˆÃ—10ã‚¤ãƒ™ãƒ³ãƒˆ = 30ä»¶)
- [ ] ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãƒ¼é–“ä¿¡é ¼æ€§ (Cohen's Kappa â‰¥ 0.7)
- [ ] `scripts/validate_ground_truth.py` ä½œæˆ

**æ‰€è¦æ™‚é–“**: 2-3æ™‚é–“

---

#### B. å®šé‡çš„è©•ä¾¡æŒ‡æ¨™
```python
# è¿½åŠ ã™ã¹ãæŒ‡æ¨™
metrics = {
    'precision': TP / (TP + FP),
    'recall': TP / (TP + FN),
    'f1_score': 2 * (precision * recall) / (precision + recall),
    'mean_average_precision': MAP,  # Ranking quality
    'normalized_mutual_information': NMI,  # Clustering quality
    'silhouette_score': silhouette,  # Event separation
}
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] `scripts/evaluate_event_detection.py` ä½œæˆ
- [ ] Confusion Matrixç”Ÿæˆ
- [ ] è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ

**æœŸå¾…åŠ¹æœ**: è«–æ–‡ã®ä¿¡é ¼æ€§ +å¤§å¹…å‘ä¸Š

---

## ğŸ“‹ Phase 2: æ ¹æœ¬çš„ãªæ–¹æ³•è«–æ”¹å–„ (Week 2: Nov 25-Dec 1)

### 2.1 Hybrid Topic Modeling (Priority: ğŸ”´ CRITICAL)

**å•é¡Œ**: BERTopicãŒå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿(6-84ã‚³ãƒ¡ãƒ³ãƒˆ/event)ã«ä¸é©åˆ‡

#### A. LDA + BERTopic ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
```python
from sklearn.decomposition import LatentDirichletAllocation
from bertopic import BERTopic

def hybrid_topic_model(docs, n_topics=10):
    # Step 1: BERTopicã§Embedding
    embeddings = embedding_model.encode(docs)
    
    # Step 2: UMAPã§æ¬¡å…ƒå‰Šæ¸›
    umap_embeddings = umap.UMAP(n_components=5).fit_transform(embeddings)
    
    # Step 3: LDAã§ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º (å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã«å¼·ã„)
    vectorizer = CountVectorizer(max_features=500)
    doc_term_matrix = vectorizer.fit_transform(docs)
    lda = LatentDirichletAllocation(n_components=n_topics)
    lda_topics = lda.fit_transform(doc_term_matrix)
    
    # Step 4: ä¸¡è€…ã®çµæœã‚’çµ±åˆ
    combined_topics = merge_topics(bertopic_topics, lda_topics)
    return combined_topics
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] `utils/hybrid_topic_model.py` ä½œæˆ
- [ ] BERTopic vs LDA vs Hybrid æ€§èƒ½æ¯”è¼ƒ
- [ ] æœ€é©ãªçµ±åˆé‡ã¿æ±ºå®š (Î±_bert=0.6, Î±_lda=0.4)

**æœŸå¾…åŠ¹æœ**: Topic coherence +20-30%, å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ

---

### 2.2 Contextual N-gram Extraction (Priority: ğŸŸ  HIGH)

**å•é¡Œ**: TF-IDFãŒæ–‡è„ˆã‚’ç„¡è¦–ã€"kkk"ãªã©ãƒã‚¤ã‚ºã‚’æŠ½å‡º

#### A. BERT-based Keyphrase Extraction
```python
from keybert import KeyBERT

kw_model = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')

def extract_contextual_keyphrases(docs, top_n=5):
    # BERT embeddingãƒ™ãƒ¼ã‚¹ã®ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºæŠ½å‡º
    keyphrases = kw_model.extract_keywords(
        docs, 
        keyphrase_ngram_range=(1, 3),  # 1-3 word phrases
        stop_words='english',  # + æ—¥æœ¬èªã€ã‚¹ãƒšã‚¤ãƒ³èª
        top_n=top_n,
        diversity=0.5  # Max Marginal Relevance (å¤šæ§˜æ€§ç¢ºä¿)
    )
    return keyphrases
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] `pip install keybert` è¿½åŠ 
- [ ] å¤šè¨€èªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰æ•´å‚™
- [ ] TF-IDF vs KeyBERT æ¯”è¼ƒ

**æœŸå¾…åŠ¹æœ**: N-gramè³ª +40-50%, ãƒã‚¤ã‚ºå‰Šæ¸›

---

#### B. Named Entity Recognition (NER)
```python
import spacy

nlp_en = spacy.load('en_core_web_sm')
nlp_ja = spacy.load('ja_core_news_sm')

def extract_entities(docs, lang='ja'):
    nlp = nlp_ja if lang == 'ja' else nlp_en
    entities = []
    for doc in docs:
        doc_nlp = nlp(doc)
        entities.extend([
            ent.text for ent in doc_nlp.ents 
            if ent.label_ in ['PERSON', 'ORG', 'EVENT', 'GPE']
        ])
    return Counter(entities).most_common(10)
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] spaCy NERçµ±åˆ
- [ ] é¸æ‰‹åã€ãƒãƒ¼ãƒ åã®è‡ªå‹•æŠ½å‡º
- [ ] ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ©ãƒ™ãƒ«ç”Ÿæˆ

**æœŸå¾…åŠ¹æœ**: ã‚¤ãƒ™ãƒ³ãƒˆè§£é‡ˆæ€§ +å¤§å¹…å‘ä¸Š

---

### 2.3 æ™‚ç³»åˆ—åˆ†æã®å¼·åŒ– (Priority: ğŸŸ¡ MEDIUM)

#### A. Dynamic Time Warping (DTW)
```python
from dtaidistance import dtw

def compute_temporal_similarity(ts1, ts2):
    # æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é¡ä¼¼åº¦ (ä½ç›¸ãšã‚Œã«é ‘å¥)
    distance = dtw.distance(ts1, ts2)
    similarity = 1 / (1 + distance)
    return similarity
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] DTWãƒ™ãƒ¼ã‚¹ã®æ™‚é–“çš„ãƒãƒƒãƒãƒ³ã‚°
- [ ] Temporal correlationã®ç²¾åº¦å‘ä¸Š
- [ ] ä½ç›¸ãšã‚Œ(lag)ã®å¯è¦–åŒ–

**æœŸå¾…åŠ¹æœ**: Temporal correlationç²¾åº¦ +15-20%

---

#### B. Change Point Detection
```python
import ruptures as rpt

def detect_events_via_changepoint(cpm_series):
    # CPMã®æ€¥æ¿€ãªå¤‰åŒ–ç‚¹ = ã‚¤ãƒ™ãƒ³ãƒˆå€™è£œ
    algo = rpt.Pelt(model='rbf').fit(cpm_series)
    changepoints = algo.predict(pen=10)
    return changepoints
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] Change point detectionã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çµ±åˆ
- [ ] BERTopicã¨ã®çµæœæ¯”è¼ƒ
- [ ] ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®æ¤œè¨

**æœŸå¾…åŠ¹æœ**: Event detection recall +10-15%

---

## ğŸ“‹ Phase 3: çµ±è¨ˆçš„å¦¥å½“æ€§ã®ç¢ºä¿ (Dec 2-7)

### 3.1 Cross-Validation (Priority: ğŸ”´ CRITICAL)

**ç›®æ¨™**: å­¦ä¼šæŸ»èª­ã«è€ãˆã‚‹çµ±è¨ˆçš„å¦¥å½“æ€§

#### A. K-Fold Cross-Validation
```python
from sklearn.model_selection import KFold

def cross_validate_event_detection(streams, k=5):
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    results = []
    
    for train_idx, test_idx in kf.split(streams):
        train_streams = [streams[i] for i in train_idx]
        test_streams = [streams[i] for i in test_idx]
        
        # Train: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        model = optimize_parameters(train_streams)
        
        # Test: æ¤œè¨¼
        precision, recall, f1 = evaluate_model(model, test_streams)
        results.append({'precision': precision, 'recall': recall, 'f1': f1})
    
    return pd.DataFrame(results)
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] K-Fold CVå®Ÿè£… (k=5)
- [ ] å¹³å‡æ€§èƒ½ Â± æ¨™æº–åå·®ãƒ¬ãƒãƒ¼ãƒˆ
- [ ] Foldé–“ã®åˆ†æ•£åˆ†æ (ANOVA)

**æœŸå¾…åŠ¹æœ**: çµæœã®å†ç¾æ€§ãƒ»ä¸€èˆ¬åŒ–æ€§èƒ½ä¿è¨¼

---

### 3.2 çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š (Priority: ğŸŸ  HIGH)

#### A. Bootstrap Confidence Intervals
```python
from scipy.stats import bootstrap

def bootstrap_ci(data, statistic=np.mean, n_resamples=10000):
    rng = np.random.default_rng(42)
    res = bootstrap(
        (data,), 
        statistic, 
        n_resamples=n_resamples,
        confidence_level=0.95,
        random_state=rng
    )
    return res.confidence_interval
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] å…¨è©•ä¾¡æŒ‡æ¨™ã«Bootstrap CIè¿½åŠ 
- [ ] 95% CIã‚’å›³è¡¨ã«è¡¨ç¤º
- [ ] CIå¹…ã®å¦¥å½“æ€§æ¤œè¨

**æœŸå¾…åŠ¹æœ**: çµ±è¨ˆçš„ä¿¡é ¼æ€§ +å¤§å¹…å‘ä¸Š

---

#### B. Inter-Rater Reliability
```python
from sklearn.metrics import cohen_kappa_score

def compute_inter_rater_reliability(annotator1, annotator2):
    kappa = cohen_kappa_score(annotator1, annotator2)
    # Interpretation:
    # 0.81-1.0: Almost perfect agreement
    # 0.61-0.80: Substantial agreement
    # 0.41-0.60: Moderate agreement
    return kappa
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] è¤‡æ•°äººã§Ground Truthä½œæˆ (nâ‰¥2)
- [ ] Cohen's Kappaè¨ˆç®— (ç›®æ¨™: Îºâ‰¥0.7)
- [ ] ä¸ä¸€è‡´äº‹ä¾‹ã®å†æ¤œè¨

**æœŸå¾…åŠ¹æœ**: ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å“è³ªä¿è¨¼

---

### 3.3 ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ (Priority: ğŸŸ¡ MEDIUM)

#### A. Baseline Methods
```python
baselines = {
    'Random': random_event_detection,
    'TF-IDF + Cosine': tfidf_cosine_similarity,
    'LDA': lda_topic_model,
    'BERTopic (original)': bertopic_original,
    'Proposed (Hybrid)': proposed_hybrid_method,
}
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] 5ã¤ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿè£…
- [ ] çµ±ä¸€è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã§æ¯”è¼ƒ
- [ ] çµ±è¨ˆçš„æœ‰æ„å·®æ¤œå®š (Wilcoxon signed-rank test)

**æœŸå¾…åŠ¹æœ**: æ‰‹æ³•ã®å„ªä½æ€§ã®å®Ÿè¨¼

---

## ğŸ“‹ Phase 4: è«–æ–‡å“è³ªå‘ä¸Š (Dec 8-14)

### 4.1 Ablation Study (Priority: ğŸ”´ CRITICAL)

**ç›®æ¨™**: å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å¯„ä¸ã‚’å®šé‡åŒ–

```python
configurations = [
    {'embedding': True, 'topic': False, 'lexical': False},  # Embedding only
    {'embedding': False, 'topic': True, 'lexical': False},  # Topic only
    {'embedding': False, 'topic': False, 'lexical': True},  # Lexical only
    {'embedding': True, 'topic': True, 'lexical': False},   # Emb + Topic
    {'embedding': True, 'topic': True, 'lexical': True},    # All (proposed)
]

for config in configurations:
    performance = evaluate(config)
    print(f"Config: {config}, F1: {performance['f1']:.3f}")
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] Ablation Studyå®Ÿè£…
- [ ] å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å¯„ä¸ã‚°ãƒ©ãƒ•ä½œæˆ
- [ ] çµ±è¨ˆçš„æœ‰æ„å·®æ¤œå®š

**æœŸå¾…åŠ¹æœ**: è«–æ–‡ã®Robustnesså‘ä¸Š

---

### 4.2 Error Analysis (Priority: ğŸŸ  HIGH)

#### A. False Positiveåˆ†æ
```python
def analyze_false_positives(predictions, ground_truth):
    fp_cases = [p for p in predictions if p not in ground_truth]
    
    # FPã®åŸå› åˆ†é¡
    categories = {
        'noise': [],      # ãƒã‚¤ã‚ºãƒˆãƒ”ãƒƒã‚¯
        'spam': [],       # ãƒœãƒƒãƒˆãƒ»ã‚¹ãƒ‘ãƒ 
        'temporal': [],   # æ™‚åˆ»ãšã‚Œ
        'semantic': [],   # æ„å‘³çš„é¡ä¼¼æ€§èª¤åˆ¤å®š
    }
    
    for fp in fp_cases:
        category = classify_error(fp)
        categories[category].append(fp)
    
    return categories
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] FP/FNäº‹ä¾‹ã®è©³ç´°åˆ†æ
- [ ] ã‚¨ãƒ©ãƒ¼åŸå› ã®ã‚«ãƒ†ã‚´ãƒªåŒ–
- [ ] æ”¹å–„ææ¡ˆã®ç”Ÿæˆ

**æœŸå¾…åŠ¹æœ**: æ‰‹æ³•ã®é™ç•Œã®æ˜ç¢ºåŒ– (Limitations section)

---

### 4.3 å¯è¦–åŒ–ã®æ”¹å–„ (Priority: ğŸŸ¡ MEDIUM)

#### A. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–
```python
import plotly.graph_objects as go

def create_interactive_timeline(events):
    fig = go.Figure()
    
    for stream in events['stream'].unique():
        stream_events = events[events['stream'] == stream]
        fig.add_trace(go.Scatter(
            x=stream_events['timestamp'],
            y=stream_events['cpm'],
            mode='lines+markers',
            name=stream,
            hovertext=stream_events['top_words'],
        ))
    
    fig.write_html('output/interactive_timeline.html')
```

**å®Ÿè£…ã‚¿ã‚¹ã‚¯**:
- [ ] Plotlyã«ã‚ˆã‚‹ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å›³è¡¨
- [ ] ã‚¤ãƒ™ãƒ³ãƒˆãƒ›ãƒãƒ¼æ™‚ã«è©³ç´°è¡¨ç¤º
- [ ] ã‚ºãƒ¼ãƒ ãƒ»ãƒ‘ãƒ³æ©Ÿèƒ½

**æœŸå¾…åŠ¹æœ**: è«–æ–‡ã®è¦–è¦šçš„èª¬å¾—åŠ›å‘ä¸Š

---

## ğŸ“Š å®Ÿè£…å„ªå…ˆé †ä½ãƒãƒˆãƒªãƒƒã‚¯ã‚¹

| ã‚¿ã‚¹ã‚¯ | Impact | Effort | Priority | Week |
|--------|--------|--------|----------|------|
| **Time Binningæœ€é©åŒ–** | ğŸ”´ High | ğŸŸ¢ Low | P1 | Week 1 |
| **Embedding Thresholdèª¿æ•´** | ğŸ”´ High | ğŸŸ¢ Low | P1 | Week 1 |
| **ãƒã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°** | ğŸ”´ High | ğŸŸ¡ Med | P1 | Week 1 |
| **Ground Truthä½œæˆ** | ğŸ”´ High | ğŸŸ¡ Med | P1 | Week 1 |
| **Hybrid Topic Model** | ğŸŸ  Med | ğŸ”´ High | P2 | Week 2 |
| **KeyBERTçµ±åˆ** | ğŸŸ  Med | ğŸŸ¡ Med | P2 | Week 2 |
| **Cross-Validation** | ğŸ”´ High | ğŸŸ¡ Med | P2 | Week 2 |
| **Ablation Study** | ğŸ”´ High | ğŸŸ¢ Low | P2 | Week 2 |
| **NERçµ±åˆ** | ğŸŸ¡ Low | ğŸ”´ High | P3 | Optional |
| **DTWå®Ÿè£…** | ğŸŸ¡ Low | ğŸ”´ High | P3 | Optional |

**å‡¡ä¾‹**:
- Impact: ğŸ”´ High, ğŸŸ  Medium, ğŸŸ¡ Low
- Effort: ğŸ”´ High (>1æ—¥), ğŸŸ¡ Med (4-8æ™‚é–“), ğŸŸ¢ Low (<4æ™‚é–“)
- Priority: P1 (å¿…é ˆ), P2 (æ¨å¥¨), P3 (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)

---

## ğŸ¯ æœŸå¾…ã•ã‚Œã‚‹æˆæœ (2é€±é–“å¾Œ)

### å®šé‡çš„ç›®æ¨™

| æŒ‡æ¨™ | ç¾çŠ¶ | ç›®æ¨™ | æ”¹å–„å¹… |
|------|------|------|--------|
| **Topic Jaccard (>0)** | 20% | 60% | +40% |
| **High Similarity (â‰¥0.7)** | 10% | 40% | +30% |
| **F1 Score** | ? | 0.75+ | - |
| **Precision** | ? | 0.80+ | - |
| **Recall** | ? | 0.70+ | - |
| **Paper Quality** | 6/10 | 8-9/10 | +2-3 |

### å®šæ€§çš„ç›®æ¨™

- âœ… **å­¦ä¼šæŸ»èª­åŸºæº–**ã‚’æº€ãŸã™çµ±è¨ˆçš„å¦¥å½“æ€§
- âœ… **å†ç¾æ€§**ã‚’ä¿è¨¼ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ãƒ—ãƒ­ãƒˆã‚³ãƒ«
- âœ… **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ**ã«ã‚ˆã‚‹æ‰‹æ³•ã®å„ªä½æ€§å®Ÿè¨¼
- âœ… **Ablation Study**ã«ã‚ˆã‚‹å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å¯„ä¸å®šé‡åŒ–
- âœ… **Ground Truth**ã«ã‚ˆã‚‹å®¢è¦³çš„è©•ä¾¡
- âœ… **Error Analysis**ã«ã‚ˆã‚‹æ‰‹æ³•ã®é™ç•Œæ˜ç¤º

---

## ğŸ“ è«–æ–‡åŸ·ç­†ã¸ã®åæ˜ 

### Methods Section è¿½åŠ é …ç›®

#### 3.X Event Detection and Matching
```markdown
3.X.1 Hybrid Topic Modeling
- BERTopic + LDAçµ±åˆæ‰‹æ³•ã®èª¬æ˜
- å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã¸ã®å¯¾å¿œã®æ­£å½“åŒ–

3.X.2 Contextual Keyphrase Extraction
- KeyBERT-based N-gramæŠ½å‡º
- å¤šè¨€èªå¯¾å¿œã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­è¨ˆ

3.X.3 Parameter Optimization
- Grid Search + Cross-Validation
- æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç†è«–çš„æ ¹æ‹ 

3.X.4 Evaluation Protocol
- Ground Truthä½œæˆæ‰‹é †
- Inter-rater reliability (Cohen's Kappa)
- Evaluation metrics (Precision, Recall, F1)
```

### Results Section è¿½åŠ é …ç›®

#### 4.X Event Detection Performance
```markdown
4.X.1 Baseline Comparison
- 5ã¤ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ‰‹æ³•ã¨ã®æ¯”è¼ƒ
- çµ±è¨ˆçš„æœ‰æ„å·®æ¤œå®šçµæœ (p<0.05)

4.X.2 Ablation Study
- å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å¯„ä¸
- æœ€é©é‡ã¿ (Î±_emb=0.7, Î±_topic=0.2, Î±_lex=0.1)

4.X.3 Cross-Validation Results
- 5-fold CVå¹³å‡æ€§èƒ½ Â± æ¨™æº–åå·®
- ä¸€èˆ¬åŒ–æ€§èƒ½ã®ä¿è¨¼
```

### Discussion Section è¿½åŠ é …ç›®

#### 5.X Limitations
```markdown
5.X.1 Data Scale Limitations
- 9ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ã¿ (ä»Šå¾Œã®æ‹¡å¼µæ–¹å‘)

5.X.2 Language Processing Challenges
- ã‚¹ãƒ©ãƒ³ã‚°ã€æ–°èªã¸ã®å¯¾å¿œé™ç•Œ
- å¤šè¨€èªNERã®ç²¾åº¦èª²é¡Œ

5.X.3 Temporal Synchronization
- ã‚¹ãƒˆãƒªãƒ¼ãƒ é–“ã®æ™‚åˆ»ãšã‚Œ (Â±5ç§’)
```

---

## ğŸš€ å®Ÿè¡Œè¨ˆç”» (2é€±é–“ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«)

### Week 1 (Nov 17-24): Quick Wins

| æ—¥ä»˜ | ã‚¿ã‚¹ã‚¯ | æ‰€è¦æ™‚é–“ | æ‹…å½“ |
|------|--------|----------|------|
| **11/17 (æ—¥)** | Ground Truthä½œæˆ (30 events) | 3æ™‚é–“ | âœ… |
| **11/18 (æœˆ)** | Time Binningæœ€é©åŒ– | 4æ™‚é–“ | - |
| **11/19 (ç«)** | Embedding Thresholdæœ€é©åŒ– | 4æ™‚é–“ | - |
| **11/20 (æ°´)** | ãƒã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè£… | 3æ™‚é–“ | - |
| **11/21 (æœ¨)** | è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ | 3æ™‚é–“ | - |
| **11/22 (é‡‘)** | ä¸­é–“è©•ä¾¡ãƒ»ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ | 2æ™‚é–“ | - |
| **11/23-24** | ãƒãƒƒãƒ•ã‚¡ãƒ»äºˆå‚™ | - | - |

**Week 1ç›®æ¨™**: Paper Quality 6/10 â†’ 7/10

---

### Week 2 (Nov 25-Dec 1): Deep Improvements

| æ—¥ä»˜ | ã‚¿ã‚¹ã‚¯ | æ‰€è¦æ™‚é–“ | æ‹…å½“ |
|------|--------|----------|------|
| **11/25 (æœˆ)** | Hybrid Topic Modelå®Ÿè£… | 6æ™‚é–“ | - |
| **11/26 (ç«)** | KeyBERTçµ±åˆ | 4æ™‚é–“ | - |
| **11/27 (æ°´)** | Cross-Validationå®Ÿè£… | 4æ™‚é–“ | - |
| **11/28 (æœ¨)** | Ablation Studyå®Ÿè¡Œ | 3æ™‚é–“ | - |
| **11/29 (é‡‘)** | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒå®Ÿé¨“ | 4æ™‚é–“ | - |
| **11/30 (åœŸ)** | æœ€çµ‚è©•ä¾¡ãƒ»çµ±è¨ˆæ¤œå®š | 3æ™‚é–“ | - |
| **12/1 (æ—¥)** | ãƒ¬ãƒãƒ¼ãƒˆå®Œæˆãƒ»ã‚³ãƒŸãƒƒãƒˆ | 2æ™‚é–“ | - |

**Week 2ç›®æ¨™**: Paper Quality 7/10 â†’ 8-9/10

---

## ğŸ“š å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¿½åŠ 

```bash
# Week 1
pip install keybert  # Contextual keyphrase extraction
pip install spacy  # NER
python -m spacy download en_core_web_sm
python -m spacy download ja_core_news_sm

# Week 2
pip install dtaidistance  # Dynamic Time Warping
pip install ruptures  # Change point detection
pip install plotly  # Interactive visualization
pip install scikit-learn  # Cross-validation, metrics
```

---

## âœ… ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### Phase 1 (Week 1)
- [ ] Ground Truth 30 eventsä½œæˆå®Œäº†
- [ ] Cohen's Kappa â‰¥ 0.7 é”æˆ
- [ ] Time Binningæœ€é©åŒ– (Grid Search)
- [ ] Embedding Thresholdæœ€é©åŒ– (ROC Curve)
- [ ] ãƒã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè£…
- [ ] è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Œæˆ
- [ ] Week 1ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ

### Phase 2 (Week 2)
- [ ] Hybrid Topic Modelå®Ÿè£…ãƒ»æ¤œè¨¼
- [ ] KeyBERTçµ±åˆå®Œäº†
- [ ] Cross-Validationå®Ÿæ–½ (k=5)
- [ ] Ablation Studyå®Œäº†
- [ ] ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒå®Œäº†
- [ ] çµ±è¨ˆçš„æœ‰æ„å·®æ¤œå®šå®Œäº†
- [ ] Paper Quality â‰¥ 8/10 é”æˆ

### Phase 3 (è«–æ–‡åŸ·ç­†æ™‚)
- [ ] Methods Sectionæ›´æ–°
- [ ] Results Sectionæ›´æ–°
- [ ] Limitations Sectionè¿½åŠ 
- [ ] å›³è¡¨æ›´æ–° (Bootstrap CIè¿½åŠ )
- [ ] å†ç¾æ€§ç¢ºä¿ (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ˜è¨˜)

---

## ğŸ“Š æˆåŠŸæŒ‡æ¨™

### æœ€ä½åŸºæº– (å­¦ä¼šæŠ•ç¨¿å¯èƒ½ãƒ¬ãƒ™ãƒ«)
- âœ… Topic Jaccard (>0): **â‰¥50%**
- âœ… F1 Score: **â‰¥0.70**
- âœ… Cohen's Kappa: **â‰¥0.70**
- âœ… Cross-Validation: **æ¨™æº–åå·® <0.05**
- âœ… Paper Quality: **â‰¥8/10**

### ç†æƒ³çš„ç›®æ¨™
- ğŸ¯ Topic Jaccard (>0): **â‰¥60%**
- ğŸ¯ F1 Score: **â‰¥0.75**
- ğŸ¯ Precision: **â‰¥0.80**
- ğŸ¯ Recall: **â‰¥0.70**
- ğŸ¯ Paper Quality: **9/10**

---

## ğŸ“ å­¦è¡“çš„æ­£å½“æ€§ã®ç¢ºä¿

### 1. å†ç¾æ€§
- âœ… å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’READMEã«æ˜è¨˜
- âœ… Random seedå›ºå®š (seed=42)
- âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¬é–‹æº–å‚™
- âœ… ã‚³ãƒ¼ãƒ‰å…¬é–‹ (GitHub)

### 2. å¦¥å½“æ€§
- âœ… Ground Truth with Inter-rater reliability
- âœ… Cross-Validation
- âœ… Baseline comparison
- âœ… Statistical significance testing

### 3. é€æ˜æ€§
- âœ… Ablation Study (å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å¯„ä¸)
- âœ… Error Analysis (FP/FNåˆ†æ)
- âœ… Limitationsæ˜è¨˜
- âœ… å¤±æ•—å®Ÿé¨“ã‚‚å ±å‘Š

---

## ğŸ“ ã‚µãƒãƒ¼ãƒˆä½“åˆ¶

### è³ªå•ãƒ»ç›¸è«‡
- Advisorå®šæœŸãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚° (11/24, 12/1)
- æŠ€è¡“çš„å•é¡Œ: Stack Overflow, GitHub Issues
- çµ±è¨ˆçš„åŠ©è¨€: çµ±è¨ˆæ‹…å½“æ•™å“¡

### é€²æ—å ±å‘Š
- æ¯æ—¥: Git commit withè©³ç´°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
- é€±æ¬¡: Progress report (ã“ã®è¨ˆç”»æ›¸æ›´æ–°)
- æœ€çµ‚: Comprehensive report (Dec 1)

---

**ã“ã®ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã«å¾“ãˆã°ã€2é€±é–“ã§å­¦ä¼šè«–æ–‡ãƒ¬ãƒ™ãƒ«(Paper Quality 8-9/10)ã‚’é”æˆã§ãã¾ã™!** ğŸš€

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: Ground Truthä½œæˆã‹ã‚‰é–‹å§‹ã—ã¾ã—ã‚‡ã†! ğŸ“
