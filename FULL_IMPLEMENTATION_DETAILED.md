# ğŸš€ ãƒ•ãƒ«å®Ÿè£…(2é€±é–“)ã®å®Œå…¨è©³ç´°ã‚¬ã‚¤ãƒ‰

**ç›®æ¨™**: Paper Quality 8/10 â†’ **10/10** (Top-tier Conference Level)  
**æœŸé–“**: 14æ—¥é–“ (112æ™‚é–“)  
**æœ€çµ‚æˆæœ**: ACM MM / AAAI / WWW æŠ•ç¨¿å¯èƒ½ãªå®Œæˆè«–æ–‡

---

## ğŸ“… **2é€±é–“ã®è©³ç´°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«**

---

# ğŸ—“ï¸ **Week 1: Core Implementation (56æ™‚é–“)**

---

## **Day 1 (8æ™‚é–“): Translation Bridgeå®Ÿè£…** â­â­â­â­â­

### **åˆå‰ (4æ™‚é–“): åŸºç›¤å®Ÿè£…**

#### **Task 1.1: Translation Moduleã®ä½œæˆ (2h)**

**æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«**: `utils/translation_bridge.py`

```python
# -*- coding: utf-8 -*-
"""
Translation Bridge for Cross-Lingual Event Matching

å¤šè¨€èªã‚¤ãƒ™ãƒ³ãƒˆã‚’è‹±èªã«ç¿»è¨³ã—ã¦æ„å‘³çš„é¡ä¼¼åº¦ã‚’è¨ˆç®—
"""

from transformers import MarianMTModel, MarianTokenizer
from langdetect import detect, DetectorFactory
import torch
from typing import List, Dict, Tuple
import numpy as np

DetectorFactory.seed = 42

class TranslationBridge:
    """å¤šè¨€èªç¿»è¨³ãƒ–ãƒªãƒƒã‚¸"""
    
    def __init__(self, cache_dir='./cache/translation'):
        """
        Args:
            cache_dir: ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.cache_dir = cache_dir
        self.models = {}
        self.tokenizers = {}
        
        # ã‚µãƒãƒ¼ãƒˆè¨€èª
        self.supported_langs = ['ja', 'es', 'fr', 'de', 'zh', 'ko', 'pt']
        
        # ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        self._load_translation_models()
    
    def _load_translation_models(self):
        """ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ãƒ­ãƒ¼ãƒ‰"""
        model_names = {
            'ja': 'Helsinki-NLP/opus-mt-ja-en',
            'es': 'Helsinki-NLP/opus-mt-es-en',
            'fr': 'Helsinki-NLP/opus-mt-fr-en',
            'de': 'Helsinki-NLP/opus-mt-de-en',
            'zh': 'Helsinki-NLP/opus-mt-zh-en',
            'ko': 'Helsinki-NLP/opus-mt-ko-en',
            'pt': 'Helsinki-NLP/opus-mt-tc-big-en-pt',  # Reverse
        }
        
        print("[Translation Bridge] Loading translation models...")
        for lang, model_name in model_names.items():
            try:
                self.tokenizers[lang] = MarianTokenizer.from_pretrained(
                    model_name, cache_dir=self.cache_dir
                )
                self.models[lang] = MarianMTModel.from_pretrained(
                    model_name, cache_dir=self.cache_dir
                )
                print(f"  âœ“ Loaded {lang} â†’ en")
            except Exception as e:
                print(f"  âœ— Failed to load {lang}: {e}")
    
    def detect_language(self, text: str) -> str:
        """è¨€èªã‚’æ¤œå‡º"""
        try:
            lang = detect(text)
            return lang if lang in self.supported_langs else 'en'
        except:
            return 'en'
    
    def translate_to_english(self, texts: List[str], src_lang: str = None) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’è‹±èªã«ç¿»è¨³
        
        Args:
            texts: ç¿»è¨³ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            src_lang: ã‚½ãƒ¼ã‚¹è¨€èª (Noneã®å ´åˆã¯è‡ªå‹•æ¤œå‡º)
        
        Returns:
            ç¿»è¨³ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        if not texts:
            return []
        
        # è¨€èªæ¤œå‡º
        if src_lang is None:
            src_lang = self.detect_language(texts[0])
        
        # è‹±èªã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if src_lang == 'en':
            return texts
        
        # æœªã‚µãƒãƒ¼ãƒˆè¨€èª
        if src_lang not in self.models:
            print(f"[Warning] Unsupported language: {src_lang}, returning original")
            return texts
        
        # ç¿»è¨³å®Ÿè¡Œ
        model = self.models[src_lang]
        tokenizer = self.tokenizers[src_lang]
        
        translated = []
        batch_size = 32
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True)
            
            # ç¿»è¨³ç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(**inputs)
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            batch_translated = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            translated.extend(batch_translated)
        
        return translated
    
    def translate_event(self, event: Dict) -> Dict:
        """
        ã‚¤ãƒ™ãƒ³ãƒˆå…¨ä½“ã‚’ç¿»è¨³
        
        Args:
            event: {
                'comments': List[str],
                'topics': List[str],
                'language': str (optional)
            }
        
        Returns:
            ç¿»è¨³ã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆè¾æ›¸
        """
        # è¨€èªæ¤œå‡º
        lang = event.get('language') or self.detect_language(event['comments'][0])
        
        # ç¿»è¨³
        translated_comments = self.translate_to_english(event['comments'], lang)
        translated_topics = self.translate_to_english(event['topics'], lang)
        
        return {
            'comments': translated_comments,
            'topics': translated_topics,
            'original_language': lang,
            'translated': True
        }
    
    def get_cross_lingual_similarity(
        self, 
        event_A: Dict, 
        event_B: Dict,
        bert_model
    ) -> float:
        """
        ç•°ãªã‚‹è¨€èªã®ã‚¤ãƒ™ãƒ³ãƒˆé–“ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        
        Args:
            event_A: ã‚¤ãƒ™ãƒ³ãƒˆA
            event_B: ã‚¤ãƒ™ãƒ³ãƒˆB
            bert_model: SentenceTransformer ãƒ¢ãƒ‡ãƒ«
        
        Returns:
            é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ (0-1)
        """
        # ä¸¡æ–¹ã‚’è‹±èªã«ç¿»è¨³
        event_A_en = self.translate_event(event_A)
        event_B_en = self.translate_event(event_B)
        
        # BERT embedding
        emb_A = bert_model.encode(event_A_en['comments'])
        emb_B = bert_model.encode(event_B_en['comments'])
        
        # Cosine similarity
        similarity = np.dot(emb_A.mean(0), emb_B.mean(0)) / \
                     (np.linalg.norm(emb_A.mean(0)) * np.linalg.norm(emb_B.mean(0)))
        
        return float(similarity)
```

**ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰**: `tests/test_translation_bridge.py`

```python
import sys
sys.path.append('..')
from utils.translation_bridge import TranslationBridge

def test_translation():
    bridge = TranslationBridge()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_cases = [
        ("ä¹…ä¿ã™ã”ã„", "ja"),
        ("visca barca", "es"),
        ("allez les bleus", "fr"),
    ]
    
    for text, lang in test_cases:
        translated = bridge.translate_to_english([text], lang)
        print(f"{lang}: {text} â†’ {translated[0]}")

if __name__ == '__main__':
    test_translation()
```

---

#### **Task 1.2: event_comparison.pyã¸ã®çµ±åˆ (2h)**

**ä¿®æ­£ãƒ•ã‚¡ã‚¤ãƒ«**: `scripts/event_comparison.py`

```python
# è¿½åŠ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ãƒ•ã‚¡ã‚¤ãƒ«å…ˆé ­)
from utils.translation_bridge import TranslationBridge

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦åˆæœŸåŒ–
TRANSLATION_BRIDGE = None

def init_translation_bridge():
    """Translation Bridgeã‚’åˆæœŸåŒ–"""
    global TRANSLATION_BRIDGE
    if TRANSLATION_BRIDGE is None:
        print("[Init] Loading Translation Bridge...")
        TRANSLATION_BRIDGE = TranslationBridge()
        print("[Init] Translation Bridge ready")

# æ—¢å­˜ã®é¡ä¼¼åº¦è¨ˆç®—é–¢æ•°ã‚’æ‹¡å¼µ
def compute_cross_lingual_similarity(event_A, event_B, embedding_model):
    """
    å¤šè¨€èªå¯¾å¿œã®é¡ä¼¼åº¦è¨ˆç®—
    
    æ—¢å­˜ã®embedding_similarityã«åŠ ãˆã¦ã€ç¿»è¨³ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦ã‚‚è¨ˆç®—
    """
    # å¾“æ¥ã®embedding similarity
    emb_sim_original = compute_embedding_similarity(event_A, event_B, embedding_model)
    
    # Translation-based similarity
    if TRANSLATION_BRIDGE is not None:
        event_A_dict = {
            'comments': event_A['top_comments'],
            'topics': event_A['topics']
        }
        event_B_dict = {
            'comments': event_B['top_comments'],
            'topics': event_B['topics']
        }
        
        emb_sim_translated = TRANSLATION_BRIDGE.get_cross_lingual_similarity(
            event_A_dict, event_B_dict, embedding_model
        )
        
        # ä¸¡æ–¹ã®å¹³å‡ (ã¾ãŸã¯é‡ã¿ä»˜ã‘)
        final_similarity = 0.5 * emb_sim_original + 0.5 * emb_sim_translated
        
        return final_similarity, emb_sim_translated
    else:
        return emb_sim_original, None

# mainã®å†’é ­ã«è¿½åŠ 
def main():
    # ... æ—¢å­˜ã®argparseè¨­å®š ...
    
    # Translation BridgeåˆæœŸåŒ–
    init_translation_bridge()
    
    # ... æ®‹ã‚Šã®å‡¦ç† ...
```

---

### **åˆå¾Œ (4æ™‚é–“): å®Ÿé¨“ãƒ»æ¤œè¨¼**

#### **Task 1.3: Translationå®Ÿé¨“å®Ÿè¡Œ (2h)**

```bash
# Translationæœ‰åŠ¹ç‰ˆã§å®Ÿè¡Œ
python scripts/event_comparison.py \
  --folder data/chat \
  --pattern "*" \
  --n-events 12 \
  --time-bins 75 \
  --use-translation  # æ–°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**:
```
[Translation Bridge] Loading translation models...
  âœ“ Loaded ja â†’ en
  âœ“ Loaded es â†’ en
  âœ“ Loaded fr â†’ en

[Event Matching] Using translation-enhanced similarity
Event 419 <-> Event 420: 
  Original embedding: 0.969
  Translated embedding: 0.985 (+0.016)
  Final similarity: 0.977

Topic Jaccard > 0: 70.0% (Before: 33.3%)
```

---

#### **Task 1.4: Before/Afteræ¯”è¼ƒåˆ†æ (2h)**

**æ–°è¦ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**: `scripts/analyze_translation_impact.py`

```python
import pandas as pd

def compare_results():
    """Translationå‰å¾Œã®çµæœã‚’æ¯”è¼ƒ"""
    
    # Before (Translationç„¡ã—)
    df_before = pd.read_csv('output/event_to_event_pairs_before.csv')
    
    # After (Translationæœ‰ã‚Š)
    df_after = pd.read_csv('output/event_to_event_pairs_after.csv')
    
    print("=== Translation Impact Analysis ===")
    print(f"Topic Jaccard > 0:")
    print(f"  Before: {(df_before['topic_jaccard'] > 0).mean():.1%}")
    print(f"  After:  {(df_after['topic_jaccard'] > 0).mean():.1%}")
    print(f"  Improvement: {((df_after['topic_jaccard'] > 0).mean() - (df_before['topic_jaccard'] > 0).mean()):.1%}")
    
    print(f"\nAverage Similarity:")
    print(f"  Before: {df_before['combined_score'].mean():.3f}")
    print(f"  After:  {df_after['combined_score'].mean():.3f}")
    print(f"  Improvement: +{(df_after['combined_score'].mean() - df_before['combined_score'].mean()):.3f}")

if __name__ == '__main__':
    compare_results()
```

---

**Day 1æˆæœç‰©**:
- âœ… `utils/translation_bridge.py` (500è¡Œ)
- âœ… `tests/test_translation_bridge.py` (100è¡Œ)
- âœ… `scripts/analyze_translation_impact.py` (150è¡Œ)
- âœ… Translationçµ±åˆå®Ÿé¨“å®Œäº†
- ğŸ“Š **Topic Jaccard > 0**: 33% â†’ **70%** (+37%)

---

## **Day 2 (8æ™‚é–“): Ground Truthç”Ÿæˆ** â­â­â­â­â­

### **åˆå‰ (4æ™‚é–“): å€™è£œç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ **

#### **Task 2.1: Ground Truthå€™è£œæŠ½å‡º (2h)**

**æ–°è¦ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**: `scripts/generate_ground_truth_candidates.py`

```python
# -*- coding: utf-8 -*-
"""
Ground Truthå€™è£œã®è‡ªå‹•ç”Ÿæˆ

ã‚·ã‚¹ãƒ†ãƒ ãŒé«˜ã‚¹ã‚³ã‚¢ãƒ»ä½ã‚¹ã‚³ã‚¢ã®ãƒšã‚¢ã‚’æŠ½å‡ºã—ã€
äººé–“ãŒãƒ©ãƒ™ãƒªãƒ³ã‚°ã—ã‚„ã™ã„å½¢å¼ã§å‡ºåŠ›
"""

import pandas as pd
import json
from pathlib import Path

class GroundTruthGenerator:
    """Ground Truthå€™è£œç”Ÿæˆå™¨"""
    
    def __init__(self, pairs_csv: str):
        """
        Args:
            pairs_csv: event_to_event_pairs.csv ã®ãƒ‘ã‚¹
        """
        self.df = pd.read_csv(pairs_csv)
    
    def extract_candidates(self, n_positive=50, n_negative=50):
        """
        Positive/Negativeã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡º
        
        Args:
            n_positive: Positiveã‚µãƒ³ãƒ—ãƒ«æ•° (é«˜ã‚¹ã‚³ã‚¢)
            n_negative: Negativeã‚µãƒ³ãƒ—ãƒ«æ•° (ä½ã‚¹ã‚³ã‚¢)
        
        Returns:
            å€™è£œã®DataFrame
        """
        # High score pairs (Positive candidates)
        high_score = self.df.nlargest(n_positive*2, 'combined_score')
        
        # Low score pairs (Negative candidates)
        low_score = self.df.nsmallest(n_negative*2, 'combined_score')
        
        # Stratified sampling
        positive_samples = high_score.sample(n=n_positive, random_state=42)
        negative_samples = low_score.sample(n=n_negative, random_state=42)
        
        # Combine
        candidates = pd.concat([positive_samples, negative_samples])
        
        # Add predicted label
        candidates['predicted_label'] = (candidates['combined_score'] > 0.5).astype(int)
        
        return candidates.reset_index(drop=True)
    
    def format_for_labeling(self, candidates):
        """
        ãƒ©ãƒ™ãƒªãƒ³ã‚°ç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
        
        Returns:
            ãƒ©ãƒ™ãƒªãƒ³ã‚°ç”¨ã®è¾æ›¸ãƒªã‚¹ãƒˆ
        """
        labeling_data = []
        
        for idx, row in candidates.iterrows():
            # Parse event labels
            event_A_label = row['event_A_label']
            event_B_label = row['event_B_label']
            
            # Extract top comments (first 5)
            comments_A = event_A_label.split('(')[0].split('ãƒ»')[:5]
            comments_B = event_B_label.split('(')[0].split('ãƒ»')[:5]
            
            labeling_data.append({
                'pair_id': f"pair_{idx:03d}",
                'event_A_id': row['event_A_id'],
                'event_B_id': row['event_B_id'],
                'event_A_comments': comments_A,
                'event_B_comments': comments_B,
                'event_A_streams': row['event_A_streams'],
                'event_B_streams': row['event_B_streams'],
                'time_diff_bins': row['time_diff_bins'],
                'time_diff_seconds': row['time_diff_bins'] * 72,  # ä»®å®š: 72ç§’/bin
                'combined_score': float(row['combined_score']),
                'predicted_label': int(row['predicted_label']),
                'ground_truth': None,  # ãƒ©ãƒ™ãƒªãƒ³ã‚°æ™‚ã«å…¥åŠ›
                'confidence': None,    # ãƒ©ãƒ™ãƒªãƒ³ã‚°æ™‚ã«å…¥åŠ› (1-5)
                'notes': ""            # ãƒ©ãƒ™ãƒªãƒ³ã‚°æ™‚ã®ãƒ¡ãƒ¢
            })
        
        return labeling_data
    
    def save_for_labeling(self, labeling_data, output_path='data/ground_truth_candidates.json'):
        """
        ãƒ©ãƒ™ãƒªãƒ³ã‚°ç”¨JSONã‚’ä¿å­˜
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(labeling_data, f, ensure_ascii=False, indent=2)
        
        print(f"[Ground Truth] Saved {len(labeling_data)} candidates to {output_path}")
        
        # çµ±è¨ˆæƒ…å ±
        predicted_positive = sum(1 for d in labeling_data if d['predicted_label'] == 1)
        print(f"  Predicted Positive: {predicted_positive}")
        print(f"  Predicted Negative: {len(labeling_data) - predicted_positive}")
    
    def generate_labeling_ui_html(self, labeling_data, output_path='data/labeling_ui.html'):
        """
        ç°¡æ˜“ãƒ©ãƒ™ãƒªãƒ³ã‚°UIã‚’HTMLç”Ÿæˆ
        """
        html = """
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Ground Truth Labeling</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .pair { border: 1px solid #ccc; padding: 15px; margin-bottom: 20px; }
        .event { display: inline-block; width: 45%; vertical-align: top; }
        .comments { font-size: 14px; color: #333; }
        .score { font-weight: bold; color: #007bff; }
        .label-buttons button { padding: 10px 20px; margin: 5px; font-size: 16px; }
        .positive { background-color: #28a745; color: white; }
        .negative { background-color: #dc3545; color: white; }
    </style>
</head>
<body>
    <h1>Ground Truth Labeling (100 pairs)</h1>
    <p>å„ãƒšã‚¢ãŒã€ŒåŒä¸€ã‚¤ãƒ™ãƒ³ãƒˆã€ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„</p>
"""
        
        for pair in labeling_data:
            html += f"""
    <div class="pair">
        <h3>Pair {pair['pair_id']}</h3>
        <div class="event">
            <h4>Event A ({pair['event_A_streams']} streams)</h4>
            <div class="comments">
                {'<br>'.join(pair['event_A_comments'])}
            </div>
        </div>
        <div class="event">
            <h4>Event B ({pair['event_B_streams']} streams)</h4>
            <div class="comments">
                {'<br>'.join(pair['event_B_comments'])}
            </div>
        </div>
        <p>Time difference: {pair['time_diff_seconds']} seconds</p>
        <p class="score">System Score: {pair['combined_score']:.3f} (Predicted: {'Same' if pair['predicted_label']==1 else 'Different'})</p>
        <div class="label-buttons">
            <button class="positive" onclick="label('{pair['pair_id']}', 1)">Same Event</button>
            <button class="negative" onclick="label('{pair['pair_id']}', 0)">Different Event</button>
        </div>
    </div>
"""
        
        html += """
    <script>
        let labels = {};
        function label(pairId, value) {
            labels[pairId] = value;
            console.log('Labeled', pairId, value);
            // Save to localStorage
            localStorage.setItem('ground_truth_labels', JSON.stringify(labels));
            alert('Labeled: ' + pairId + ' = ' + (value ? 'Same' : 'Different'));
        }
        
        // Load existing labels
        const saved = localStorage.getItem('ground_truth_labels');
        if (saved) {
            labels = JSON.parse(saved);
        }
    </script>
</body>
</html>
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html)
        
        print(f"[Ground Truth] Generated labeling UI: {output_path}")

def main():
    # Load pairs
    generator = GroundTruthGenerator('output/event_to_event_pairs.csv')
    
    # Extract candidates
    candidates = generator.extract_candidates(n_positive=50, n_negative=50)
    
    # Format for labeling
    labeling_data = generator.format_for_labeling(candidates)
    
    # Save JSON
    generator.save_for_labeling(labeling_data)
    
    # Generate HTML UI
    generator.generate_labeling_ui_html(labeling_data)
    
    print("\n[Next Step] Open data/labeling_ui.html in browser and start labeling!")

if __name__ == '__main__':
    main()
```

---

#### **Task 2.2: ç°¡æ˜“ãƒ©ãƒ™ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  (2h)**

**æ–°è¦ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**: `scripts/labeling_tool.py`

```python
# -*- coding: utf-8 -*-
"""
å¯¾è©±çš„ãƒ©ãƒ™ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ« (CLIç‰ˆ)

ãƒ–ãƒ©ã‚¦ã‚¶ãªã—ã§ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ãƒ©ãƒ™ãƒªãƒ³ã‚°
"""

import json
from pathlib import Path

class LabelingTool:
    """å¯¾è©±çš„ãƒ©ãƒ™ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«"""
    
    def __init__(self, candidates_path='data/ground_truth_candidates.json'):
        """
        Args:
            candidates_path: å€™è£œJSONã®ãƒ‘ã‚¹
        """
        with open(candidates_path, 'r', encoding='utf-8') as f:
            self.candidates = json.load(f)
        
        self.labeled_count = 0
        self.output_path = 'data/ground_truth_labeled.json'
        
        # Load existing labels
        if Path(self.output_path).exists():
            with open(self.output_path, 'r', encoding='utf-8') as f:
                self.labeled = json.load(f)
            self.labeled_count = len([c for c in self.labeled if c['ground_truth'] is not None])
        else:
            self.labeled = self.candidates.copy()
    
    def display_pair(self, pair):
        """ãƒšã‚¢ã‚’è¡¨ç¤º"""
        print("\n" + "="*80)
        print(f"Pair {pair['pair_id']} ({self.labeled_count+1}/{len(self.candidates)})")
        print("="*80)
        
        print(f"\n[Event A] ({pair['event_A_streams']} streams)")
        for i, comment in enumerate(pair['event_A_comments'], 1):
            print(f"  {i}. {comment}")
        
        print(f"\n[Event B] ({pair['event_B_streams']} streams)")
        for i, comment in enumerate(pair['event_B_comments'], 1):
            print(f"  {i}. {comment}")
        
        print(f"\nTime Difference: {pair['time_diff_seconds']} seconds")
        print(f"System Score: {pair['combined_score']:.3f}")
        print(f"System Prediction: {'Same Event' if pair['predicted_label']==1 else 'Different Event'}")
    
    def label_interactive(self):
        """å¯¾è©±çš„ã«ãƒ©ãƒ™ãƒªãƒ³ã‚°"""
        print("\nğŸ·ï¸  Ground Truth Labeling Tool")
        print("Instructions:")
        print("  1 = Same Event")
        print("  0 = Different Event")
        print("  s = Skip")
        print("  q = Quit and Save")
        print()
        
        for i, pair in enumerate(self.labeled):
            # Already labeled
            if pair['ground_truth'] is not None:
                continue
            
            # Display
            self.display_pair(pair)
            
            # Input
            while True:
                response = input("\nYour label (1/0/s/q): ").strip().lower()
                
                if response == 'q':
                    self.save()
                    print(f"\nâœ“ Saved {self.labeled_count} labels. Goodbye!")
                    return
                elif response == 's':
                    print("Skipped")
                    break
                elif response in ['1', '0']:
                    pair['ground_truth'] = int(response)
                    
                    # Confidence (optional)
                    conf = input("Confidence (1-5, optional): ").strip()
                    if conf.isdigit() and 1 <= int(conf) <= 5:
                        pair['confidence'] = int(conf)
                    
                    self.labeled_count += 1
                    print(f"âœ“ Labeled as {'Same' if pair['ground_truth']==1 else 'Different'}")
                    
                    # Auto-save every 10
                    if self.labeled_count % 10 == 0:
                        self.save()
                        print(f"\n[Auto-saved] {self.labeled_count} labels")
                    
                    break
                else:
                    print("Invalid input. Please enter 1, 0, s, or q")
        
        # All done
        self.save()
        print(f"\nğŸ‰ Labeling complete! Total: {self.labeled_count} labels")
    
    def save(self):
        """ãƒ©ãƒ™ãƒ«ã‚’ä¿å­˜"""
        with open(self.output_path, 'w', encoding='utf-8') as f:
            json.dump(self.labeled, f, ensure_ascii=False, indent=2)

def main():
    tool = LabelingTool()
    tool.label_interactive()

if __name__ == '__main__':
    main()
```

---

### **åˆå¾Œ (4æ™‚é–“): ãƒ©ãƒ™ãƒªãƒ³ã‚°å®Ÿè¡Œãƒ»è©•ä¾¡**

#### **Task 2.3: å®Ÿéš›ã®ãƒ©ãƒ™ãƒªãƒ³ã‚°ä½œæ¥­ (2h)**

```bash
# ãƒ©ãƒ™ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«èµ·å‹•
python scripts/labeling_tool.py
```

**ç›®æ¨™**: 100ãƒšã‚¢ã‚’ãƒ©ãƒ™ãƒªãƒ³ã‚°
- 1ãƒšã‚¢ã‚ãŸã‚Šå¹³å‡30ç§’
- åˆè¨ˆ50åˆ† (ä¼‘æ†©å«ã‚ã¦2æ™‚é–“)

---

#### **Task 2.4: è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ (2h)**

**æ–°è¦ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**: `scripts/evaluate_with_ground_truth.py`

```python
# -*- coding: utf-8 -*-
"""
Ground Truthãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡

Precision, Recall, F1-Scoreã‚’è¨ˆç®—
"""

import json
import pandas as pd
import numpy as np
from sklearn.metrics import (
    precision_score, recall_score, f1_score,
    confusion_matrix, classification_report,
    average_precision_score, roc_auc_score
)
import matplotlib.pyplot as plt
import seaborn as sns

class GroundTruthEvaluator:
    """Ground Truthãƒ™ãƒ¼ã‚¹è©•ä¾¡å™¨"""
    
    def __init__(self, labeled_path='data/ground_truth_labeled.json'):
        """
        Args:
            labeled_path: ãƒ©ãƒ™ãƒªãƒ³ã‚°æ¸ˆã¿JSONã®ãƒ‘ã‚¹
        """
        with open(labeled_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        # Filter labeled only
        self.labeled = [d for d in self.data if d['ground_truth'] is not None]
        
        print(f"[Ground Truth] Loaded {len(self.labeled)} labeled pairs")
    
    def evaluate(self, threshold=0.5):
        """
        è©•ä¾¡ã‚’å®Ÿè¡Œ
        
        Args:
            threshold: é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã®é–¾å€¤
        
        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸
        """
        # Ground truth labels
        y_true = [d['ground_truth'] for d in self.labeled]
        
        # Predictions (binary)
        y_pred = [(d['combined_score'] > threshold) for d in self.labeled]
        
        # Scores (continuous)
        y_scores = [d['combined_score'] for d in self.labeled]
        
        # Metrics
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        
        # Confusion Matrix
        cm = confusion_matrix(y_true, y_pred)
        
        # Average Precision (AP)
        ap = average_precision_score(y_true, y_scores)
        
        # ROC-AUC
        try:
            auc = roc_auc_score(y_true, y_scores)
        except:
            auc = None
        
        results = {
            'threshold': threshold,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'average_precision': ap,
            'roc_auc': auc,
            'confusion_matrix': cm.tolist(),
            'n_samples': len(self.labeled)
        }
        
        return results
    
    def print_report(self, results):
        """è©•ä¾¡çµæœã‚’è¡¨ç¤º"""
        print("\n" + "="*70)
        print("Ground Truth Evaluation Report")
        print("="*70)
        
        print(f"\nSample Size: {results['n_samples']}")
        print(f"Threshold: {results['threshold']:.2f}")
        print(f"\nMetrics:")
        print(f"  Precision: {results['precision']:.3f}")
        print(f"  Recall:    {results['recall']:.3f}")
        print(f"  F1-Score:  {results['f1_score']:.3f}")
        print(f"  AP:        {results['average_precision']:.3f}")
        if results['roc_auc']:
            print(f"  ROC-AUC:   {results['roc_auc']:.3f}")
        
        print(f"\nConfusion Matrix:")
        cm = np.array(results['confusion_matrix'])
        print(f"  TN={cm[0,0]}, FP={cm[0,1]}")
        print(f"  FN={cm[1,0]}, TP={cm[1,1]}")
    
    def plot_confusion_matrix(self, results, output_path='output/confusion_matrix.png'):
        """Confusion Matrixã‚’å¯è¦–åŒ–"""
        cm = np.array(results['confusion_matrix'])
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['Different', 'Same'],
                    yticklabels=['Different', 'Same'])
        plt.title(f"Confusion Matrix (F1={results['f1_score']:.3f})")
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        plt.savefig(output_path, dpi=150)
        print(f"[Plot] Saved confusion matrix: {output_path}")
    
    def optimize_threshold(self):
        """æœ€é©ãªé–¾å€¤ã‚’æ¢ç´¢"""
        y_true = [d['ground_truth'] for d in self.labeled]
        y_scores = [d['combined_score'] for d in self.labeled]
        
        thresholds = np.arange(0.3, 0.9, 0.05)
        results = []
        
        for th in thresholds:
            y_pred = [(score > th) for score in y_scores]
            f1 = f1_score(y_true, y_pred)
            prec = precision_score(y_true, y_pred)
            rec = recall_score(y_true, y_pred)
            results.append({
                'threshold': th,
                'f1': f1,
                'precision': prec,
                'recall': rec
            })
        
        df = pd.DataFrame(results)
        best_idx = df['f1'].idxmax()
        best_threshold = df.loc[best_idx, 'threshold']
        
        print(f"\n[Threshold Optimization]")
        print(f"  Best F1: {df.loc[best_idx, 'f1']:.3f} at threshold={best_threshold:.2f}")
        
        # Plot
        plt.figure(figsize=(10, 6))
        plt.plot(df['threshold'], df['f1'], 'o-', label='F1-Score')
        plt.plot(df['threshold'], df['precision'], 's-', label='Precision')
        plt.plot(df['threshold'], df['recall'], '^-', label='Recall')
        plt.axvline(best_threshold, color='red', linestyle='--', label=f'Best ({best_threshold:.2f})')
        plt.xlabel('Threshold')
        plt.ylabel('Score')
        plt.title('Threshold Optimization')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('output/threshold_optimization.png', dpi=150)
        print(f"[Plot] Saved threshold optimization: output/threshold_optimization.png")
        
        return best_threshold, df

def main():
    evaluator = GroundTruthEvaluator()
    
    # Evaluate with default threshold
    results = evaluator.evaluate(threshold=0.5)
    evaluator.print_report(results)
    evaluator.plot_confusion_matrix(results)
    
    # Optimize threshold
    best_th, th_results = evaluator.optimize_threshold()
    
    # Re-evaluate with best threshold
    print(f"\n[Re-evaluation with best threshold={best_th:.2f}]")
    results_best = evaluator.evaluate(threshold=best_th)
    evaluator.print_report(results_best)

if __name__ == '__main__':
    main()
```

---

**Day 2æˆæœç‰©**:
- âœ… `scripts/generate_ground_truth_candidates.py` (300è¡Œ)
- âœ… `scripts/labeling_tool.py` (200è¡Œ)
- âœ… `scripts/evaluate_with_ground_truth.py` (250è¡Œ)
- âœ… **Ground Truth**: 100ãƒšã‚¢ãƒ©ãƒ™ãƒªãƒ³ã‚°å®Œäº†
- ğŸ“Š **Precision**: 0.85-0.90
- ğŸ“Š **F1-Score**: 0.83-0.88
- ğŸ“Š **Paper Quality**: 8 â†’ **9** (+1ç‚¹)

---

## **Day 3-4 (16æ™‚é–“): Hierarchical Event Detection** â­â­â­â­

### **Day 3åˆå‰ (4æ™‚é–“): åŸºç›¤è¨­è¨ˆ**

#### **Task 3.1: Hierarchical BERTopicã®è¨­è¨ˆ (2h)**

**æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«**: `utils/hierarchical_detector.py`

```python
# -*- coding: utf-8 -*-
"""
Hierarchical Event Detection

3ãƒ¬ãƒ™ãƒ«éšå±¤ (Coarse â†’ Medium â†’ Fine) ã§ã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¤œå‡º
"""

from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
from umap import UMAP
from hdbscan import HDBSCAN
from bertopic.representation import MaximalMarginalRelevance
import numpy as np
from typing import List, Dict, Tuple

class HierarchicalEventDetector:
    """éšå±¤çš„ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºå™¨"""
    
    def __init__(self, embedding_model):
        """
        Args:
            embedding_model: SentenceTransformer ãƒ¢ãƒ‡ãƒ«
        """
        self.embedding_model = embedding_model
        
        # 3ãƒ¬ãƒ™ãƒ«ã®BERTopic ãƒ¢ãƒ‡ãƒ«
        self.models = {
            'coarse': self._create_bertopic(min_topic_size=50, level='coarse'),
            'medium': self._create_bertopic(min_topic_size=20, level='medium'),
            'fine': self._create_bertopic(min_topic_size=5, level='fine')
        }
    
    def _create_bertopic(self, min_topic_size, level):
        """ãƒ¬ãƒ™ãƒ«åˆ¥BERTopicãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦èª¿æ•´
        params = {
            'coarse': {'n_neighbors': 40, 'min_cluster_size': 30},
            'medium': {'n_neighbors': 25, 'min_cluster_size': 15},
            'fine': {'n_neighbors': 15, 'min_cluster_size': 5}
        }[level]
        
        vectorizer = CountVectorizer(
            token_pattern=r"(?u)\b\w+\b",
            max_features=8000,
            min_df=1,
            ngram_range=(1, 3),
            max_df=1.0
        )
        
        umap_model = UMAP(
            n_components=10,
            n_neighbors=params['n_neighbors'],
            min_dist=0.0,
            metric="cosine",
            random_state=42
        )
        
        hdbscan_model = HDBSCAN(
            min_cluster_size=params['min_cluster_size'],
            min_samples=2,
            metric="euclidean",
            cluster_selection_method="eom",
            prediction_data=True
        )
        
        representation_model = MaximalMarginalRelevance(diversity=0.5)
        
        return BERTopic(
            embedding_model=self.embedding_model,
            vectorizer_model=vectorizer,
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            representation_model=representation_model,
            min_topic_size=min_topic_size,
            verbose=False
        )
    
    def detect_hierarchical(self, comments: List[str], embeddings=None):
        """
        éšå±¤çš„ã«ã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¤œå‡º
        
        Args:
            comments: ã‚³ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            embeddings: äº‹å‰è¨ˆç®—æ¸ˆã¿embedding (optional)
        
        Returns:
            {
                'coarse': [...],
                'medium': [...],
                'fine': [...]
            }
        """
        if embeddings is None:
            embeddings = self.embedding_model.encode(comments)
        
        results = {}
        
        # Level 1: Coarse Events
        print(f"  [Level 1: Coarse] Detecting large-scale events...")
        coarse_topics, coarse_probs = self.models['coarse'].fit_transform(comments, embeddings)
        results['coarse'] = self._extract_events(comments, coarse_topics, 'coarse')
        print(f"    â†’ Found {len(results['coarse'])} coarse events")
        
        # Level 2: Medium Events (within each coarse event)
        print(f"  [Level 2: Medium] Detecting medium-scale events...")
        medium_events = []
        for c_event in results['coarse']:
            indices = c_event['indices']
            if len(indices) < 20:  # Skip small events
                continue
            
            sub_comments = [comments[i] for i in indices]
            sub_embeddings = embeddings[indices]
            
            medium_topics, _ = self.models['medium'].fit_transform(sub_comments, sub_embeddings)
            m_events = self._extract_events(sub_comments, medium_topics, 'medium', base_indices=indices)
            medium_events.extend(m_events)
        
        results['medium'] = medium_events
        print(f"    â†’ Found {len(results['medium'])} medium events")
        
        # Level 3: Fine Events (within each medium event)
        print(f"  [Level 3: Fine] Detecting fine-grained events...")
        fine_events = []
        for m_event in results['medium']:
            indices = m_event['indices']
            if len(indices) < 10:  # Skip small events
                continue
            
            sub_comments = [comments[i] for i in indices]
            sub_embeddings = embeddings[indices]
            
            fine_topics, _ = self.models['fine'].fit_transform(sub_comments, sub_embeddings)
            f_events = self._extract_events(sub_comments, fine_topics, 'fine', base_indices=indices)
            fine_events.extend(f_events)
        
        results['fine'] = fine_events
        print(f"    â†’ Found {len(results['fine'])} fine events")
        
        return results
    
    def _extract_events(self, comments, topics, level, base_indices=None):
        """ãƒˆãƒ”ãƒƒã‚¯ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆã‚’æŠ½å‡º"""
        events = []
        unique_topics = set(topics) - {-1}  # Exclude noise
        
        for topic_id in unique_topics:
            # Get comments in this topic
            mask = np.array(topics) == topic_id
            topic_indices = np.where(mask)[0]
            
            if base_indices is not None:
                # Map back to original indices
                topic_indices = [base_indices[i] for i in topic_indices]
            
            topic_comments = [comments[i] for i in topic_indices]
            
            events.append({
                'level': level,
                'topic_id': int(topic_id),
                'indices': topic_indices,
                'comments': topic_comments,
                'size': len(topic_comments)
            })
        
        return events
    
    def select_best_level(self, hierarchical_results, target_size=20):
        """
        ç›®æ¨™ã‚¤ãƒ™ãƒ³ãƒˆæ•°ã«æœ€ã‚‚è¿‘ã„ãƒ¬ãƒ™ãƒ«ã‚’é¸æŠ
        
        Args:
            hierarchical_results: detect_hierarchical()ã®çµæœ
            target_size: ç›®æ¨™ã‚¤ãƒ™ãƒ³ãƒˆæ•°
        
        Returns:
            é¸æŠã•ã‚ŒãŸãƒ¬ãƒ™ãƒ«ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        """
        coarse_count = len(hierarchical_results['coarse'])
        medium_count = len(hierarchical_results['medium'])
        fine_count = len(hierarchical_results['fine'])
        
        # Closest to target
        distances = {
            'coarse': abs(coarse_count - target_size),
            'medium': abs(medium_count - target_size),
            'fine': abs(fine_count - target_size)
        }
        
        best_level = min(distances, key=distances.get)
        
        print(f"\n[Level Selection]")
        print(f"  Coarse: {coarse_count} events (distance: {distances['coarse']})")
        print(f"  Medium: {medium_count} events (distance: {distances['medium']})")
        print(f"  Fine: {fine_count} events (distance: {distances['fine']})")
        print(f"  â†’ Selected: {best_level} ({len(hierarchical_results[best_level])} events)")
        
        return hierarchical_results[best_level], best_level
```

ã“ã®ç¶šãã§ã€Day 3åˆå¾Œï½Day 14ã¾ã§ã®**å®Œå…¨å®Ÿè£…è©³ç´°**ã‚’è¨˜è¼‰ã—ã¾ã™ã‹?

ãã‚Œã¨ã‚‚ã€ã¾ãš**Day 1-2ã®å®Ÿè£…ã‚’é–‹å§‹**ã—ã¾ã™ã‹?

ã©ã¡ã‚‰ãŒè‰¯ã„ã§ã—ã‚‡ã†ã‹?
