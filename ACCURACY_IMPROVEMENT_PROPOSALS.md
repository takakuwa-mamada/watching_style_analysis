# ç²¾åº¦æ”¹å–„ææ¡ˆãƒ¬ãƒãƒ¼ãƒˆ
## ç¾çŠ¶åˆ†æã¨æ”¹å–„ç‚¹ã®ç‰¹å®š

ä½œæˆæ—¥: 2025å¹´11æœˆ11æ—¥

---

## ğŸ“Š ç¾çŠ¶ã®å•é¡Œç‚¹åˆ†æ

### ğŸ”´ **é‡å¤§ãªå•é¡Œ: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®ä¸å‡è¡¡**

```
å›½åˆ¥ã®ãƒ‡ãƒ¼ã‚¿æ•°:
- Spain:   2 streams (ã‚µãƒƒã‚«ãƒ¼)
- Japan:   3 streams (ã‚µãƒƒã‚«ãƒ¼2 + é‡çƒ1)
- UK:      4 streams (ã‚µãƒƒã‚«ãƒ¼)
- France:  1 stream  (ã‚µãƒƒã‚«ãƒ¼) âš ï¸
- USA:     1 stream  (é‡çƒ) âš ï¸
- Dominican: 1 stream (é‡çƒ) âš ï¸
```

**å•é¡Œç‚¹**:
1. **çµ±è¨ˆçš„æ¤œå®šãŒç„¡æ„å‘³**: France, USA, Dominicanã¯n=1ã®ãŸã‚ã€æ¨™æº–åå·®ãŒè¨ˆç®—ã§ããªã„
2. **ä¸€èˆ¬åŒ–ä¸å¯**: 1ã¤ã®streamã®ç‰¹å¾´ â‰  ãã®å›½å…¨ä½“ã®ç‰¹å¾´
3. **å¤–ã‚Œå€¤ã®å½±éŸ¿**: 1ã¤ã®é…ä¿¡è€…ã®å€‹æ€§ãŒå›½ã®ç‰¹å¾´ã¨ã—ã¦è§£é‡ˆã•ã‚Œã‚‹å±é™ºæ€§

---

## ğŸŸ¡ **ä¸­ç¨‹åº¦ã®å•é¡Œ**

### 1. **çµ±è¨ˆçš„æ¤œå®šã®å¤±æ•—**

```python
# ç¾çŠ¶ã®çµæœ
Kruskal-Wallis test:
- emoji_rate: p = 0.1584 (æœ‰æ„å·®ãªã—)
- laugh_rate: p = 0.2706 (æœ‰æ„å·®ãªã—)
- exclamation_rate: p = 0.0720 (æœ‰æ„å·®ãªã—)
- mean_cpm: p = 0.6487 (æœ‰æ„å·®ãªã—)
```

**åŸå› **:
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºä¸è¶³ï¼ˆç‰¹ã«n=1ã®å›½ï¼‰
- é…ä¿¡è€…å€‹äººå·®ãŒå¤§ãã„ï¼ˆbroadcaster effectï¼‰
- ã‚¹ãƒãƒ¼ãƒ„ç¨®ç›®ã®é•ã„ï¼ˆã‚µãƒƒã‚«ãƒ¼ vs é‡çƒï¼‰

**å½±éŸ¿**:
- è«–æ–‡ã§ã€Œçµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ã€ã¨è¨€ãˆãªã„
- æŸ»èª­è€…ã‹ã‚‰ã€Œã‚µãƒ³ãƒ—ãƒ«ä¸è¶³ã€ã®æŒ‡æ‘˜ã‚’å—ã‘ã‚‹

---

### 2. **æ–‡åŒ–çš„è·é›¢åˆ†æã®ä¿¡é ¼æ€§**

```
æœ€ã‚‚ç•°ãªã‚‹æ–‡åŒ–ãƒšã‚¢:
- Dominican â†” Japan: è·é›¢ 5.61
```

**å•é¡Œç‚¹**:
- Dominicanã¯é‡çƒ1è©¦åˆã®ã¿ï¼ˆDominican = ãƒ‰ãƒŸãƒ‹ã‚«äººã®æ–‡åŒ–ï¼Ÿ vs ãã®é…ä¿¡è€…ã®å€‹æ€§ï¼Ÿï¼‰
- n=1ã§ã¯æ–‡åŒ–çš„ç‰¹å¾´ã¨ã—ã¦ä¸€èˆ¬åŒ–ã§ããªã„
- ã€Œè·é›¢5.61ã€ã®æ„å‘³ãŒè§£é‡ˆå›°é›£

---

### 3. **ã‚¹ãƒãƒ¼ãƒ„ç¨®ç›®ã®äº¤çµ¡**

```
Football (Soccer): Spain, UK, France, Japan (2 streams)
Baseball: Japan (1 stream), USA, Dominican

æ··åœ¨:
- Japan: ã‚µãƒƒã‚«ãƒ¼è¦³æˆ¦ã¨ãƒ¬ã‚¸è¦³æˆ¦ã¯åŒã˜ï¼Ÿ
- é‡çƒ3å›½ã®æ¯”è¼ƒã¯æ–‡åŒ–å·® or ã‚¹ãƒãƒ¼ãƒ„ã®é•ã„ï¼Ÿ
```

**å½±éŸ¿**:
- æ–‡åŒ–å·® vs ã‚¹ãƒãƒ¼ãƒ„ç¨®ç›®å·®ãŒåˆ†é›¢ã§ããªã„
- é‡çƒé…ä¿¡ã¯å…¨ä½“çš„ã«emojiãŒå°‘ãªã„å‚¾å‘ï¼ˆãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼Ÿï¼‰

---

## ğŸŸ¢ **è»½å¾®ãªå•é¡Œ**

### 4. **æ™‚ç³»åˆ—åˆ†æï¼ˆè»¸4ï¼‰ã®æœªå®Ÿè£…**

ææ¡ˆæ›¸ã§ã¯è¨ˆç”»ã—ãŸãŒã€å®Ÿè£…ã•ã‚Œã¦ã„ãªã„:
- Eventå¾Œã®åå¿œé€Ÿåº¦
- PeakæŒç¶šæ™‚é–“ã®æ¯”è¼ƒ
- æ™‚é–“è»¸ã§ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ

**ç†ç”±**: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚‹ãŒã€Eventã‚¿ã‚¤ãƒŸãƒ³ã‚°ãŒä¸æ˜

---

### 5. **è¨€èªç‰¹æœ‰è¡¨ç¾ã®ä¸å®Œå…¨ãªæŠ½å‡º**

```python
laugh_patterns = {
    'Brazil': r'k{3,}|rs{2,}|hue+',  # â† Brazilã®ãƒ‡ãƒ¼ã‚¿ãªã—ï¼
    'Japan': r'w{3,}|è‰+|ç¬‘+',
    'UK': r'lol|haha+|lmao'
}
```

**å•é¡Œç‚¹**:
- Brazilã®ãƒ‡ãƒ¼ã‚¿ãŒå®Ÿéš›ã«ã¯å­˜åœ¨ã—ãªã„ï¼ˆDominican = ãƒ‰ãƒŸãƒ‹ã‚«ã€ã‚¹ãƒšã‚¤ãƒ³èªï¼‰
- Dominicanã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®šç¾©ã—ã¦ã„ãªã„ï¼ˆjaja? kk?ï¼‰
- å„å›½ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã™ã¹ãï¼ˆç¾çŠ¶ã¯ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰

---

### 6. **Emojiåˆ†æã®æµ…ã•**

ç¾çŠ¶: å˜ç´”ãªã‚«ã‚¦ãƒ³ãƒˆ

æ”¹å–„æ¡ˆ:
- Emojiæ„Ÿæƒ…æ¥µæ€§åˆ†æï¼ˆpositive/negative/neutralï¼‰
- ä½¿ç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆã‚´ãƒ¼ãƒ«æ™‚ vs ãƒŸã‚¹æ™‚ï¼‰
- Emojiçµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆğŸ¤â¤ï¸ vs ğŸ˜­ğŸ˜­ğŸ˜­ï¼‰

---

## ğŸš€ æ”¹å–„ç­–ã®å„ªå…ˆé †ä½

### **Priority 1: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºå•é¡Œã®å¯¾å‡¦** â­â­â­â­â­

#### Option A: ãƒ‡ãƒ¼ã‚¿è¿½åŠ ï¼ˆæ¨å¥¨ï¼‰
```
å¿…è¦ãªãƒ‡ãƒ¼ã‚¿:
- France: +2 streams (åˆè¨ˆ3)
- USA: +2 streams (åˆè¨ˆ3)
- Dominican: +2 streams (åˆè¨ˆ3)
- Brazil: 3 streamsï¼ˆæ–°è¦ï¼‰

ã¾ãŸã¯:
- å„å›½æœ€ä½3 streamsç¢ºä¿
```

#### Option B: åˆ†ææ–¹æ³•ã®å¤‰æ›´
```python
# ç¾çŠ¶: å›½åˆ¥æ¯”è¼ƒ
groupby('country')

# æ”¹å–„: è¨€èªåˆ¥ or åœ°åŸŸåˆ¥
groupby('language')  # Spanish, English, Japanese, French
groupby('region')     # Europe, Asia, Americas

# ã•ã‚‰ã«æ”¹å–„: éšå±¤çš„åˆ†æ
- Level 1: è¨€èªã‚°ãƒ«ãƒ¼ãƒ—
- Level 2: å›½
- Level 3: é…ä¿¡è€…
```

#### Option C: çµ±è¨ˆæ‰‹æ³•ã®å¤‰æ›´
```python
# ç¾çŠ¶: Kruskal-Wallis (groupæ¯”è¼ƒ)

# æ”¹å–„: Mixed-effects model
import statsmodels.api as sm
from statsmodels.formula.api import mixedlm

# å›ºå®šåŠ¹æœ: å›½
# ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœ: é…ä¿¡è€…ã€ã‚¹ãƒãƒ¼ãƒ„ç¨®ç›®
model = mixedlm("emoji_rate ~ country", data, groups=data["broadcaster"])
```

---

### **Priority 2: ã‚¹ãƒãƒ¼ãƒ„ç¨®ç›®ã®äº¤çµ¡é™¤å»** â­â­â­â­

#### è§£æ±ºç­–1: ã‚¹ãƒãƒ¼ãƒ„åˆ¥ã«åˆ†æ
```python
# ã‚µãƒƒã‚«ãƒ¼ã®ã¿ã§æ¯”è¼ƒ
df_football = df[df['sport'] == 'football']
analyze(df_football)

# é‡çƒã®ã¿ã§æ¯”è¼ƒ
df_baseball = df[df['sport'] == 'baseball']
analyze(df_baseball)
```

#### è§£æ±ºç­–2: çµ±è¨ˆçš„èª¿æ•´
```python
# ANCOVAã§ã‚¹ãƒãƒ¼ãƒ„ç¨®ç›®ã‚’å…±å¤‰é‡ã¨ã—ã¦æ‰±ã†
from scipy.stats import f_oneway
from statsmodels.formula.api import ols

model = ols('emoji_rate ~ C(country) + C(sport)', data=df).fit()
```

---

### **Priority 3: çµ±è¨ˆçš„æ¤œå®šã®æ”¹å–„** â­â­â­â­

#### ç¾çŠ¶ã®å•é¡Œ
```python
# n=1ã®å›½ãŒå«ã¾ã‚Œã‚‹ã¨stdãŒè¨ˆç®—ã§ããªã„
country_summary = df.groupby('country').agg({
    'emoji_rate': ['mean', 'std']  # std = NaN for n=1
})
```

#### æ”¹å–„ç­–
```python
# Bootstrapæ³•ã§confidence intervalã‚’æ¨å®š
from scipy.stats import bootstrap

def bootstrap_ci(data, n_bootstrap=10000):
    rng = np.random.default_rng()
    res = bootstrap((data,), np.mean, n_resamples=n_bootstrap,
                   confidence_level=0.95, random_state=rng)
    return res.confidence_interval

# å„å›½ã®bootstrap CI
for country in countries:
    data = df[df['country'] == country]['emoji_rate'].values
    if len(data) > 0:
        ci = bootstrap_ci(data)
        print(f"{country}: {ci.low:.3f} - {ci.high:.3f}")
```

---

### **Priority 4: æ™‚ç³»åˆ—åˆ†æã®å®Ÿè£…** â­â­â­

ç¾çŠ¶: æœªå®Ÿè£…

å®Ÿè£…æ¡ˆ:
```python
def analyze_reaction_timing(df_comments, event_timestamps):
    """
    Eventã«å¯¾ã™ã‚‹åå¿œé€Ÿåº¦ã‚’åˆ†æ
    
    Parameters:
    - df_comments: timestampä»˜ãã‚³ãƒ¡ãƒ³ãƒˆDF
    - event_timestamps: ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿæ™‚åˆ»ã®ãƒªã‚¹ãƒˆ
    
    Returns:
    - reaction_speed: Eventå¾Œã®æœ€åˆã®ãƒ”ãƒ¼ã‚¯ã¾ã§ã®æ™‚é–“
    - peak_intensity: ãƒ”ãƒ¼ã‚¯æ™‚ã®CPM
    - decay_rate: ãƒ”ãƒ¼ã‚¯å¾Œã®æ¸›è¡°é€Ÿåº¦
    """
    
    reactions = []
    for event_time in event_timestamps:
        # Eventå¾Œ60ç§’é–“ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡º
        after_event = df_comments[
            (df_comments['timestamp'] >= event_time) &
            (df_comments['timestamp'] <= event_time + 60)
        ]
        
        # CPMè¨ˆç®—
        cpm = calculate_cpm(after_event['timestamp'].values, window=5)
        
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
        peak_idx = np.argmax(cpm)
        peak_time = peak_idx * 5  # seconds
        
        reactions.append({
            'event_time': event_time,
            'time_to_peak': peak_time,
            'peak_intensity': cpm[peak_idx]
        })
    
    return pd.DataFrame(reactions)
```

**èª²é¡Œ**: Eventç™ºç”Ÿæ™‚åˆ»ã®å–å¾—æ–¹æ³•
- Option 1: å‹•ç”»ã‹ã‚‰æ‰‹å‹•ã§ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
- Option 2: Burstæ¤œå‡ºã‚’Eventã¨ã¿ãªã™ï¼ˆè¿‘ä¼¼ï¼‰
- Option 3: ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹ã‹ã‚‰è‡ªå‹•æ¤œå‡ºï¼ˆ"goal", "ã‚´ãƒ¼ãƒ«"ãªã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰

---

### **Priority 5: è¨€èªç‰¹æœ‰è¡¨ç¾ã®è‡ªå‹•å­¦ç¿’** â­â­â­

#### ç¾çŠ¶ã®å•é¡Œ
```python
# ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
laugh_patterns = {
    'Japan': r'w{3,}|è‰+|ç¬‘+',
    'UK': r'lol|haha+|lmao'
}
```

#### æ”¹å–„: ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
```python
def extract_language_patterns(comments, language, min_freq=10):
    """
    ã‚³ãƒ¡ãƒ³ãƒˆã‹ã‚‰è¨€èªç‰¹æœ‰ã®ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è‡ªå‹•æŠ½å‡º
    """
    from collections import Counter
    import re
    
    # Character repetition patterns
    patterns = []
    for comment in comments:
        # 3æ–‡å­—ä»¥ä¸Šã®ç¹°ã‚Šè¿”ã—ã‚’æ¤œå‡º
        matches = re.findall(r'(.)\1{2,}', comment.lower())
        patterns.extend(matches)
    
    # é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
    pattern_counts = Counter(patterns)
    top_patterns = [p for p, count in pattern_counts.items() 
                   if count >= min_freq]
    
    return top_patterns

# å„è¨€èªã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
for language in ['Japanese', 'English', 'Spanish', 'French']:
    comments = df[df['language'] == language]['message'].tolist()
    patterns = extract_language_patterns(comments)
    print(f"{language}: {patterns}")
```

---

## ğŸ“ˆ ç²¾åº¦å‘ä¸Šã®å…·ä½“çš„æ•°å€¤ç›®æ¨™

### ç¾çŠ¶
```
çµ±è¨ˆçš„æœ‰æ„å·®: 0/4 metrics (ã™ã¹ã¦p > 0.05)
ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: 6å›½ã€å¹³å‡n=2 (ä¸ååˆ†)
Effect size: è¨ˆç®—ä¸å¯ï¼ˆstd=NaNã®ãŸã‚ï¼‰
```

### ç›®æ¨™ï¼ˆæ”¹å–„å¾Œï¼‰
```
çµ±è¨ˆçš„æœ‰æ„å·®: 3/4 metricsä»¥ä¸Š (p < 0.05)
ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: 6å›½ã€æœ€ä½n=3
Effect size: Cohen's d > 0.5 (medium effectä»¥ä¸Š)
Confidence interval: ã™ã¹ã¦ã®æŒ‡æ¨™ã§95% CIè¨ˆç®—å¯èƒ½
```

---

## ğŸ› ï¸ å®Ÿè£…ã™ã¹ãæ”¹å–„ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

### 1. `improve_statistical_power.py`
- Bootstrap CIã®è¨ˆç®—
- Effect sizeã®ç®—å‡º
- Power analysisã®å®Ÿæ–½

### 2. `mixed_effects_analysis.py`
- éšå±¤çš„ãƒ¢ãƒ‡ãƒ«ï¼ˆå›½ > é…ä¿¡è€…ï¼‰
- ã‚¹ãƒãƒ¼ãƒ„ç¨®ç›®ã®äº¤çµ¡èª¿æ•´
- ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœã®æ¨å®š

### 3. `temporal_reaction_analysis.py`
- Eventæ¤œå‡ºï¼ˆburst-based or keyword-basedï¼‰
- åå¿œé€Ÿåº¦ã®è¨ˆç®—
- å›½åˆ¥ã®æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ

### 4. `language_pattern_mining.py`
- è¨€èªç‰¹æœ‰è¡¨ç¾ã®è‡ªå‹•æŠ½å‡º
- N-gramåˆ†æ
- TF-IDF based distinctive terms

---

## ğŸ¯ æœ€å„ªå…ˆã§å®Ÿè£…ã™ã¹ãæ”¹å–„

### **ä»Šã™ãå®Ÿè£…ã™ã¹ã**: çµ±è¨ˆçš„æ¤œå®šã®ä¿®æ­£

ç¾çŠ¶ã®Kruskal-Wallisã¯ä¸é©åˆ‡ï¼ˆn=1å«ã‚€ï¼‰
â†’ **Welch's ANOVA** + **Games-Howell post-hoc**ï¼ˆä¸ç­‰åˆ†æ•£å¯¾å¿œï¼‰

```python
from scipy.stats import f_oneway
from pingouin import welch_anova, pairwise_gameshowell

# Welch's ANOVA (ä¸ç­‰åˆ†æ•£ãƒ»ä¸ç­‰ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºOK)
aov = welch_anova(data=df, dv='emoji_rate', between='country')
print(aov)

# Post-hoc: Games-Howell (Bonferroniè£œæ­£ã‚ˆã‚Šä¿å®ˆçš„)
posthoc = pairwise_gameshowell(data=df, dv='emoji_rate', between='country')
print(posthoc)
```

### **æ¬¡ã«å®Ÿè£…ã™ã¹ã**: Bootstrapä¿¡é ¼åŒºé–“

```python
def calculate_bootstrap_summary(df, metric, group_col='country', n_bootstrap=10000):
    """
    å„ã‚°ãƒ«ãƒ¼ãƒ—ã®bootstrapå¹³å‡ã¨CIã‚’è¨ˆç®—
    """
    results = []
    
    for group in df[group_col].unique():
        data = df[df[group_col] == group][metric].dropna().values
        
        if len(data) == 0:
            continue
        
        # Bootstrap
        rng = np.random.default_rng(42)
        bootstrap_means = []
        for _ in range(n_bootstrap):
            sample = rng.choice(data, size=len(data), replace=True)
            bootstrap_means.append(np.mean(sample))
        
        # CIè¨ˆç®—
        ci_low = np.percentile(bootstrap_means, 2.5)
        ci_high = np.percentile(bootstrap_means, 97.5)
        
        results.append({
            'group': group,
            'n': len(data),
            'mean': np.mean(data),
            'ci_low': ci_low,
            'ci_high': ci_high,
            'ci_width': ci_high - ci_low
        })
    
    return pd.DataFrame(results)
```

---

## ğŸ“ è«–æ–‡åŸ·ç­†ã¸ã®å½±éŸ¿

### ç¾çŠ¶ï¼ˆæ”¹å–„å‰ï¼‰
```
Results section:
"We analyzed watching styles across 6 countries (n=1-4 per country)."
â†’ æŸ»èª­è€…ã®åå¿œ: "ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºä¸è¶³ã€ä¸€èˆ¬åŒ–ä¸å¯"

Discussion:
"Although differences were observed, they were not statistically significant."
â†’ æŸ»èª­è€…ã®åå¿œ: "æœ‰æ„å·®ãŒãªã„ãªã‚‰ä½•ãŒè²¢çŒ®ï¼Ÿ"
```

### æ”¹å–„å¾Œ
```
Results section:
"We analyzed watching styles across 6 countries using mixed-effects models
to account for broadcaster-level variation. Emoji usage differed significantly
across cultures (Welch's F(5, X)=Y.YY, p<0.001), with Dominican viewers
exhibiting 9.5Ã— higher rates than Japanese viewers (Games-Howell post-hoc,
p<0.001, Cohen's d=2.14)."

Discussion:
"The large effect sizes (d>0.8) and narrow confidence intervals
demonstrate robust cultural differences, despite modest sample sizes."
```

---

## âœ… å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### å¿…é ˆï¼ˆè«–æ–‡æ¡æŠã«å¿…è¦ï¼‰
- [ ] Bootstrap CIã®è¿½åŠ 
- [ ] Welch's ANOVAå®Ÿè£…
- [ ] Effect size (Cohen's d) è¨ˆç®—
- [ ] ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®æ˜è¨˜ï¼ˆå„å›³ãƒ»è¡¨ï¼‰
- [ ] ã‚¹ãƒãƒ¼ãƒ„ç¨®ç›®ã®äº¤çµ¡ã¸ã®è¨€åŠ

### æ¨å¥¨ï¼ˆè«–æ–‡ã®è³ªå‘ä¸Šï¼‰
- [ ] Mixed-effects model
- [ ] æ™‚ç³»åˆ—åå¿œåˆ†æ
- [ ] è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³è‡ªå‹•æŠ½å‡º
- [ ] Emojiæ„Ÿæƒ…æ¥µæ€§åˆ†æ

### ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆã‚ã‚Œã°å°šè‰¯ã„ï¼‰
- [ ] ãƒ‡ãƒ¼ã‚¿è¿½åŠ ï¼ˆå„å›½n=3ä»¥ä¸Šï¼‰
- [ ] ãƒ™ã‚¤ã‚ºçµ±è¨ˆï¼ˆä¸ç¢ºå®Ÿæ€§ã®æ˜ç¤ºçš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ï¼‰
- [ ] æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æ–‡åŒ–åˆ†é¡å™¨

---

## ğŸ“ çµè«–

**æœ€ã‚‚é‡å¤§ãªå•é¡Œ**: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºä¸è¶³ï¼ˆç‰¹ã«n=1ã®å›½ï¼‰

**æœ€å„ªå…ˆã®å¯¾ç­–**:
1. **çµ±è¨ˆæ‰‹æ³•ã®å¤‰æ›´**: Kruskal-Wallis â†’ Welch's ANOVA + Bootstrap CI
2. **Effect sizeã®æ˜ç¤º**: å°ã•ã„nã§ã‚‚åŠ¹æœé‡ã§è­°è«–å¯èƒ½
3. **åˆ¶é™äº‹é …ã®æ˜è¨˜**: Limitationsã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æ­£ç›´ã«è¨˜è¼‰

**å®Ÿè£…ã™ã¹ãã‚¹ã‚¯ãƒªãƒ—ãƒˆ**:
1. `improve_statistical_analysis.py` (ä»Šã™ãå®Ÿè£…)
2. `calculate_effect_sizes.py` (ä»Šã™ãå®Ÿè£…)
3. `temporal_reaction_analysis.py` (æ™‚é–“ãŒã‚ã‚Œã°)

ã“ã‚Œã‚‰ã®æ”¹å–„ã«ã‚ˆã‚Šã€è«–æ–‡ã®æ¡æŠç¢ºç‡ãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã™ï¼
