# -*- coding: utf-8 -*-
"""
Translation Bridge for Cross-Lingual Event Matching

å¤šè¨€èªã‚¤ãƒ™ãƒ³ãƒˆã‚’è‹±èªã«ç¿»è¨³ã—ã¦æ„å‘³çš„é¡ä¼¼åº¦ã‚’è¨ˆç®—
Helsinki-NLP ã®ç„¡æ–™ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨

Author: Generated by GitHub Copilot
Date: 2025-11-20
"""

from transformers import MarianMTModel, MarianTokenizer
from langdetect import detect, DetectorFactory
import torch
from typing import List, Dict, Tuple
import numpy as np
import warnings

# è¨€èªæ¤œå‡ºã®å†ç¾æ€§ç¢ºä¿
DetectorFactory.seed = 42
warnings.filterwarnings('ignore')


class TranslationBridge:
    """å¤šè¨€èªç¿»è¨³ãƒ–ãƒªãƒƒã‚¸ - ã‚¤ãƒ™ãƒ³ãƒˆé–“ã®è¨€èªãƒãƒªã‚¢ã‚’è§£æ¶ˆ"""
    
    def __init__(self, cache_dir='./cache/translation', device=None):
        """
        Args:
            cache_dir (str): ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            device (str): 'cuda' or 'cpu' (None=è‡ªå‹•æ¤œå‡º)
        """
        self.cache_dir = cache_dir
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.models = {}
        self.tokenizers = {}
        
        # ã‚µãƒãƒ¼ãƒˆè¨€èª (ä¸»è¦7è¨€èª)
        self.supported_langs = ['ja', 'es', 'fr', 'de', 'zh', 'ko', 'pt']
        
        # ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        self._load_translation_models()
    
    def _load_translation_models(self):
        """Helsinki-NLPç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ãƒ­ãƒ¼ãƒ‰"""
        model_names = {
            'ja': 'Helsinki-NLP/opus-mt-ja-en',
            'es': 'Helsinki-NLP/opus-mt-es-en',
            'fr': 'Helsinki-NLP/opus-mt-fr-en',
            'de': 'Helsinki-NLP/opus-mt-de-en',
            'zh': 'Helsinki-NLP/opus-mt-zh-en',
            'ko': 'Helsinki-NLP/opus-mt-ko-en',
            'pt': 'Helsinki-NLP/opus-mt-tc-big-pt-en',  # Portuguese to English
        }
        
        print(f"\n[Translation Bridge] Loading translation models on {self.device}...")
        
        for lang, model_name in model_names.items():
            try:
                print(f"  Loading {lang} â†’ en...", end=' ')
                
                # Tokenizer
                self.tokenizers[lang] = MarianTokenizer.from_pretrained(
                    model_name, 
                    cache_dir=self.cache_dir
                )
                
                # Model
                model = MarianMTModel.from_pretrained(
                    model_name, 
                    cache_dir=self.cache_dir
                )
                model = model.to(self.device)
                model.eval()  # Inference mode
                
                self.models[lang] = model
                
                print("âœ“")
                
            except Exception as e:
                print(f"âœ— ({e})")
                # å¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œ (ä»–ã®è¨€èªã¯ä½¿ãˆã‚‹)
        
        print(f"[Translation Bridge] Ready with {len(self.models)} language pairs")
    
    def detect_language(self, text: str) -> str:
        """
        è¨€èªã‚’è‡ªå‹•æ¤œå‡º
        
        Args:
            text (str): æ¤œå‡ºå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            str: è¨€èªã‚³ãƒ¼ãƒ‰ (ja, es, en, etc.)
        """
        try:
            lang = detect(text)
            return lang if lang in self.supported_langs else 'en'
        except:
            # æ¤œå‡ºå¤±æ•—æ™‚ã¯è‹±èªã¨ä»®å®š
            return 'en'
    
    def translate_to_english(
        self, 
        texts: List[str], 
        src_lang: str = None,
        batch_size: int = 32
    ) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’è‹±èªã«ç¿»è¨³
        
        Args:
            texts (List[str]): ç¿»è¨³ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            src_lang (str): ã‚½ãƒ¼ã‚¹è¨€èª (None=è‡ªå‹•æ¤œå‡º)
            batch_size (int): ãƒãƒƒãƒã‚µã‚¤ã‚º
        
        Returns:
            List[str]: ç¿»è¨³ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        if not texts:
            return []
        
        # è¨€èªæ¤œå‡º (æœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆã§åˆ¤æ–­)
        if src_lang is None:
            src_lang = self.detect_language(texts[0])
        
        # è‹±èªã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if src_lang == 'en':
            return texts
        
        # æœªã‚µãƒãƒ¼ãƒˆè¨€èª
        if src_lang not in self.models:
            print(f"[Translation Warning] Unsupported language '{src_lang}', returning original texts")
            return texts
        
        # ç¿»è¨³å®Ÿè¡Œ
        model = self.models[src_lang]
        tokenizer = self.tokenizers[src_lang]
        
        translated = []
        
        # ãƒãƒƒãƒå‡¦ç†
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            try:
                inputs = tokenizer(
                    batch, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=512
                )
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # ç¿»è¨³ç”Ÿæˆ
                with torch.no_grad():
                    outputs = model.generate(**inputs, max_length=512)
                
                # ãƒ‡ã‚³ãƒ¼ãƒ‰
                batch_translated = tokenizer.batch_decode(outputs, skip_special_tokens=True)
                translated.extend(batch_translated)
                
            except Exception as e:
                print(f"[Translation Error] Batch {i//batch_size}: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
                translated.extend(batch)
        
        return translated
    
    def translate_event(self, event: Dict) -> Dict:
        """
        ã‚¤ãƒ™ãƒ³ãƒˆå…¨ä½“ã‚’ç¿»è¨³ (ã‚³ãƒ¡ãƒ³ãƒˆ + ãƒˆãƒ”ãƒƒã‚¯)
        
        Args:
            event (dict): {
                'comments': List[str],
                'topics': List[str],
                'language': str (optional)
            }
        
        Returns:
            dict: ç¿»è¨³ã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆè¾æ›¸
        """
        # è¨€èªæ¤œå‡º
        lang = event.get('language') or self.detect_language(event['comments'][0])
        
        # ã‚³ãƒ¡ãƒ³ãƒˆç¿»è¨³
        translated_comments = self.translate_to_english(event['comments'], lang)
        
        # ãƒˆãƒ”ãƒƒã‚¯ç¿»è¨³
        translated_topics = self.translate_to_english(event['topics'], lang)
        
        return {
            'comments': translated_comments,
            'topics': translated_topics,
            'original_language': lang,
            'translated': lang != 'en'
        }
    
    def get_cross_lingual_similarity(
        self, 
        event_A: Dict, 
        event_B: Dict,
        bert_model
    ) -> Tuple[float, Dict]:
        """
        ç•°ãªã‚‹è¨€èªã®ã‚¤ãƒ™ãƒ³ãƒˆé–“ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        
        Args:
            event_A (dict): ã‚¤ãƒ™ãƒ³ãƒˆA
            event_B (dict): ã‚¤ãƒ™ãƒ³ãƒˆB
            bert_model: SentenceTransformer ãƒ¢ãƒ‡ãƒ«
        
        Returns:
            tuple: (é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢, è©³ç´°æƒ…å ±)
        """
        # ä¸¡æ–¹ã‚’è‹±èªã«ç¿»è¨³
        event_A_en = self.translate_event(event_A)
        event_B_en = self.translate_event(event_B)
        
        # BERT embeddingè¨ˆç®—
        emb_A = bert_model.encode(event_A_en['comments'], show_progress_bar=False)
        emb_B = bert_model.encode(event_B_en['comments'], show_progress_bar=False)
        
        # å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«
        vec_A = np.mean(emb_A, axis=0)
        vec_B = np.mean(emb_B, axis=0)
        
        # Cosine similarity
        similarity = np.dot(vec_A, vec_B) / \
                     (np.linalg.norm(vec_A) * np.linalg.norm(vec_B))
        
        # è©³ç´°æƒ…å ±
        details = {
            'lang_A': event_A_en['original_language'],
            'lang_B': event_B_en['original_language'],
            'translated_A': event_A_en['translated'],
            'translated_B': event_B_en['translated'],
            'cross_lingual': event_A_en['original_language'] != event_B_en['original_language']
        }
        
        return float(similarity), details
    
    def batch_translate_events(self, events: List[Dict]) -> List[Dict]:
        """
        è¤‡æ•°ã‚¤ãƒ™ãƒ³ãƒˆã‚’ä¸€æ‹¬ç¿»è¨³
        
        Args:
            events (List[Dict]): ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        
        Returns:
            List[Dict]: ç¿»è¨³æ¸ˆã¿ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        """
        translated_events = []
        
        for i, event in enumerate(events):
            print(f"\r[Translation] Processing event {i+1}/{len(events)}", end='')
            translated_event = self.translate_event(event)
            translated_events.append(translated_event)
        
        print()  # æ”¹è¡Œ
        return translated_events
    
    def get_translation_stats(self, events: List[Dict]) -> Dict:
        """
        ç¿»è¨³çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
        
        Args:
            events (List[Dict]): ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        
        Returns:
            dict: çµ±è¨ˆæƒ…å ±
        """
        lang_counts = {}
        
        for event in events:
            lang = event.get('language') or self.detect_language(event['comments'][0])
            lang_counts[lang] = lang_counts.get(lang, 0) + 1
        
        return {
            'total_events': len(events),
            'language_distribution': lang_counts,
            'translation_required': sum(1 for e in events 
                                        if self.detect_language(e['comments'][0]) != 'en')
        }


# ==================== Testing Functions ====================

def test_basic_translation():
    """åŸºæœ¬çš„ãªç¿»è¨³ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*70)
    print("Translation Bridge - Basic Test")
    print("="*70)
    
    bridge = TranslationBridge()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_cases = [
        ("ä¹…ä¿ã™ã”ã„", "ja", "Kubo is amazing"),
        ("visca barca", "es", "long live barca"),
        ("allez les bleus", "fr", "go blues"),
        ("great goal", "en", "great goal"),
    ]
    
    print("\n[Translation Tests]")
    for text, lang, expected in test_cases:
        translated = bridge.translate_to_english([text], lang)
        print(f"  {lang}: '{text}' â†’ '{translated[0]}'")
        print(f"      (Expected: '{expected}')")
    
    print("\nâœ“ Basic translation test complete")


def test_language_detection():
    """è¨€èªæ¤œå‡ºãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*70)
    print("Translation Bridge - Language Detection Test")
    print("="*70)
    
    bridge = TranslationBridge()
    
    test_texts = [
        "This is English",
        "ã“ã‚Œã¯æ—¥æœ¬èªã§ã™",
        "Esto es espaÃ±ol",
        "C'est franÃ§ais",
        "Das ist Deutsch",
    ]
    
    print("\n[Language Detection]")
    for text in test_texts:
        detected = bridge.detect_language(text)
        print(f"  '{text}' â†’ {detected}")
    
    print("\nâœ“ Language detection test complete")


if __name__ == '__main__':
    # Run tests
    test_language_detection()
    test_basic_translation()
    
    print("\nğŸ‰ All tests passed!")
