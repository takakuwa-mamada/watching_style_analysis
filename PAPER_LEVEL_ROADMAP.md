# ğŸ“Š è«–æ–‡ãƒ¬ãƒ™ãƒ«åˆ°é”è¨ˆç”»: å®¢è¦³çš„åˆ†æ + å¯è¦–åŒ– + è«–ç†çš„èª¬å¾—åŠ› + æ–°è¦æ€§

**ç›®æ¨™**: ãƒ¬ãƒ™ãƒ«3 â†’ ãƒ¬ãƒ™ãƒ«10 (è«–æ–‡æŠ•ç¨¿å¯èƒ½ãªå“è³ª)

**æˆ¦ç•¥**: æ®µéšçš„æ”¹å–„ + å®šé‡è©•ä¾¡ + æ¯”è¼ƒå®Ÿé¨“ + èª¬å¾—åŠ›ã®ã‚ã‚‹å¯è¦–åŒ–

---

## ğŸ¯ ä»Šæ—¥ã‹ã‚‰å§‹ã‚ã‚‹æœ€å„ªå…ˆã‚¿ã‚¹ã‚¯

### **Task 1: Ground Truthä½œæˆï¼ˆ2-3æ™‚é–“ï¼‰** â­â­â­â­â­

**ç›®çš„**: ç¾åœ¨ã®æ‰‹æ³•ã‚’**å®¢è¦³çš„ã«è©•ä¾¡**ã™ã‚‹

**æ‰‹é †**:
```bash
cd "g:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\å¤§å­¦\4å¹´\ã‚¼ãƒŸ\watching_style_analysis"
python create_ground_truth.py --create
```

**åˆ¤å®šåŸºæº–ï¼ˆæ˜ç¢ºã«å®šç¾©ï¼‰**:
- âœ… `1` = åŒã˜ã‚¤ãƒ™ãƒ³ãƒˆ: åŒã˜è©¦åˆã®åŒã˜ç¬é–“ï¼ˆã‚´ãƒ¼ãƒ«ã€ã‚«ãƒ¼ãƒ‰ãªã©ï¼‰
- âŒ `0` = ç•°ãªã‚‹ã‚¤ãƒ™ãƒ³ãƒˆ: ç•°ãªã‚‹è©¦åˆ OR åŒã˜è©¦åˆã®ç•°ãªã‚‹ç¬é–“  
- â“ `?` = ä¸æ˜: åˆ¤æ–­ãŒå›°é›£ï¼ˆå¾Œã§å†æ¤œè¨ï¼‰

**åˆ¤å®šã®ãƒã‚¤ãƒ³ãƒˆ**:
1. ãƒ©ãƒ™ãƒ«ã®å†…å®¹ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹
2. æ™‚é–“å¸¯ãŒè¿‘ã„ã‹ï¼ˆtime_diff_bins < 10ï¼‰
3. embeddingé¡ä¼¼åº¦ãŒé«˜ã„ã‹ï¼ˆ> 0.7ï¼‰
4. å¸¸è­˜çš„ã«åŒã˜ã‚¤ãƒ™ãƒ³ãƒˆã¨æ€ãˆã‚‹ã‹

**æˆæœç‰©**:
- `output/ground_truth.json` - æ­£è§£ãƒ‡ãƒ¼ã‚¿
- ç¾åœ¨ã®Precision/Recall/F1-score
- ã“ã‚ŒãŒ**Baselineæ€§èƒ½**ã¨ãªã‚‹

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
```
äºˆæ¸¬:
- Precision: 0.55 (æ¤œå‡ºã—ãŸãƒšã‚¢ã®55%ãŒæ­£è§£)
- Recall: 0.45 (å®Ÿéš›ã®æ­£è§£ã®45%ã‚’æ¤œå‡º)
- F1-score: 0.50

ã“ã‚Œã‚’è«–æ–‡ã§å ±å‘Šã—ã€æ”¹å–„ã‚’ç¤ºã™
```

---

## ğŸ“š Phase 1-8ã®è©³ç´°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

### **Phase 1: å®¢è¦³çš„è©•ä¾¡ (Day 1-2)** 

#### 1.1 Ground Truthä½œæˆ âœ“
- 28ãƒšã‚¢ã‚’ç›®è¦–åˆ¤å®š
- åˆ¤å®šçµæœã‚’ `ground_truth.json` ã«ä¿å­˜

#### 1.2 ç¾çŠ¶æ€§èƒ½ã®æ¸¬å®š
```bash
python create_ground_truth.py --evaluate
```

**å¯è¦–åŒ–**:
- Confusion Matrixï¼ˆæ··åŒè¡Œåˆ—ï¼‰
- Precision-Recallæ›²ç·š
- ROCæ›²ç·š

**è©³ç´°åˆ†æ**:
- False Positives: ã©ã®ãƒšã‚¢ã‚’èª¤æ¤œå‡ºã—ãŸã‹
- False Negatives: ã©ã®ãƒšã‚¢ã‚’è¦‹é€ƒã—ãŸã‹
- True Positives: æ­£ã—ãæ¤œå‡ºã§ããŸã‚±ãƒ¼ã‚¹

**è«–æ–‡ã§ã®è¨˜è¼‰**:
```latex
\begin{table}[h]
\caption{Performance on Ground Truth Dataset}
\begin{tabular}{|l|c|c|c|}
\hline
Method & Precision & Recall & F1-score \\
\hline
Current Method & 0.55 & 0.45 & 0.50 \\
\hline
\end{tabular}
\end{table}
```

---

### **Phase 2: æœ€æ–°è«–æ–‡èª¿æŸ» (Day 2-3)**

#### 2.1 ä½“ç³»çš„æ–‡çŒ®èª¿æŸ»

**æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³**:
- Google Scholar: https://scholar.google.com/
- Semantic Scholar: https://www.semanticscholar.org/
- arXiv: https://arxiv.org/

**æ¤œç´¢ã‚¯ã‚¨ãƒª**:
```
1. "multi-stream event detection" after:2020
2. "live streaming chat analysis" after:2021
3. "cross-platform event matching" after:2020
4. "sports event detection social media" after:2022
5. "contrastive learning event detection" after:2021
6. "time series similarity DTW" after:2020
7. "multilingual event detection" after:2020
```

**è¨˜éŒ²ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆExcel/Notionï¼‰**:
| ã‚¿ã‚¤ãƒˆãƒ« | è‘—è€… | å¹´ | ä¼šè­° | æ‰‹æ³• | è©•ä¾¡æŒ‡æ¨™ | æœ¬ç ”ç©¶ã¨ã®é–¢é€£ | å¼•ç”¨? |
|---------|------|----|----|------|---------|--------------|------|
| ... | ... | ... | ... | ... | ... | ... | âœ“/âœ— |

**ç›®æ¨™**: 
- æœ€ä½15æœ¬ã®è«–æ–‡ã‚’èª¿æŸ»
- ã†ã¡10æœ¬ã‚’ Related Work ã«å¼•ç”¨
- ã†ã¡3-5æœ¬ã‚’è©³ç´°æ¯”è¼ƒ

#### 2.2 æ–°è¦æ€§ã®ç‰¹å®š

**æ—¢å­˜ç ”ç©¶ã®é™ç•Œ**:
| æ—¢å­˜ç ”ç©¶ | é™ç•Œ | æœ¬ç ”ç©¶ã®è²¢çŒ® |
|---------|------|------------|
| Twitter Event Detection | ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ | ãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ™‚ç³»åˆ— |
| Single Language | è‹±èªã®ã¿ | å¤šè¨€èªï¼ˆJA/EN/PTï¼‰ |
| Word-based Topic Model | ãƒ•ãƒ¬ãƒ¼ã‚ºãŒåˆ†è§£ã•ã‚Œã‚‹ | N-gram preservation |
| Static Matching | æ™‚é–“çš„ãšã‚Œã‚’è€ƒæ…®ã—ãªã„ | Temporal correlation |

**æ–°è¦æ€§ï¼ˆNoveltyï¼‰ã®ä¸»å¼µ**:
```
æœ¬ç ”ç©¶ã®3ã¤ã®è²¢çŒ®:
1. å¤šè¨€èªãƒ©ã‚¤ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‹ã‚‰ã®ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡º
2. N-gramãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ä¿æŒã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
3. ã‚³ãƒ¡ãƒ³ãƒˆæ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ´»ç”¨ã—ãŸãƒãƒƒãƒãƒ³ã‚°
```

---

### **Phase 3: Baselineå®Ÿè£… (Day 3-4)**

#### 3.1 Simple Baselines

**Baseline 1: Threshold-based**
```python
def baseline_threshold(pair):
    """æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«: é–¾å€¤ã ã‘ã§åˆ¤å®š"""
    if pair['embedding_similarity'] > 0.7 and pair['time_diff_bins'] < 5:
        return 1  # Same event
    return 0  # Different event
```

**Baseline 2: Lexical Only**
```python
def baseline_lexical(pair):
    """èªå½™ã®é‡è¤‡ã®ã¿"""
    if pair['lexical_similarity'] > 0.3:
        return 1
    return 0
```

**Baseline 3: No N-gram**
```python
def baseline_no_ngram(pair):
    """N-gramæŠ½å‡ºãªã—ï¼ˆBERTopicã®å…ƒã®æŒ™å‹•ï¼‰"""
    # topic_jaccardã‚’è¨ˆç®—ã›ãšã«embeddingã¨lexicalã®ã¿
    score = pair['embedding_similarity'] * 0.7 + pair['lexical_similarity'] * 0.3
    return 1 if score > 0.6 else 0
```

#### 3.2 æ¯”è¼ƒå®Ÿé¨“

**è©•ä¾¡**:
```bash
python evaluate_baselines.py --ground-truth output/ground_truth.json
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
| Method | Precision | Recall | F1 |
|--------|-----------|--------|-----|
| Threshold-based | 0.45 | 0.52 | 0.48 |
| Lexical Only | 0.38 | 0.42 | 0.40 |
| No N-gram | 0.52 | 0.48 | 0.50 |
| **Proposed (Full)** | **0.65** | **0.58** | **0.61** |

**è«–æ–‡ã§ã®è¨˜è¼‰**:
```
We compare our method against three baselines:
(1) Threshold-based matching using only embedding similarity
(2) Lexical matching using only word overlap
(3) BERTopic without N-gram preservation

Table X shows that our proposed method achieves 
F1-score of 0.61, outperforming all baselines.
```

---

### **Phase 4: ææ¡ˆæ‰‹æ³•ã®æ”¹è‰¯ (Day 4-6)**

#### 4.1 æ—¢ã«å®Ÿè£…æ¸ˆã¿ âœ“
- N-gram Preservation (TfidfVectorizer)
- Multi-modal Similarity (embedding + lexical + topic + temporal)

#### 4.2 è¿½åŠ æ”¹è‰¯1: Multi-variate DTW

**ç›®çš„**: ã‚ˆã‚Šç²¾å¯†ãªæ™‚ç³»åˆ—ãƒãƒƒãƒãƒ³ã‚°

**å®Ÿè£…è¨ˆç”»**:
```python
from tslearn.metrics import dtw
import numpy as np

def compute_multivariate_dtw(event_A, event_B, streams):
    """
    3ã¤ã®æ™‚ç³»åˆ—ã‚’åŒæ™‚ã«æ¯”è¼ƒ:
    1. ã‚³ãƒ¡ãƒ³ãƒˆæ•°
    2. æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ï¼ˆç°¡æ˜“ç‰ˆ: ãƒã‚¸ãƒ†ã‚£ãƒ–èªã®å‰²åˆï¼‰
    3. ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ
    """
    # event_Aã®æ™‚ç³»åˆ—ã‚’å–å¾—
    ts_A = extract_multivariate_timeseries(event_A, streams)
    ts_B = extract_multivariate_timeseries(event_B, streams)
    
    # DTWè·é›¢
    distance = dtw(ts_A, ts_B)
    
    # é¡ä¼¼åº¦ã«å¤‰æ›
    similarity = 1.0 / (1.0 + distance)
    
    return similarity

def extract_multivariate_timeseries(event, streams):
    """ã‚¤ãƒ™ãƒ³ãƒˆå‘¨è¾ºã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    # Â±5 binsã®ç¯„å›²
    bin_id = event['bin_id']
    bins = range(max(0, bin_id - 5), min(nr_bins, bin_id + 6))
    
    # 3æ¬¡å…ƒæ™‚ç³»åˆ—
    ts = []
    for b in bins:
        comment_count = get_comment_count_at_bin(event, b, streams)
        sentiment = get_sentiment_at_bin(event, b, streams)
        topic_dist = get_topic_distribution_at_bin(event, b, streams)
        
        ts.append([comment_count, sentiment, topic_dist])
    
    return np.array(ts)
```

**æœŸå¾…åŠ¹æœ**: F1-score +0.03-0.05

#### 4.3 è¿½åŠ æ”¹è‰¯2: æ„Ÿæƒ…åˆ†æã®è¿½åŠ 

**ç›®çš„**: ã‚³ãƒ¡ãƒ³ãƒˆã®æ„Ÿæƒ…ï¼ˆèˆˆå¥®åº¦ï¼‰ã‚‚è€ƒæ…®

**ç°¡æ˜“å®Ÿè£…**:
```python
# ãƒã‚¸ãƒ†ã‚£ãƒ–èªãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–èªã®ãƒªã‚¹ãƒˆ
POSITIVE_WORDS = ["goal", "amazing", "great", "win", "ã™ã”ã„", "æœ€é«˜"]
NEGATIVE_WORDS = ["miss", "bad", "lose", "terrible", "ãƒ€ãƒ¡", "æœ€æ‚ª"]

def compute_sentiment_score(comments):
    """ç°¡æ˜“æ„Ÿæƒ…ã‚¹ã‚³ã‚¢"""
    pos_count = sum(1 for c in comments if any(w in c.lower() for w in POSITIVE_WORDS))
    neg_count = sum(1 for c in comments if any(w in c.lower() for w in NEGATIVE_WORDS))
    
    total = len(comments)
    return (pos_count - neg_count) / max(1, total)
```

**æœŸå¾…åŠ¹æœ**: F1-score +0.02

---

### **Phase 5: Ablation Study (Day 6-7)**

#### 5.1 å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å¯„ä¸ã‚’å®šé‡åŒ–

**å®Ÿé¨“è¨­å®š**:
```python
experiments = [
    {"name": "Full Model", "use_embedding": True, "use_lexical": True, "use_topic": True, "use_temporal": True},
    {"name": "w/o N-gram", "use_embedding": True, "use_lexical": True, "use_topic": False, "use_temporal": True},
    {"name": "w/o Temporal", "use_embedding": True, "use_lexical": True, "use_topic": True, "use_temporal": False},
    {"name": "w/o Embedding", "use_embedding": False, "use_lexical": True, "use_topic": True, "use_temporal": True},
    {"name": "w/o Lexical", "use_embedding": True, "use_lexical": False, "use_topic": True, "use_temporal": True},
]
```

**çµæœã®å¯è¦–åŒ–**:
```python
import matplotlib.pyplot as plt

components = ["Full", "w/o N-gram", "w/o Temporal", "w/o Embedding", "w/o Lexical"]
f1_scores = [0.61, 0.55, 0.58, 0.48, 0.59]

plt.figure(figsize=(10, 6))
plt.barh(components, f1_scores, color=['green', 'orange', 'orange', 'red', 'orange'])
plt.xlabel('F1-score')
plt.title('Ablation Study: Component Contributions')
plt.xlim(0, 1.0)
plt.grid(axis='x', alpha=0.3)
plt.savefig('output/ablation_study.png', dpi=300, bbox_inches='tight')
```

**è«–æ–‡ã§ã®è¨˜è¼‰**:
```latex
\begin{table}[h]
\caption{Ablation Study Results}
\begin{tabular}{|l|c|c|c|c|}
\hline
Configuration & Precision & Recall & F1 & Î” F1 \\
\hline
Full Model & 0.65 & 0.58 & 0.61 & - \\
w/o N-gram & 0.60 & 0.51 & 0.55 & -0.06 \\
w/o Temporal & 0.63 & 0.54 & 0.58 & -0.03 \\
w/o Embedding & 0.52 & 0.45 & 0.48 & -0.13 \\
w/o Lexical & 0.64 & 0.55 & 0.59 & -0.02 \\
\hline
\end{tabular}
\end{table}

The ablation study (Table X) shows that embedding similarity 
is the most important component (Î” F1 = -0.13), followed by 
N-gram preservation (Î” F1 = -0.06).
```

---

### **Phase 6: èª¬å¾—åŠ›ã®ã‚ã‚‹å¯è¦–åŒ– (Day 7-8)**

#### 6.1 å¿…é ˆã®å›³è¡¨ï¼ˆFigure 1-6ï¼‰

**Figure 1: ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦å›³**
```python
# Drawio or PowerPoint ã§ä½œæˆ
# å…¥åŠ› â†’ å‡¦ç† â†’ å‡ºåŠ›ã®æµã‚Œã‚’æ˜ç¢ºã«å›³ç¤º
```

**Figure 2: æˆåŠŸäº‹ä¾‹ã®å¯è¦–åŒ–**
```python
# Event 56 â†” 59 (topic_jaccard=1.0)
fig, axes = plt.subplots(2, 1, figsize=(12, 8))

# ä¸Šæ®µ: Event 56ã®æ™‚ç³»åˆ—
# ä¸‹æ®µ: Event 59ã®æ™‚ç³»åˆ—
# ç¸¦ç·šã§ peak bin ã‚’å¼·èª¿
# å…±é€šãƒˆãƒ”ãƒƒã‚¯èª "éŸ“å›½ç™ºç‹‚" ã‚’è¡¨ç¤º
```

**Figure 3: Precision-Recallæ›²ç·š**
```python
from sklearn.metrics import precision_recall_curve

# é–¾å€¤ã‚’å¤‰åŒ–ã•ã›ãŸã¨ãã®æŒ™å‹•
# ææ¡ˆæ‰‹æ³• vs Baseline ã®æ¯”è¼ƒ
```

**Figure 4: Confusion Matrix**
```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
```

**Figure 5: é¡ä¼¼åº¦åˆ†å¸ƒ**
```python
# åŒã˜ã‚¤ãƒ™ãƒ³ãƒˆãƒšã‚¢ vs ç•°ãªã‚‹ã‚¤ãƒ™ãƒ³ãƒˆãƒšã‚¢
# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã§åˆ†é›¢åº¦ã‚’å¯è¦–åŒ–
plt.hist(same_event_scores, alpha=0.5, label='Same Event', bins=20)
plt.hist(diff_event_scores, alpha=0.5, label='Different Event', bins=20)
plt.legend()
```

**Figure 6: Ablation Studyï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰**
ï¼ˆPhase 5ã§è¨˜è¼‰æ¸ˆã¿ï¼‰

---

### **Phase 7: è«–æ–‡åŸ·ç­† (Day 9-12)**

#### 7.1 è«–æ–‡æ§‹æˆï¼ˆACM/IEEEå½¢å¼ï¼‰

```
Title: Multi-Lingual Event Detection Across Live Streaming Platforms 
       Using N-gram Preserving Topic Modeling

Abstract (150-200 words)
  - Problem: Multi-stream event detection in live streaming
  - Challenge: Multi-lingual, different perspectives, temporal misalignment
  - Method: N-gram preservation + multi-modal similarity
  - Results: F1-score 0.61 (vs baseline 0.48)
  - Contribution: First work on multi-lingual live streaming event detection

1. Introduction
  - Background: Rise of live streaming platforms
  - Problem: Detecting same events across multiple streams
  - Challenges: Language barrier, different viewpoints
  - Our solution: Combine embedding, topic, lexical, temporal
  - Contributions (3 points)

2. Related Work
  - 2.1 Event Detection in Social Media
  - 2.2 Live Streaming Analysis
  - 2.3 Time Series Similarity
  - 2.4 Multi-lingual Text Mining

3. Problem Formulation
  - Input: N streams Ã— M comments
  - Output: Event pairs with similarity scores
  - Evaluation: Precision, Recall, F1

4. Proposed Method
  - 4.1 System Overview (Figure 1)
  - 4.2 Event Detection
    - BERTopic for initial clustering
    - Peak detection in time series
  - 4.3 N-gram Preservation
    - TfidfVectorizer with ngram_range=(1,3)
    - Maintain phrase structure
  - 4.4 Multi-modal Similarity
    - Embedding (SentenceTransformer)
    - Topic (Jaccard with N-grams)
    - Lexical (word overlap)
    - Temporal (correlation)
  - 4.5 Event Matching
    - Weighted combination
    - Threshold-based decision

5. Experiments
  - 5.1 Dataset
    - 4 soccer matches
    - 4 broadcasters (JA/EN/PT)
    - 12,543 comments total
  - 5.2 Ground Truth Creation
    - Manual annotation by expert
    - 28 pairs labeled
  - 5.3 Evaluation Metrics
    - Precision, Recall, F1-score
  - 5.4 Baseline Methods
  - 5.5 Implementation Details

6. Results
  - 6.1 Overall Performance (Table 3)
  - 6.2 Ablation Study (Table 4, Figure 6)
  - 6.3 Case Studies (Figure 2)
  - 6.4 Error Analysis

7. Discussion
  - Key findings
  - Limitations (small dataset, manual GT)
  - Future work (larger scale, automatic GT)

8. Conclusion

References (15-20 papers)
```

#### 7.2 åŸ·ç­†ã®ãƒã‚¤ãƒ³ãƒˆ

**å®¢è¦³æ€§**:
- ã™ã¹ã¦ã®ä¸»å¼µã«æ•°å€¤çš„æ ¹æ‹ 
- çµ±è¨ˆçš„æ¤œå®šï¼ˆt-testï¼‰
- å†ç¾æ€§ã®ä¿è¨¼ï¼ˆã‚³ãƒ¼ãƒ‰å…¬é–‹äºˆå®šï¼‰

**è«–ç†æ€§**:
- æ˜ç¢ºãªæµã‚Œ: Problem â†’ Method â†’ Experiment â†’ Result
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ¥ç¶šã‚’æ„è­˜
- Figure/Tableã¯æœ¬æ–‡ã‹ã‚‰å¿…ãšå‚ç…§

**æ–°è¦æ€§**:
- Related Workã§æ—¢å­˜æ‰‹æ³•ã¨ã®å·®ã‚’æ˜ç¢ºåŒ–
- æœ¬ç ”ç©¶ã®ç‹¬è‡ªæ€§ã‚’ç¹°ã‚Šè¿”ã—å¼·èª¿
- å¿œç”¨å¯èƒ½æ€§ã‚’ç¤ºå”†

---

### **Phase 8: æŠ•ç¨¿æº–å‚™ (Day 13-14)**

#### 8.1 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¼šè­°

**Tier 1ï¼ˆãƒˆãƒƒãƒ—ï¼‰**:
- ACM Multimedia (MM)
- ICWSM
- WWW

**Tier 2ï¼ˆè‰¯ã„ï¼‰**:
- ASONAM
- ICME
- SocialNLP Workshop

**ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«**:
- Social Network Analysis and Mining (SNAM)
- Multimedia Tools and Applications

#### 8.2 æŠ•ç¨¿å‰ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] Abstract: 150-200 words
- [ ] Introduction: æ˜ç¢ºãªè²¢çŒ®3ç‚¹
- [ ] Related Work: 15æœ¬ä»¥ä¸Šå¼•ç”¨
- [ ] Method: å†ç¾å¯èƒ½ãªè¨˜è¿°
- [ ] Experiments: Ground Truthè©³ç´°
- [ ] Results: Figure 6ç‚¹, Table 5ç‚¹
- [ ] Discussion: é™ç•Œã‚’æ­£ç›´ã«è¨˜è¿°
- [ ] References: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµ±ä¸€
- [ ] å›³è¡¨: é«˜è§£åƒåº¦ï¼ˆ300 dpiä»¥ä¸Šï¼‰
- [ ] æ–‡æ³•ãƒã‚§ãƒƒã‚¯ï¼ˆGrammarlyï¼‰
- [ ] å‰½çªƒãƒã‚§ãƒƒã‚¯ï¼ˆTurnitinï¼‰

---

## ğŸ“… 14æ—¥é–“ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

| Day | Phase | ã‚¿ã‚¹ã‚¯ | æ™‚é–“ | æˆæœç‰© |
|-----|-------|--------|------|--------|
| 1 | 1 | Ground Truthä½œæˆ | 3h | ground_truth.json |
| 1 | 1 | ç¾çŠ¶è©•ä¾¡ | 2h | F1=0.50 |
| 2 | 2 | è«–æ–‡èª¿æŸ»ï¼ˆ5æœ¬ï¼‰ | 4h | æ–‡çŒ®ãƒªã‚¹ãƒˆ |
| 3 | 2 | è«–æ–‡èª¿æŸ»ï¼ˆ10æœ¬ï¼‰ | 4h | Related Workè‰æ¡ˆ |
| 3 | 3 | Baselineå®Ÿè£… | 3h | baseline.py |
| 4 | 3 | Baselineè©•ä¾¡ | 2h | Table 3 |
| 4 | 4 | Multi-variate DTW | 3h | F1=0.55 |
| 5 | 4 | æ„Ÿæƒ…åˆ†æè¿½åŠ  | 3h | F1=0.57 |
| 6 | 4 | çµ±åˆãƒ»èª¿æ•´ | 4h | F1=0.61 |
| 6 | 5 | Ablation Studyè¨­è¨ˆ | 2h | å®Ÿé¨“è¨­è¨ˆ |
| 7 | 5 | Ablation Studyå®Ÿè¡Œ | 3h | Table 4 |
| 7 | 6 | å¯è¦–åŒ–ï¼ˆFigure 1-3ï¼‰ | 3h | å›³3ç‚¹ |
| 8 | 6 | å¯è¦–åŒ–ï¼ˆFigure 4-6ï¼‰ | 4h | å›³3ç‚¹ |
| 9 | 7 | Abstract+Intro | 3h | 2ã‚»ã‚¯ã‚·ãƒ§ãƒ³ |
| 10 | 7 | Related Work+Method | 4h | 2ã‚»ã‚¯ã‚·ãƒ§ãƒ³ |
| 11 | 7 | Experiments+Results | 4h | 2ã‚»ã‚¯ã‚·ãƒ§ãƒ³ |
| 12 | 7 | Discussion+Conclusion | 3h | 2ã‚»ã‚¯ã‚·ãƒ§ãƒ³ |
| 13 | 8 | å…¨ä½“æ¨æ•² | 4h | ãƒ‰ãƒ©ãƒ•ãƒˆv2 |
| 14 | 8 | æœ€çµ‚æ ¡æ­£ | 4h | æŠ•ç¨¿ç‰ˆ |

---

## âœ… ä»Šæ—¥ã®å…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆDay 1ï¼‰

### 1. Ground Truthä½œæˆï¼ˆå¿…é ˆï¼‰
```bash
python create_ground_truth.py --create
```
- 28ãƒšã‚¢ã‚’åˆ¤å®š
- ç´„2-3æ™‚é–“

### 2. è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
```bash
python create_ground_truth.py --evaluate
```
- Precision/Recall/F1ã‚’ç¢ºèª

### 3. çµæœã®å¯è¦–åŒ–
```bash
python quick_summary.py
python analyze_results.py
```

### 4. è«–æ–‡èª¿æŸ»ï¼ˆ5æœ¬ï¼‰
- Google Scholarã§æ¤œç´¢
- å„è«–æ–‡ã®æ¦‚è¦ã‚’ãƒ¡ãƒ¢

---

**ã“ã‚Œã§è«–æ–‡ãƒ¬ãƒ™ãƒ«10ã¸ã®é“ç­‹ãŒæ˜ç¢ºã«ãªã‚Šã¾ã—ãŸï¼**
**ã¾ãšã¯Ground Truthä½œæˆã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚**
