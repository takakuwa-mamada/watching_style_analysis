# -*- coding: utf-8 -*-
"""
YouTube/SNSã‚³ãƒ¡ãƒ³ãƒˆã®å›½åˆ¥ãƒ»æ™‚ç³»åˆ—ãƒˆãƒ”ãƒƒã‚¯å¯è¦–åŒ–ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
- ä¸Šä½10ãƒˆãƒ”ãƒƒã‚¯ã®ã¿ã‚’æç”»ï¼ˆå‡¡ä¾‹ã‚‚ä¸Šä½10ã®ã¿ï¼‰
- æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè‡ªå‹•è¨­å®šï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰
- ä¼¼é€šã£ãŸãƒˆãƒ”ãƒƒã‚¯ï¼ˆä¸Šä½èªãŒé«˜Jaccardï¼‰ã®è‡ªå‹•çµ±åˆ â†’ å‡¡ä¾‹ãƒ©ãƒ™ãƒ«ã‚’äººé–“å¯èª­ã«
- Windows ã§ã®ä¸¦åˆ—èµ·å‹•ã‚¨ãƒ©ãƒ¼å›é¿ï¼ˆfreeze_support / ãƒ—ãƒ­ã‚»ã‚¹æ•°åˆ¶å¾¡ï¼‰
- BERTopic ã®ä»£è¡¨èªæŠ½å‡ºã§åŸ‹ã‚è¾¼ã¿ãŒå¿…è¦ã«ãªã‚‹ã‚±ãƒ¼ã‚¹ã«å‚™ãˆã€embedding_model ã‚’æ˜ç¤ºè¨­å®š
"""

import os
import re
import glob
import itertools
import warnings
from collections import defaultdict, Counter
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd

# ===== Matplotlibï¼ˆæç”»&æ—¥æœ¬èªï¼‰ =====
import matplotlib
matplotlib.use("Agg")  # GUIä¸è¦ã®ç’°å¢ƒã§ã‚‚ä¿å­˜ã§ãã‚‹ã‚ˆã†ã«
import matplotlib.pyplot as plt
from matplotlib import font_manager, rcParams

# ---- æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰ ----
JP_FONT_CANDIDATES = [
    "Meiryo", "Yu Gothic", "Yu Gothic UI", "MS Gothic",
    "Hiragino Sans", "Hiragino Kaku Gothic ProN",
    "Noto Sans CJK JP", "IPAGothic",
]
available_fonts = {f.name for f in font_manager.fontManager.ttflist}
chosen_font = None
for name in JP_FONT_CANDIDATES:
    if name in available_fonts:
        rcParams["font.family"] = name
        chosen_font = name
        break
else:
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåŒæ¢±ãƒ•ã‚©ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰
    local_font = os.path.join(os.path.dirname(__file__), "fonts", "NotoSansCJKjp-Regular.otf")
    if os.path.exists(local_font):
        font_manager.fontManager.addfont(local_font)
        rcParams["font.family"] = "Noto Sans CJK JP"
        chosen_font = "Noto Sans CJK JP"

rcParams["axes.unicode_minus"] = False
print(f"[Matplotlib] Using font: {chosen_font or '<<not found - text may garble>>'}")

# ===== ãã®ã»ã‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª =====
from langdetect import detect, DetectorFactory
DetectorFactory.seed = 42

from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from bertopic.representation import MaximalMarginalRelevance  # ä»£è¡¨èªã‚’ã‚ã‹ã‚Šã‚„ã™ã
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import silhouette_score
from umap import UMAP
from hdbscan import HDBSCAN

from gensim.corpora import Dictionary
from gensim.models.coherencemodel import CoherenceModel

warnings.filterwarnings("ignore", category=UserWarning)

# =====================
# è¨­å®š
# =====================
CHAT_DIR = "data/chat"   # ãƒãƒ£ãƒƒãƒˆCSVã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
OUTPUT_DIR = "output"    # çµæœå‡ºåŠ›å…ˆ
os.makedirs(OUTPUT_DIR, exist_ok=True)

# å¤šè¨€èªåŸ‹ã‚è¾¼ã¿ï¼ˆç¿»è¨³ã›ãšã«ç›´æ¥ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
EMB_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
embedding_model_global = SentenceTransformer(EMB_NAME)

# BERTopic ã®ä¸‹å›ã‚Šï¼ˆå®‰å®šå¯„ã›ï¼‰
vectorizer_model = CountVectorizer(
    token_pattern=r"(?u)\b\w+\b", max_features=4000, min_df=2
)
umap_model = UMAP(
    n_components=5, n_neighbors=15, min_dist=0.00, metric="cosine", random_state=42
)
# Windowsã®ä¸¦åˆ— spawn å›é¿ã®ãŸã‚ core_dist_n_jobs=1 ã‚’å¼·åˆ¶
hdbscan_model = HDBSCAN(
    min_cluster_size=20,
    min_samples=5,
    metric="euclidean",
    cluster_selection_method="eom",
    prediction_data=True,
    core_dist_n_jobs=1,
)
# MMRã¯ c-TF-IDF ãƒ™ãƒ¼ã‚¹ã§ã€Œä»£è¡¨èªã€ãŒç›´æ„Ÿçš„ / embedding ä¸è¦
representation_model = MaximalMarginalRelevance(diversity=0.5)

# å‚è€ƒï¼šã‚·ãƒ¼ãƒ‰ãƒˆãƒ”ãƒƒã‚¯ï¼ˆä»»æ„ï¼‰
USE_SEED = True
seed_topic_list = [
    ["pitch", "strike", "ball", "fastball", "slider", "pitching"],
    ["bat", "homer", "home run", "slugger", "batting"],
    ["defense", "catch", "outfield", "infield", "double play"],
    ["umpire", "referee", "call", "review", "challenge"],
    ["injury", "hurt", "rehab", "out"],
    ["weather", "rain", "delay"],
    ["cheer", "chant", "song", "boo", "applause"],
    ["strategy", "tactics", "lineup", "substitution"],
]

# =====================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =====================
def preprocess_text(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = re.sub(r"http\S+|www\.\S+", " ", text)
    text = re.sub(r"@\w+", " ", text)
    text = re.sub(r"#\w+", " ", text)
    # çµµæ–‡å­—ã®è¶…ç°¡æ˜“ç½®æ›
    text = (text.replace("ğŸ˜‚", " laugh ")
                .replace("ğŸ˜­", " cry ")
                .replace("ğŸ‘", " clap ")
                .replace("ğŸ”¥", " fire "))
    # è¨˜å·ã‚’å‰Šé™¤ï¼ˆCJKã‚’æ®‹ã™ï¼‰
    text = re.sub(r"[^\w\s\u4E00-\u9FFF\u3040-\u30FF\uAC00-\uD7AF]", " ", text)
    text = re.sub(r"\s+", " ", text).strip().lower()
    return text

def detect_lang_safe(text: str) -> str:
    try:
        return detect(text)
    except Exception:
        return "unk"

def parse_country_from_filename(path: str) -> str:
    name = os.path.basename(path)
    m = re.findall(r"(japan|japanese|jpn|india|indian|dominican|usa|korea|korean|mexico|taiwan|china|chinese|france)", name.lower())
    if m:
        return m[-1].capitalize()
    if re.search(r"[\u3040-\u30ff\u4e00-\u9faf]", name):
        return "Japan"
    return "Unknown"

def read_csv_any(path: str) -> pd.DataFrame:
    """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’é †ã«è©¦ã—ã¦èª­ã‚€"""
    encodings = ["utf-8", "utf-8-sig", "cp932", "iso-8859-1"]
    for enc in encodings:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            continue
    return pd.read_csv(path, engine="python")

# ------- ãƒˆãƒ”ãƒƒã‚¯é‡è¤‡ã®è‡ªå‹•çµ±åˆï¼ˆä¸Šä½èªã®Jaccardã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼‰ -------
def jaccard_set(a: set, b: set) -> float:
    if not a and not b:
        return 0.0
    return len(a & b) / (len(a | b) + 1e-12)

def build_topic_groups(words_by_tid: Dict[int, List[Tuple[str, float]]],
                       jaccard_threshold: float = 0.6) -> List[List[int]]:
    """
    ä¸Šä½èªã®é›†åˆãŒä¼¼ã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’çµ±åˆã™ã‚‹ã€‚
    - words_by_tid: {topic_id: [(word, weight), ...]}
    - return: [[topic_id, ...], ...] ã®ã‚°ãƒ«ãƒ¼ãƒ—é…åˆ—
    """
    tids = sorted([t for t in words_by_tid.keys() if t != -1])
    sets = {t: set(w for w, _s in words_by_tid[t][:10]) for t in tids}

    parent = {t: t for t in tids}

    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x

    def union(x, y):
        rx, ry = find(x), find(y)
        if rx != ry:
            parent[ry] = rx

    for i, ti in enumerate(tids):
        for tj in tids[i+1:]:
            if jaccard_set(sets[ti], sets[tj]) >= jaccard_threshold:
                union(ti, tj)

    groups = defaultdict(list)
    for t in tids:
        groups[find(t)].append(t)

    return list(groups.values())

def merged_label(words_by_tid: Dict[int, List[Tuple[str, float]]],
                 members: List[int],
                 max_words: int = 4) -> str:
    """
    ã‚°ãƒ«ãƒ¼ãƒ—å†…ãƒˆãƒ”ãƒƒã‚¯ã®ä¸Šä½èªï¼ˆé‡ã¿å’Œï¼‰ã‹ã‚‰ä»£è¡¨ãƒ©ãƒ™ãƒ«ã‚’ä½œã‚‹ã€‚
    """
    counter = Counter()
    for t in members:
        for w, s in words_by_tid.get(t, [])[:10]:
            if isinstance(w, str) and w.strip():
                counter[w] += float(s)
    top = [w for w, _ in counter.most_common(max_words)]
    # æ—¥æœ¬èªãƒ»è‹±èªæ··åœ¨ã§ã‚‚å¯èª­ãªã‚ˆã†ã«ä¸­é»’åŒºåˆ‡ã‚Š
    return "ãƒ»".join(top)

# ------- ãƒˆãƒ”ãƒƒã‚¯æ™‚ç³»åˆ—ï¼ˆçµ±åˆå¾Œï¼‰ã‚’è¨ˆç®— -------
def compute_group_timeseries(topics_over_time: pd.DataFrame,
                             groups: List[List[int]]) -> pd.DataFrame:
    """
    BERTopicã®topics_over_timeï¼ˆåˆ—: ['Topic','Timestamp','Frequency']ï¼‰ã‚’
    ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆè¤‡æ•°topic_idã®çµ±åˆï¼‰ã«åˆç®—ã—ãŸæ™‚ç³»åˆ—(%ã«æ›ç®—)ã¸ã€‚
    """
    df = topics_over_time.copy()
    totals = df.groupby("Timestamp", as_index=True)["Frequency"].sum().rename("total")
    # group_id ã‚’ 0.. ã«å†ä»˜ä¸
    gmap = {}
    for gid, members in enumerate(groups):
        for t in members:
            gmap[t] = gid

    df["Group"] = df["Topic"].map(gmap).dropna()
    df_g = df.groupby(["Group", "Timestamp"], as_index=False)["Frequency"].sum()

    # å‰²åˆ(%)ã«ã™ã‚‹
    df_g = df_g.merge(totals, left_on="Timestamp", right_index=True, how="left")
    df_g["Percentage"] = 100.0 * df_g["Frequency"] / df_g["total"].clip(lower=1)
    df_g = df_g.drop(columns=["total"])
    return df_g

# ------- å¯è¦–åŒ–ï¼ˆä¸Šä½10ã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿ï¼‰ -------
def plot_top_groups(df_g: pd.DataFrame,
                    labels: Dict[int, str],
                    out_png: str,
                    title: str,
                    top_k: int = 10):
    # å„ã‚°ãƒ«ãƒ¼ãƒ—ã®ç·å‡ºç¾ã§ä¸Šä½Kã‚’é¸ã¶
    order = (df_g.groupby("Group")["Frequency"].sum()
                 .sort_values(ascending=False))
    top_groups = order.index.tolist()[:top_k]

    plt.figure(figsize=(12, 6))
    for gid in top_groups:
        d = df_g[df_g["Group"] == gid].sort_values("Timestamp")
        if d.empty:
            continue
        label = labels.get(gid, f"G{gid}")
        # å‡¡ä¾‹ãŒé•·éããªã„ã‚ˆã†ã«ãƒˆãƒªãƒ 
        if len(label) > 40:
            label = label[:37] + "..."
        plt.plot(d["Timestamp"], d["Percentage"], marker=".", linewidth=1.2, label=label)

    plt.title(title)
    plt.xlabel("Time")
    plt.ylabel("Percentage of comments")
    plt.legend(ncol=2, fontsize=9, frameon=False)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(out_png, dpi=220)
    plt.close()
    print(f"âœ… æ™‚ç³»åˆ—å›³: {out_png}")

# =====================
# ãƒ¡ã‚¤ãƒ³
# =====================
def main():
    csv_files = glob.glob(os.path.join(CHAT_DIR, "*.csv"))
    if not csv_files:
        print("CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:", CHAT_DIR)

    all_country_rows = []
    topic_words_rows = []
    metrics_rows = []

    for csv_file in csv_files:
        try:
            df = read_csv_any(csv_file)
            if df.empty or "message" not in df.columns:
                print(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {csv_file}")
                continue

            country = parse_country_from_filename(csv_file)

            # å‰å‡¦ç†
            df = df.dropna(subset=["message"]).copy()
            df["message_clean"] = df["message"].astype(str).apply(preprocess_text)
            df = df[df["message_clean"].str.len() > 0].copy()
            if df.empty:
                print(f"âš ï¸ ç©ºãƒ‡ãƒ¼ã‚¿: {csv_file}")
                continue

            df["lang"] = df["message_clean"].apply(detect_lang_safe)

            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            if "timestamp" in df.columns and not df["timestamp"].isnull().all():
                df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
                df = df.dropna(subset=["timestamp"]).copy()
            else:
                # ç–‘ä¼¼ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆå‡ç­‰é–“éš”ï¼‰
                df["timestamp"] = pd.date_range("2024-01-01", periods=len(df), freq="5S")

            texts = df["message_clean"].tolist()
            if len(texts) < 50:
                print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {csv_file}")
                continue

            # ===== åŸ‹ã‚è¾¼ã¿ =====
            emb = embedding_model_global.encode(
                texts, batch_size=64, show_progress_bar=False, normalize_embeddings=True
            )

            # ===== BERTopic =====
            topic_model = BERTopic(
                embedding_model=embedding_model_global,        # â† None ã ã¨è¡¨ç¾æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹å ´åˆãŒã‚ã‚‹
                vectorizer_model=vectorizer_model,
                umap_model=umap_model,
                hdbscan_model=hdbscan_model,
                calculate_probabilities=False,
                representation_model=representation_model,
                seed_topic_list=seed_topic_list if USE_SEED else None,
                min_topic_size=20,
                nr_topics=None,
                verbose=False,
            )

            topics, _ = topic_model.fit_transform(texts, embeddings=emb)

            # ç„¡åŠ¹ãƒˆãƒ”ãƒƒã‚¯é™¤å»
            valid_idx = [i for i, t in enumerate(topics) if t != -1]
            if len(valid_idx) < 30:
                print(f"âš ï¸ æœ‰åŠ¹ãƒˆãƒ”ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {csv_file}")
                continue

            df_valid = df.iloc[valid_idx].copy()
            topics_valid = [topics[i] for i in valid_idx]

            # ä¸Šä½èªã‚’å–å¾—ï¼ˆé‡ã¿ä»˜ãï¼‰
            words_by_tid: Dict[int, List[Tuple[str, float]]] = {}
            topic_info = topic_model.get_topic_info()
            valid_tids = sorted([t for t in topic_info["Topic"].tolist() if t != -1])
            for tid in valid_tids:
                items = topic_model.get_topic(tid) or []  # [(word, score), ...]
                # å®‰å…¨ã«ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæ–‡å­—åˆ—ã®ã¿ï¼‰
                items = [(str(w), float(s)) for w, s in items if isinstance(w, str) and str(w).strip()]
                words_by_tid[tid] = items

            # ===== æ™‚ç³»åˆ—: å…ƒã®ãƒˆãƒ”ãƒƒã‚¯å˜ä½ =====
            nr_bins = max(12, min(50, len(df_valid) // 100))
            try:
                tot = topic_model.topics_over_time(
                    docs=df_valid["message_clean"].tolist(),
                    topics=topics_valid,
                    timestamps=df_valid["timestamp"].tolist(),
                    nr_bins=nr_bins,
                    datetime_format=None,
                )
            except Exception:
                tot = topic_model.topics_over_time(
                    docs=df_valid["message_clean"].tolist(),
                    topics=topics_valid,
                    timestamps=df_valid["timestamp"].tolist(),
                    nr_bins=20,
                    datetime_format=None,
                )

            # ===== ä¼¼é€šã£ãŸãƒˆãƒ”ãƒƒã‚¯ã‚’è‡ªå‹•çµ±åˆ =====
            groups = build_topic_groups(words_by_tid, jaccard_threshold=0.6)
            # ã‚°ãƒ«ãƒ¼ãƒ—ä»£è¡¨ãƒ©ãƒ™ãƒ«
            gid_label: Dict[int, str] = {}
            for gid, members in enumerate(groups):
                gid_label[gid] = merged_label(words_by_tid, members, max_words=4)

            # ã‚°ãƒ«ãƒ¼ãƒ—æ™‚ç³»åˆ—ï¼ˆ%ï¼‰
            df_g = compute_group_timeseries(tot, groups)

            # ===== æç”»ï¼šä¸Šä½10ã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿ =====
            title = f"Topics Over Time (Top-10, merged) : {os.path.basename(csv_file)} [{country}]"
            out_img = os.path.join(
                OUTPUT_DIR, os.path.basename(csv_file).replace(".csv", "_timeline.png")
            )
            plot_top_groups(df_g, gid_label, out_img, title, top_k=10)

            # ===== å›½Ã—ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒï¼ˆå…¨ä½“æ¯”ç‡ï¼‰â†’ ã¾ãšå…ƒãƒˆãƒ”ãƒƒã‚¯ã§é›†è¨ˆ =====
            topic_counts = pd.Series(topics_valid).value_counts().sort_index()
            topic_share = (topic_counts / topic_counts.sum()).rename("share").to_frame()
            topic_share["country"] = country
            topic_share["topic_id"] = topic_share.index
            all_country_rows.append(topic_share.reset_index(drop=True))

            # ===== å‡ºåŠ›: å„ãƒˆãƒ”ãƒƒã‚¯ï¼ˆçµ±åˆå‰ï¼‰ã®ãƒˆãƒƒãƒ—èªï¼ˆå¯èª­æ€§å‘ä¸Šã®å‚è€ƒï¼‰=====
            for tid in valid_tids:
                top_terms = [w for w, _ in words_by_tid.get(tid, [])[:10]]
                topic_words_rows.append({
                    "file": os.path.basename(csv_file),
                    "country": country,
                    "topic_id": tid,
                    "top_words": ", ".join(top_terms),
                })

            # ===== å¦¥å½“æ€§è©•ä¾¡ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã‚’åšã‚ã«ï¼‰ =====
            tokens = [t.split() for t in df_valid["message_clean"].tolist()]
            dictionary = Dictionary(tokens)
            corpus = [dictionary.doc2bow(toks) for toks in tokens]

            top_words_per_topic = []
            for tid in sorted(topic_counts.index):
                # ä»£è¡¨èªï¼ˆæ–‡å­—åˆ—ï¼‰ã ã‘ã‚’æŠ½å‡ºã€2èªæœªæº€ã¯è©•ä¾¡å¯¾è±¡å¤–
                words = words_by_tid.get(tid, [])
                top_terms = [w for (w, _s) in words[:10] if isinstance(w, str) and w.strip()]
                if len(top_terms) >= 2:
                    top_words_per_topic.append(top_terms)

            if not top_words_per_topic:
                coh_npmi = float("nan")
                coh_umass = float("nan")
            else:
                # Windowsã®ä¸¦åˆ—èµ·å‹•å•é¡Œã‚’é¿ã‘ã‚‹ãŸã‚ processes=1 å›ºå®š
                coh_npmi = CoherenceModel(
                    topics=top_words_per_topic,
                    texts=tokens,
                    dictionary=dictionary,
                    coherence="c_npmi",
                    processes=1
                ).get_coherence()
                coh_umass = CoherenceModel(
                    topics=top_words_per_topic,
                    corpus=corpus,
                    dictionary=dictionary,
                    coherence="u_mass",
                    processes=1
                ).get_coherence()

            # Silhouetteï¼ˆã‚¯ãƒ©ã‚¹ã‚¿åˆ†é›¢åº¦ï¼‰
            try:
                sil = (
                    silhouette_score(np.asarray(embedding_model_global.encode(
                        df_valid["message_clean"].tolist(),
                        batch_size=64, show_progress_bar=False, normalize_embeddings=True
                    )), np.array(topics_valid))
                    if len(set(topics_valid)) > 1 else float("nan")
                )
            except Exception:
                sil = float("nan")

            # æ¦‚æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            n_unique_topics = int(len(set([t for t in topics_valid if t != -1])))
            metrics_rows.append({
                "file": os.path.basename(csv_file),
                "country": country,
                "n_docs": int(len(df)),
                "n_valid": int(len(df_valid)),
                "n_topics": n_unique_topics,
                "coherence_c_npmi": coh_npmi,
                "coherence_umass": coh_umass,
                "silhouette": sil,
            })

        except Exception as e:
            import traceback
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {os.path.basename(csv_file)} - {e}")
            traceback.print_exc()

    # ===== å‡ºåŠ›ã¾ã¨ã‚ =====
    if all_country_rows:
        country_topic_share_all = pd.concat(all_country_rows, ignore_index=True)
        # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã«è·¨ã‚‹å ´åˆã¯å¹³å‡
        country_topic_share = (
            country_topic_share_all.groupby(["country", "topic_id"], as_index=False)["share"].mean()
        )
        country_topic_share.to_csv(
            os.path.join(OUTPUT_DIR, "country_topic_share.csv"),
            index=False, encoding="utf-8-sig"
        )

    if metrics_rows:
        pd.DataFrame(metrics_rows).to_csv(
            os.path.join(OUTPUT_DIR, "metrics.csv"),
            index=False, encoding="utf-8-sig"
        )

    if topic_words_rows:
        pd.DataFrame(topic_words_rows).to_csv(
            os.path.join(OUTPUT_DIR, "topic_words.csv"),
            index=False, encoding="utf-8-sig"
        )

    print("ğŸ¯ ã™ã¹ã¦å®Œäº†")


if __name__ == "__main__":
    # Windows ã®ä¸¦åˆ—èµ·å‹•å•é¡Œå¯¾ç­–
    import multiprocessing as mp
    mp.freeze_support()
    main()
