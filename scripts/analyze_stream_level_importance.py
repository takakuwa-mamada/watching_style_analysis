#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é…ä¿¡å˜ä½ã§ã®è©¦åˆé‡è¦åº¦åˆ†æï¼ˆçµ±è¨ˆçš„æ¤œå‡ºåŠ›å¼·åŒ–ç‰ˆï¼‰

æ”¹å–„ç‚¹:
- è©¦åˆå˜ä½ï¼ˆN=6ï¼‰ã‹ã‚‰é…ä¿¡å˜ä½ï¼ˆN=31ï¼‰ã¸ã®å¤‰æ›´
- Cohen's dãŒæ­£ã—ãè¨ˆç®—ã•ã‚Œã‚‹
- ã‚ˆã‚Šè©³ç´°ãªçµ±è¨ˆåˆ†æï¼ˆBootstrap CIã€ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒï¼‰
- é…ä¿¡è€…ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•æŠ½å‡º
"""

import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import kruskal, mannwhitneyu
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['Yu Gothic', 'Meiryo', 'MS Gothic']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ‘ã‚¹è¨­å®š
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
OUTPUT_DIR = BASE_DIR / "output" / "stream_level_match_importance_analysis"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

print("=" * 80)
print("é…ä¿¡å˜ä½ è©¦åˆé‡è¦åº¦åˆ†æ - Stream-Level Match Importance Analysis")
print("=" * 80)


def load_match_metadata():
    """è©¦åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    metadata_path = DATA_DIR / "match_metadata.csv"
    df = pd.read_csv(metadata_path, encoding='utf-8-sig')
    print(f"\nâœ“ è©¦åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)} è©¦åˆ")
    return df


def detect_language_from_filename(filename):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰è¨€èªã‚’æ¨å®š"""
    filename_lower = filename.lower()
    
    # æ—¥æœ¬èª
    if any(char in filename for char in ['ã€', 'ã€‘', 'é…ä¿¡', 'åŒæ™‚è¦–è´', 'åˆ†æ']):
        return 'Japanese'
    
    # ã‚¹ãƒšã‚¤ãƒ³èª
    if any(word in filename_lower for word in ['directo', 'vivo', 'minuto', 'jornada', 'liga']):
        return 'Spanish'
    
    # ãƒ•ãƒ©ãƒ³ã‚¹èª
    if any(word in filename_lower for word in ['live', 'place', 'spectacle', 'forme']):
        if 'barcelone' in filename_lower or 'clasico' in filename_lower:
            return 'French'
    
    # ãƒãƒ«ãƒˆã‚¬ãƒ«èª
    if filename.startswith('Bra'):
        return 'Portuguese'
    
    # è‹±èªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    return 'English'


def analyze_single_stream(match_folder, csv_file, tier_info):
    """å˜ä¸€é…ä¿¡ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ"""
    try:
        df = pd.read_csv(csv_file, encoding='utf-8-sig')
        
        # ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–
        if 'message' in df.columns and 'comment' not in df.columns:
            df.rename(columns={'message': 'comment'}, inplace=True)
        
        if 'comment' not in df.columns:
            return None
        
        # åŸºæœ¬æƒ…å ±
        result = {
            'stream_id': csv_file.stem.replace('_chat_log', ''),
            'match_folder': match_folder,
            'match_name': tier_info['match_name_ja'],
            'tier': tier_info['tier'],
            'tier_label': tier_info['tier_label'],
            'importance_score': tier_info['importance_score'],
            'league': tier_info['league'],
            'match_type': tier_info['match_type'],
            'detected_language': detect_language_from_filename(csv_file.name),
            'total_comments': len(df)
        }
        
        # æ„Ÿæƒ…è¡¨ç¾æŒ‡æ¨™
        df['has_emoji'] = df['comment'].str.contains(
            r'[\U0001F000-\U0001F9FF]|[\u2600-\u27BF]|[\u2B50]|[\u26BD]|[\u26A1]|[\u2764]|[\U0001FA00-\U0001FAFF]',
            regex=True, na=False
        )
        result['emoji_rate'] = df['has_emoji'].mean() * 100
        
        df['has_exclamation'] = df['comment'].str.contains('!|ï¼', regex=True, na=False)
        result['exclamation_rate'] = df['has_exclamation'].mean() * 100
        
        df['comment_length'] = df['comment'].str.len()
        result['mean_comment_length'] = df['comment_length'].mean()
        result['median_comment_length'] = df['comment_length'].median()
        
        # ãƒãƒ¼ã‚¹ãƒˆæŒ‡æ¨™
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
            df_sorted = df.dropna(subset=['timestamp']).sort_values('timestamp')
            
            if len(df_sorted) > 0:
                df_sorted['minute'] = df_sorted['timestamp'].dt.floor('1min')
                comments_per_minute = df_sorted.groupby('minute').size()
                
                if len(comments_per_minute) > 0:
                    result['max_cpm'] = comments_per_minute.max()
                    result['mean_cpm'] = comments_per_minute.mean()
                    result['median_cpm'] = comments_per_minute.median()
                    result['std_cpm'] = comments_per_minute.std()
                    result['burst_frequency'] = (comments_per_minute > comments_per_minute.mean() + comments_per_minute.std()).sum()
                else:
                    result.update({'max_cpm': np.nan, 'mean_cpm': np.nan, 'median_cpm': np.nan, 
                                  'std_cpm': np.nan, 'burst_frequency': np.nan})
            else:
                result.update({'max_cpm': np.nan, 'mean_cpm': np.nan, 'median_cpm': np.nan, 
                              'std_cpm': np.nan, 'burst_frequency': np.nan})
        
        # ãƒˆãƒ”ãƒƒã‚¯å¤šæ§˜æ€§
        all_text = ' '.join(df['comment'].dropna().astype(str))
        words = all_text.split()
        
        if len(words) > 0:
            word_counts = pd.Series(words).value_counts()
            probabilities = word_counts / word_counts.sum()
            entropy = -np.sum(probabilities * np.log2(probabilities))
            
            result['topic_entropy'] = entropy
            result['unique_words'] = len(word_counts)
            result['total_words'] = len(words)
            result['lexical_diversity'] = len(word_counts) / len(words) if len(words) > 0 else 0
        else:
            result.update({'topic_entropy': np.nan, 'unique_words': 0, 
                          'total_words': 0, 'lexical_diversity': 0})
        
        return result
        
    except Exception as e:
        print(f"  âš  ã‚¨ãƒ©ãƒ¼: {csv_file.name} - {e}")
        return None


def analyze_all_streams(metadata):
    """å…¨é…ä¿¡ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æï¼ˆé…ä¿¡å˜ä½ï¼‰"""
    print("\n" + "=" * 80)
    print("å…¨é…ä¿¡ãƒ‡ãƒ¼ã‚¿åˆ†æä¸­ï¼ˆé…ä¿¡å˜ä½ï¼‰...")
    print("=" * 80)
    
    results = []
    
    for _, match_row in metadata.iterrows():
        match_folder = match_row['match_folder']
        folder_path = DATA_DIR / "football" / match_folder
        
        if not folder_path.exists():
            continue
        
        print(f"\nğŸ“Š {match_row['match_name_ja']} (Tier {match_row['tier']})")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        csv_files = list(folder_path.glob("*_chat_log.csv"))
        if not csv_files:
            csv_files = list(folder_path.glob("*.csv"))
        
        tier_info = match_row.to_dict()
        
        for csv_file in csv_files:
            stream_result = analyze_single_stream(match_folder, csv_file, tier_info)
            if stream_result:
                results.append(stream_result)
                cpm_str = f"{stream_result['max_cpm']:.0f}" if not np.isnan(stream_result['max_cpm']) else 'N/A'
                print(f"  âœ“ {stream_result['stream_id'][:50]}: "
                      f"{stream_result['total_comments']:,}ä»¶, "
                      f"çµµæ–‡å­—{stream_result['emoji_rate']:.1f}%, "
                      f"CPM{cpm_str}")
    
    results_df = pd.DataFrame(results)
    
    # ä¿å­˜
    output_path = OUTPUT_DIR / "stream_level_raw_data.csv"
    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ é…ä¿¡å˜ä½ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜: {output_path}")
    print(f"  ç·é…ä¿¡æ•°: {len(results_df)}")
    
    return results_df


def perform_statistical_tests(df):
    """é…ä¿¡å˜ä½ã§ã®çµ±è¨ˆæ¤œå®š"""
    print("\n" + "=" * 80)
    print("çµ±è¨ˆæ¤œå®šå®Ÿè¡Œä¸­ï¼ˆé…ä¿¡å˜ä½ã€N={})...".format(len(df)))
    print("=" * 80)
    
    results = []
    
    metrics = [
        ('emoji_rate', 'çµµæ–‡å­—ç‡ (%)'),
        ('exclamation_rate', 'æ„Ÿå˜†ç¬¦ç‡ (%)'),
        ('mean_comment_length', 'å¹³å‡ã‚³ãƒ¡ãƒ³ãƒˆé•·'),
        ('max_cpm', 'æœ€å¤§CPM'),
        ('mean_cpm', 'å¹³å‡CPM'),
        ('topic_entropy', 'ãƒˆãƒ”ãƒƒã‚¯å¤šæ§˜æ€§')
    ]
    
    for metric, metric_name in metrics:
        if metric not in df.columns or df[metric].isna().all():
            continue
        
        print(f"\nğŸ“Š {metric_name}")
        
        # Tieråˆ¥ã®ãƒ‡ãƒ¼ã‚¿
        tier1 = df[df['tier'] == 1][metric].dropna()
        tier2 = df[df['tier'] == 2][metric].dropna()
        tier3 = df[df['tier'] == 3][metric].dropna()
        tier4 = df[df['tier'] == 4][metric].dropna()
        
        print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: Tier1={len(tier1)}, Tier2={len(tier2)}, Tier3={len(tier3)}, Tier4={len(tier4)}")
        
        # Kruskal-Wallisæ¤œå®š
        groups = [tier1, tier2, tier3, tier4]
        groups_valid = [g for g in groups if len(g) > 0]
        
        if len(groups_valid) >= 2:
            h_stat, p_value = kruskal(*groups_valid)
            significance = "âœ“ æœ‰æ„" if p_value < 0.05 else "éæœ‰æ„"
            print(f"  Kruskal-Wallis: H={h_stat:.3f}, p={p_value:.4f} ({significance})")
            
            # Tier 1 vs Tier 4 ã®è©³ç´°æ¯”è¼ƒ
            if len(tier1) > 0 and len(tier4) > 0:
                mean_diff = tier1.mean() - tier4.mean()
                pooled_std = np.sqrt((tier1.std()**2 + tier4.std()**2) / 2)
                cohens_d = mean_diff / pooled_std if pooled_std > 0 else np.nan
                
                u_stat, u_p = mannwhitneyu(tier1, tier4, alternative='two-sided')
                
                # åŠ¹æœé‡ã®è§£é‡ˆ
                if np.isnan(cohens_d):
                    effect_interpretation = "undefined"
                elif abs(cohens_d) < 0.2:
                    effect_interpretation = "negligible"
                elif abs(cohens_d) < 0.5:
                    effect_interpretation = "small"
                elif abs(cohens_d) < 0.8:
                    effect_interpretation = "medium"
                else:
                    effect_interpretation = "large"
                
                print(f"  Tier 1 vs Tier 4:")
                print(f"    å¹³å‡å·®: {mean_diff:.2f}")
                print(f"    Cohen's d: {cohens_d:.3f} ({effect_interpretation})")
                print(f"    Mann-Whitney U: p={u_p:.4f}")
                
                # Bootstrapä¿¡é ¼åŒºé–“
                n_bootstrap = 1000
                bootstrap_diffs = []
                for _ in range(n_bootstrap):
                    sample1 = np.random.choice(tier1, size=len(tier1), replace=True)
                    sample4 = np.random.choice(tier4, size=len(tier4), replace=True)
                    bootstrap_diffs.append(sample1.mean() - sample4.mean())
                
                ci_lower = np.percentile(bootstrap_diffs, 2.5)
                ci_upper = np.percentile(bootstrap_diffs, 97.5)
                print(f"    Bootstrap 95% CI: [{ci_lower:.2f}, {ci_upper:.2f}]")
                
                results.append({
                    'metric': metric,
                    'metric_name': metric_name,
                    'kruskal_h': h_stat,
                    'kruskal_p': p_value,
                    'tier1_n': len(tier1),
                    'tier1_mean': tier1.mean(),
                    'tier1_std': tier1.std(),
                    'tier1_median': tier1.median(),
                    'tier4_n': len(tier4),
                    'tier4_mean': tier4.mean(),
                    'tier4_std': tier4.std(),
                    'tier4_median': tier4.median(),
                    'mean_diff': mean_diff,
                    'cohens_d': cohens_d,
                    'effect_interpretation': effect_interpretation,
                    'mannwhitney_u': u_stat,
                    'mannwhitney_p': u_p,
                    'bootstrap_ci_lower': ci_lower,
                    'bootstrap_ci_upper': ci_upper
                })
    
    results_df = pd.DataFrame(results)
    
    # ä¿å­˜
    output_path = OUTPUT_DIR / "stream_level_statistical_tests.csv"
    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ çµ±è¨ˆæ¤œå®šçµæœä¿å­˜: {output_path}")
    
    return results_df


def create_visualizations(df, stats_df):
    """é…ä¿¡å˜ä½ã§ã®å¯è¦–åŒ–"""
    print("\n" + "=" * 80)
    print("å¯è¦–åŒ–ä½œæˆä¸­...")
    print("=" * 80)
    
    # å›³1: é…ä¿¡å˜ä½ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼ˆN=31è¡¨ç¤ºï¼‰
    fig, axes = plt.subplots(2, 3, figsize=(16, 10))
    fig.suptitle(f'é…ä¿¡å˜ä½ã§ã®è©¦åˆé‡è¦åº¦åˆ†æ (N={len(df)} streams)', 
                fontsize=16, fontweight='bold')
    
    metrics_to_plot = [
        ('emoji_rate', 'çµµæ–‡å­—ç‡ (%)'),
        ('exclamation_rate', 'æ„Ÿå˜†ç¬¦ç‡ (%)'),
        ('mean_comment_length', 'å¹³å‡ã‚³ãƒ¡ãƒ³ãƒˆé•·'),
        ('max_cpm', 'æœ€å¤§CPM'),
        ('mean_cpm', 'å¹³å‡CPM'),
        ('topic_entropy', 'ãƒˆãƒ”ãƒƒã‚¯å¤šæ§˜æ€§')
    ]
    
    for idx, (metric, title) in enumerate(metrics_to_plot):
        ax = axes[idx // 3, idx % 3]
        
        if metric in df.columns and not df[metric].isna().all():
            df_sorted = df.sort_values('tier')
            
            # ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ + ã‚¹ãƒˆãƒªãƒƒãƒ—ãƒ—ãƒ­ãƒƒãƒˆ
            sns.boxplot(data=df_sorted, x='tier_label', y=metric, ax=ax,
                       order=['Ultra-High', 'High', 'Medium', 'Low'],
                       palette='RdYlGn_r', showmeans=True)
            
            sns.stripplot(data=df_sorted, x='tier_label', y=metric, ax=ax,
                         order=['Ultra-High', 'High', 'Medium', 'Low'],
                         color='black', alpha=0.4, size=4)
            
            # på€¤ã‚’è¡¨ç¤º
            if not stats_df.empty and metric in stats_df['metric'].values:
                p_val = stats_df[stats_df['metric'] == metric]['kruskal_p'].values[0]
                sig_text = f'p={p_val:.4f}'
                if p_val < 0.001:
                    sig_text = 'p<0.001***'
                elif p_val < 0.01:
                    sig_text = f'p={p_val:.4f}**'
                elif p_val < 0.05:
                    sig_text = f'p={p_val:.4f}*'
                
                ax.text(0.5, 0.95, sig_text, transform=ax.transAxes,
                       ha='center', va='top', fontsize=9,
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
            
            ax.set_title(title, fontsize=11, fontweight='bold')
            ax.set_xlabel('è©¦åˆé‡è¦åº¦', fontsize=9)
            ax.set_ylabel(title, fontsize=9)
            ax.grid(axis='y', alpha=0.3)
        else:
            ax.text(0.5, 0.5, f'{title}\nãƒ‡ãƒ¼ã‚¿ãªã—', 
                   ha='center', va='center', transform=ax.transAxes)
    
    plt.tight_layout()
    output_path = OUTPUT_DIR / "stream_level_boxplot.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"  âœ“ ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜: {output_path.name}")
    plt.close()
    
    # å›³2: åŠ¹æœé‡ã¨æœ‰æ„æ€§ã®å¯è¦–åŒ–
    if not stats_df.empty:
        fig, ax = plt.subplots(figsize=(10, 6))
        
        stats_df_sorted = stats_df.sort_values('cohens_d', ascending=True)
        colors = ['red' if p < 0.05 else 'orange' if p < 0.10 else 'gray' 
                 for p in stats_df_sorted['kruskal_p']]
        
        bars = ax.barh(range(len(stats_df_sorted)), stats_df_sorted['cohens_d'], color=colors)
        ax.set_yticks(range(len(stats_df_sorted)))
        ax.set_yticklabels(stats_df_sorted['metric_name'])
        ax.set_xlabel("Cohen's d (Tier 1 vs Tier 4)", fontsize=11)
        ax.set_title("åŠ¹æœé‡ã¨çµ±è¨ˆçš„æœ‰æ„æ€§ï¼ˆé…ä¿¡å˜ä½ N=31ï¼‰", fontsize=13, fontweight='bold')
        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
        ax.axvline(x=0.2, color='green', linestyle='--', alpha=0.5, label='Small effect')
        ax.axvline(x=0.5, color='orange', linestyle='--', alpha=0.5, label='Medium effect')
        ax.axvline(x=0.8, color='red', linestyle='--', alpha=0.5, label='Large effect')
        ax.legend(loc='lower right', fontsize=9)
        ax.grid(axis='x', alpha=0.3)
        
        # å‡¡ä¾‹
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='red', label='p<0.05 (æœ‰æ„)'),
            Patch(facecolor='orange', label='p<0.10 (å‚¾å‘)'),
            Patch(facecolor='gray', label='pâ‰¥0.10 (éæœ‰æ„)')
        ]
        ax.legend(handles=legend_elements, loc='upper right', fontsize=9)
        
        plt.tight_layout()
        output_path = OUTPUT_DIR / "effect_size_and_significance.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"  âœ“ åŠ¹æœé‡ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜: {output_path.name}")
        plt.close()
    
    print("\nâœ“ å…¨å¯è¦–åŒ–å®Œäº†")


def create_summary_report(df, stats_df):
    """é…ä¿¡å˜ä½åˆ†æã®ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ"""
    print("\n" + "=" * 80)
    print("ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­...")
    print("=" * 80)
    
    report = []
    report.append("# é…ä¿¡å˜ä½ è©¦åˆé‡è¦åº¦åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n\n")
    report.append(f"**åˆ†ææ—¥æ™‚**: {pd.Timestamp.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n")
    report.append(f"**åˆ†æå˜ä½**: é…ä¿¡å˜ä½ï¼ˆStream-levelï¼‰\n")
    report.append(f"**ç·é…ä¿¡æ•°**: N={len(df)}\n\n")
    report.append("---\n\n")
    
    # ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
    report.append("## ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦\n\n")
    tier_summary = df.groupby('tier_label').agg({
        'total_comments': 'sum',
        'stream_id': 'count',
        'emoji_rate': 'mean',
        'exclamation_rate': 'mean',
        'max_cpm': 'mean'
    }).reindex(['Ultra-High', 'High', 'Medium', 'Low'])
    
    report.append("| Tier | é…ä¿¡æ•° | ç·ã‚³ãƒ¡ãƒ³ãƒˆæ•° | å¹³å‡çµµæ–‡å­—ç‡(%) | å¹³å‡æ„Ÿå˜†ç¬¦ç‡(%) | å¹³å‡æœ€å¤§CPM |\n")
    report.append("|------|--------|-------------|----------------|----------------|-----------|\n")
    for tier_label, row in tier_summary.iterrows():
        report.append(f"| {tier_label} | {row['stream_id']:.0f} | {row['total_comments']:,.0f} | "
                     f"{row['emoji_rate']:.2f} | {row['exclamation_rate']:.2f} | {row['max_cpm']:.0f} |\n")
    report.append("\n")
    
    # çµ±è¨ˆçš„æœ‰æ„å·®
    report.append("## ğŸ“ˆ çµ±è¨ˆçš„æœ‰æ„å·®ã®æ¤œå‡º\n\n")
    
    significant_results = stats_df[stats_df['kruskal_p'] < 0.05]
    if len(significant_results) > 0:
        report.append("### âœ… æœ‰æ„å·®ãŒæ¤œå‡ºã•ã‚ŒãŸæŒ‡æ¨™ï¼ˆp<0.05ï¼‰\n\n")
        for _, row in significant_results.iterrows():
            report.append(f"- **{row['metric_name']}**\n")
            report.append(f"  - Kruskal-Wallis: H={row['kruskal_h']:.3f}, **p={row['kruskal_p']:.4f}**\n")
            report.append(f"  - Tier 1 å¹³å‡: {row['tier1_mean']:.2f} (N={row['tier1_n']:.0f})\n")
            report.append(f"  - Tier 4 å¹³å‡: {row['tier4_mean']:.2f} (N={row['tier4_n']:.0f})\n")
            report.append(f"  - åŠ¹æœé‡: Cohen's d={row['cohens_d']:.3f} ({row['effect_interpretation']})\n")
            report.append(f"  - Bootstrap 95% CI: [{row['bootstrap_ci_lower']:.2f}, {row['bootstrap_ci_upper']:.2f}]\n\n")
    
    trend_results = stats_df[(stats_df['kruskal_p'] >= 0.05) & (stats_df['kruskal_p'] < 0.10)]
    if len(trend_results) > 0:
        report.append("### ğŸ”¶ å‚¾å‘ãŒè¦‹ã‚‰ã‚ŒãŸæŒ‡æ¨™ï¼ˆ0.05â‰¤p<0.10ï¼‰\n\n")
        for _, row in trend_results.iterrows():
            report.append(f"- **{row['metric_name']}**: p={row['kruskal_p']:.4f}, d={row['cohens_d']:.3f}\n")
        report.append("\n")
    
    # æ”¹å–„ã®ç¢ºèª
    report.append("## ğŸ” è©¦åˆå˜ä½åˆ†æã‹ã‚‰ã®æ”¹å–„\n\n")
    report.append("| é …ç›® | è©¦åˆå˜ä½ (N=6) | é…ä¿¡å˜ä½ (N=31) | æ”¹å–„ |\n")
    report.append("|------|---------------|----------------|------|\n")
    report.append("| ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º | 6è©¦åˆ | 31é…ä¿¡ | **5.2å€** |\n")
    report.append("| Cohen's dè¨ˆç®— | ä¸å¯ (NaN) | **å¯èƒ½** | âœ… |\n")
    report.append("| çµ±è¨ˆçš„æ¤œå‡ºåŠ› | ä½ | **ä¸­ï½é«˜** | âœ… |\n")
    if len(significant_results) > 0:
        report.append(f"| æœ‰æ„å·®æ¤œå‡º | 0æŒ‡æ¨™ | **{len(significant_results)}æŒ‡æ¨™** | âœ… |\n")
    report.append("\n")
    
    # ä¿å­˜
    output_path = OUTPUT_DIR / "STREAM_LEVEL_ANALYSIS_SUMMARY.md"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.writelines(report)
    
    print(f"\nâœ“ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_path}")
    print("\n" + "".join(report))


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("\nğŸš€ é…ä¿¡å˜ä½ è©¦åˆé‡è¦åº¦åˆ†æé–‹å§‹\n")
    
    # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    metadata = load_match_metadata()
    
    # 2. å…¨é…ä¿¡ã®ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆé…ä¿¡å˜ä½ï¼‰
    results_df = analyze_all_streams(metadata)
    
    # 3. çµ±è¨ˆæ¤œå®š
    stats_df = perform_statistical_tests(results_df)
    
    # 4. å¯è¦–åŒ–
    create_visualizations(results_df, stats_df)
    
    # 5. ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
    create_summary_report(results_df, stats_df)
    
    print("\n" + "=" * 80)
    print("âœ… é…ä¿¡å˜ä½ è©¦åˆé‡è¦åº¦åˆ†æå®Œäº†!")
    print("=" * 80)
    print(f"\nğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {OUTPUT_DIR}")
    print("\nç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
    for file in sorted(OUTPUT_DIR.glob("*")):
        print(f"  - {file.name}")
    print()


if __name__ == "__main__":
    main()
