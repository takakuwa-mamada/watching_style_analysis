# -*- coding: utf-8 -*-
"""
event_comparison.py  (å…¨é¢æ”¹è‰¯ç‰ˆ)

ç›®çš„:
- åŒä¸€è©¦åˆã‚’æ‰±ã†è¤‡æ•°ã®é…ä¿¡è€…CSVã§ã€ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡ºâ†’ä¼¼ãƒˆãƒ”ãƒƒã‚¯çµ±åˆâ†’æ™‚ç³»åˆ—(%)
- ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãƒˆãƒ”ãƒƒã‚¯ã®æ™‚ç³»åˆ—ã‹ã‚‰ãƒ”ãƒ¼ã‚¯ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆè¤‡æ•°ï¼‰ã‚’æŠ½å‡º
- ã‚¹ãƒˆãƒªãƒ¼ãƒ é–“ã§ã€Œèªå½™é¡ä¼¼(Jaccard) Ã— æ™‚é–“ãšã‚Œã€æ¡ä»¶ã§å…±é€šã‚¤ãƒ™ãƒ³ãƒˆã¨ã—ã¦ç…§åˆ
- å„å…±é€šã‚¤ãƒ™ãƒ³ãƒˆã®é…ä¿¡è€…é–“ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹å·®ã‚’ Jensenâ€“Shannonè·é›¢ã§å®šé‡åŒ–
- ã™ã¹ã¦ã®CSVå‡ºåŠ›ã«å¯¾å¿œã™ã‚‹å¯è¦–åŒ–PNGã‚’è‡ªå‹•ç”Ÿæˆï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã€è·é›¢è¡Œåˆ—ã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€æ™‚ç³»åˆ—ï¼‰

ä½¿ã„æ–¹ä¾‹ (PowerShell):
  # ãƒ•ã‚©ãƒ«ãƒ€æŒ‡å®šï¼ˆæ¨å¥¨ï¼‰
  python event_comparison.py `
    --folder "data/football" `
    --pattern "*.csv" `
    --time-bins 300 `
    --peak-pad 1 `
    --topk 200 `
    --time-match-th 60 `
    --jaccard-th 0.5 `
    --word-match-th 0.4 `
    --save-json

  # ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã‚„æ—¥æœ¬èªãƒ‘ã‚¹ã¯PowerShellå¤‰æ•°çµŒç”±ãŒå®‰å…¨ï¼‰
  $files = Get-ChildItem -Path "data/football" -Filter *.csv | ForEach-Object { $_.FullName }
  python event_comparison.py --files $files --time-bins 300 --n-events 5 --save-json

å‡ºåŠ›:
- output/event_comparison_results.csv        â€¦ å„å…±é€šã‚¤ãƒ™ãƒ³ãƒˆã®è·é›¢çµ±è¨ˆ
- output/event_comparison_results.png        â€¦ ä¸Šã®è·é›¢è¡Œåˆ—ã®å¯è¦–åŒ–
- output/event_eventmap.csv                  â€¦ å…±é€šã‚¤ãƒ™ãƒ³ãƒˆÃ—é…ä¿¡è€…ã®ã€Œæœ‰ç„¡ã€ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— (çŸ©å½¢è¡¨)
- output/event_eventmap.png                  â€¦ â†‘ã®å¯è¦–åŒ–
- output/event_comments.json                 â€¦ ï¼ˆ--save-jsonæ™‚ï¼‰å„ã‚¤ãƒ™ãƒ³ãƒˆÃ—é…ä¿¡è€…ã®æŠ½å‡ºã‚³ãƒ¡ãƒ³ãƒˆ
- output/wordclouds/event_<EID>/WC_<basename>.png   â€¦ ã‚¤ãƒ™ãƒ³ãƒˆã”ã¨ã®ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
- output/timelines/<basename>_timeline.png         â€¦ å„é…ä¿¡è€…ã®Top-10(çµ±åˆ)æ™‚ç³»åˆ—
"""

import argparse
import os, re, json
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Optional

# ===== Windows UTF-8å¯¾å¿œ =====
import sys
import io
# æ¨™æº–å‡ºåŠ›ã‚’UTF-8ã«è¨­å®šï¼ˆWindows cp932ã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# ===== è¿½åŠ ãƒ©ã‚¤ãƒ–ãƒ©ãƒª =====
# è¨€èªæ¤œå‡ºãƒ©ã‚¤ãƒ–ãƒ©ãƒª langdetect ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ç’°å¢ƒã«å¯¾å¿œã™ã‚‹ãŸã‚ã€
# try-import ã‚’ç”¨ã„ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ç”¨æ„ã™ã‚‹ã€‚
try:
    from langdetect import detect, DetectorFactory  # type: ignore
    DetectorFactory.seed = 42
    _LANGDETECT_AVAILABLE = True
except ImportError:
    _LANGDETECT_AVAILABLE = False
    # ãƒ€ãƒŸãƒ¼é–¢æ•°ã‚’å®šç¾©
    def detect(text: str) -> str:
        return "unk"
    class DetectorFactory:
        seed = None

# ===== Noise Filterçµ±åˆ =====
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.noise_filter import NoiseFilter
NOISE_FILTER = NoiseFilter()


import numpy as np
import pandas as pd

# ===== Matplotlibï¼ˆæç”»&æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆï¼‰ =====
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from matplotlib import font_manager, rcParams
import unicodedata  # for converting emoji characters to names in charts

# æ—¥æœ¬èªã¨çµµæ–‡å­—ãƒ•ã‚©ãƒ³ãƒˆã®å€™è£œã‚’è¨­å®š
JP_FONT_CANDIDATES = [
    "Meiryo", "Yu Gothic", "Yu Gothic UI", "MS Gothic",
    "Hiragino Sans", "Hiragino Kaku Gothic ProN",
    "Noto Sans CJK JP", "IPAGothic",
]
EMOJI_FONT_CANDIDATES = [
    "Noto Color Emoji",   # Linux
    "Segoe UI Emoji",     # Windows
    "Apple Color Emoji",  # macOS
    "Twemoji Mozilla",    # Fallback
]

_available = {f.name for f in font_manager.fontManager.ttflist}
_used_jp_font = None
for _name in JP_FONT_CANDIDATES:
    if _name in _available:
        _used_jp_font = _name
        break

# å„ªå…ˆã™ã‚‹çµµæ–‡å­—ãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’å–å¾—
_emoji_font_path: Optional[str] = None
for _ename in EMOJI_FONT_CANDIDATES:
    try:
        _emoji_font_path = font_manager.findfont(_ename, fallback_to_default=False)
        # `findfont` may return a path even if font is not found; ensure name actually matches
        if _ename in _available:
            break
        # If not, still accept the found path
        break
    except Exception:
        continue
# è¿½åŠ : ã‚«ãƒ©ãƒ¼ãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ç›´æ¥NotoColorEmoji.ttfã‚’æŒ‡å®šã™ã‚‹
if not _emoji_font_path or ("Color" not in os.path.basename(_emoji_font_path)):
    possible_color_paths = [
        "/usr/share/fonts/truetype/noto/NotoColorEmoji.ttf",
        "/usr/share/fonts/truetype/noto/NotoColorEmoji-Regular.ttf",
        "/usr/share/fonts/truetype/noto/NotoColorEmoji-Regular.otf",
    ]
    for _p in possible_color_paths:
        if os.path.exists(_p):
            _emoji_font_path = _p
            break

# Set Matplotlib global font family as list: first Japanese font if available, then sans-serif
families: List[str] = []
if _used_jp_font:
    families.append(_used_jp_font)
# Do not set emoji font globally because color emojis may not render properly in all contexts; instead use per-plot
families.append("sans-serif")
rcParams["font.family"] = families
rcParams["axes.unicode_minus"] = False
print(f"[Matplotlib] Using fonts: {', '.join(families)} (emoji font path: {_emoji_font_path})")

# ---- Label helpers ----
def _abbr_stream_name(path: str, maxlen: int = 12) -> str:
    """Return a short alias from a CSV path (basename without extension, truncated)."""
    base = os.path.basename(path)
    name = base.replace('.csv', '')
    # common noise cleanup
    for noise in ["_chat_log", "chat_log", "_log", "log"]:
        name = name.replace(noise, "")
    return (name if len(name) <= maxlen else (name[:maxlen-1] + "â€¦"))

# ===== Topicé–¢é€£ =====
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from bertopic.representation import MaximalMarginalRelevance
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from umap import UMAP
from hdbscan import HDBSCAN

# ===== å‡ºåŠ›å…ˆ =====
OUT_DIR = "output"
os.makedirs(OUT_DIR, exist_ok=True)

# ===== æ©Ÿèƒ½ãƒ•ãƒ©ã‚°ï¼ˆé•·æœŸçš„ãªæ”¹å–„ã®ãŸã‚ï¼‰ =====
ENABLE_WORDCLOUDS = False        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆï¼ˆé‡ã„å‡¦ç†ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚å¤–ï¼‰
ENABLE_DETAILED_METRICS = False  # è©³ç´°ãªãƒšã‚¢ãƒ¯ã‚¤ã‚ºãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚å¤–ï¼‰
ENABLE_JSON_EXPORT = True        # JSONå‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰

# ===== ãã®ã»ã‹ =====
# åˆ©ç”¨ã™ã‚‹SentenceTransformerãƒ¢ãƒ‡ãƒ«ã€‚
# å­¦ä¼šç™ºè¡¨ç”¨ï¼šè»½é‡ã§é«˜é€Ÿãªãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´ï¼ˆå‡¦ç†æ™‚é–“ã‚’50-70%å‰Šæ¸›ï¼‰
# ç²¾åº¦ã¯è‹¥å¹²ä½ä¸‹ã™ã‚‹ãŒã€å¤šè¨€èªå¯¾å¿œã¯ç¶­æŒ
EMB_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
# å…ƒã®ãƒ¢ãƒ‡ãƒ«ï¼ˆé‡ã„ï¼‰: "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens"

# ç”¨èªæ­£è¦åŒ–ï¼ˆå„è¨€èªã®é¡ç¾©èªã‚’å…±é€šèªã«å¤‰æ›ï¼‰
# ã‚¹ãƒãƒ¼ãƒ„å®Ÿæ³ã§é »å‡ºã™ã‚‹å¾—ç‚¹ã‚„ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³ãªã©ã€å›½ã”ã¨ã®è¡¨ç¾ã‚’è‹±èªãƒ™ãƒ¼ã‚¹ã®ã‚­ãƒ¼ã«çµ±åˆã™ã‚‹ã€‚
# ã“ã“ã§ã¯ä¸»è¦ãªä¾‹ã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãŠã‚Šã€å¿…è¦ã«å¿œã˜ã¦æ‹¡å¼µã—ã¦ãã ã•ã„ã€‚
TERM_MAP: Dict[str, str] = {
    # ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³
    "ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³": "home_run",
    "æœ¬å¡æ‰“": "home_run",
    "ã»ãƒ¼ã‚€ã‚‰ã‚“": "home_run",
    "homerun": "home_run",
    "home run": "home_run",
    "homer": "home_run",
    "hr": "home_run",
    "jonron": "home_run",
    # å¾—ç‚¹ãƒ»ã‚´ãƒ¼ãƒ«
    "å¾—ç‚¹": "score",
    "å¾—ç‚¹æ™‚": "score",
    "ã‚´ãƒ¼ãƒ«": "score",
    "goal": "score",
    "score": "score",
    "puntuaciÃ³n": "score",
    "gol": "score",
    # ã‚¢ã‚¦ãƒˆ
    "ã‚¢ã‚¦ãƒˆ": "out",
    "out": "out",
    # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯
    "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯": "strike",
    "strike": "strike",
    # ãƒœãƒ¼ãƒ«
    "ãƒœãƒ¼ãƒ«": "ball",
    "ball": "ball",
    # ãƒ•ã‚©ã‚¢ãƒœãƒ¼ãƒ«
    "ãƒ•ã‚©ã‚¢ãƒœãƒ¼ãƒ«": "walk",
    "walk": "walk",
    # ãƒ•ã‚¡ã‚¦ãƒ«
    "ãƒ•ã‚¡ã‚¦ãƒ«": "foul",
    "foul": "foul",
    # é€€å ´
    "é€€å ´": "ejection",
    "é€€å ´å‡¦åˆ†": "ejection",
    "ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰": "ejection",
    "èµ¤ç´™": "ejection",
    "ejected": "ejection",
    "expulsado": "ejection",
    "expulsion": "ejection",
    "red card": "ejection",
    "tarjeta roja": "ejection",
    "cartÃ£o vermelho": "ejection",
    # å¯©åˆ¤
    "å¯©åˆ¤": "umpire",
    "umpire": "umpire",
    # æ€ªæˆ‘
    "æ€ªæˆ‘": "injury",
    "injury": "injury",
    "hurt": "injury",
    # å¿œæ´ãƒ»æ­“å£°
    "å¿œæ´": "cheer",
    "cheer": "cheer",
    "boo": "boo",
    "æ‹æ‰‹": "clap",
    # ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰ï¼ˆè­¦å‘Šï¼‰
    "ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰": "warning",
    "yellow card": "warning",
    "è­¦å‘Š": "warning",
    "tarjeta amarilla": "warning",
    "cartÃ£o amarelo": "warning",
    "amarilla": "warning",
    # äº¤ä»£ãƒ»é¸æ‰‹äº¤ä»£
    "äº¤ä»£": "substitution",
    "é¸æ‰‹äº¤ä»£": "substitution",
    "substitution": "substitution",
    "substituiÃ§Ã£o": "substitution",
    "æ›¿ãˆ": "substitution",
    # ã‚´ãƒ¼ãƒ«ã«é–¢ã™ã‚‹è¿½åŠ è¡¨ç¾
    "ã‚´ãƒ¼ãƒ«ï¼": "score",
    "ã‚´ãƒ¼ãƒ«ã ": "score",
    "goooooal": "score",
    "goool": "score",
    "ã‚´ãƒ¼ãƒ«": "score",
    "goal": "score",
    "gol": "score",
    "golaÃ§o": "score",
    "gole": "score",
    "goal!": "score",
    "goooool": "score",
    # å¾—ç‚¹ã«é–¢ã™ã‚‹åˆ¥è¡¨ç¾
    "å¾—ç‚¹!": "score",
    "scored": "score",
    "goal!": "score",
    "goals": "score",
    # ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆPKï¼‰
    "ãƒšãƒŠãƒ«ãƒ†ã‚£": "penalty",
    "penal": "penalty",
    "penalty": "penalty",
    "pÃªnalti": "penalty",
    "penalti": "penalty",
    "pk": "penalty",
    # ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯
    "ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯": "free_kick",
    "free kick": "free_kick",
    "tiro libre": "free_kick",
    "tiro livre": "free_kick",
    "tiro de falta": "free_kick",
    # ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯
    "ã‚³ãƒ¼ãƒŠãƒ¼": "corner",
    "ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯": "corner",
    "corner": "corner",
    "escanteio": "corner",
    "esquina": "corner",
    # ã‚ªãƒ•ã‚µã‚¤ãƒ‰
    "ã‚ªãƒ•ã‚µã‚¤ãƒ‰": "offside",
    "offside": "offside",
    "impedimento": "offside",
    "fuera de juego": "offside",
    # ãƒ•ã‚¡ã‚¦ãƒ«
    "ãƒ•ã‚¡ã‚¦ãƒ«": "foul",
    "foul": "foul",
    "falta": "foul",
    # å¯©åˆ¤ï¼ãƒ¬ãƒ•ã‚§ãƒªãƒ¼
    "å¯©åˆ¤": "umpire",
    "ãƒ¬ãƒ•ã‚§ãƒªãƒ¼": "umpire",
    "referee": "umpire",
    "Ã¡rbitro": "umpire",
    # ãã®ä»–
    # ...
}

# ========================================
# å¤šè¨€èªé¡ç¾©èªè¾æ›¸ï¼ˆãƒˆãƒ”ãƒƒã‚¯é‡è¤‡æ¤œå‡ºæ”¹å–„ï¼‰
# ========================================
MULTILINGUAL_SYNONYMS = {
    # ã‚µãƒƒã‚«ãƒ¼é–¢é€£
    "goal": {"goal", "gol", "ã‚´ãƒ¼ãƒ«", "å¾—ç‚¹", "ê³¨", "gooool", "golll", "scored", "scoring"},
    "penalty": {"penalty", "pÃªnalti", "penalti", "ãƒšãƒŠãƒ«ãƒ†ã‚£", "pk", "íŒ¨ë„í‹°", "spot kick"},
    "offside": {"offside", "impedimento", "ã‚ªãƒ•ã‚µã‚¤ãƒ‰", "ì˜¤í”„ì‚¬ì´ë“œ"},
    "soccer": {"soccer", "football", "futebol", "ã‚µãƒƒã‚«ãƒ¼", "ì¶•êµ¬", "futbol"},
    "corner": {"corner", "escanteio", "ã‚³ãƒ¼ãƒŠãƒ¼", "ì½”ë„ˆ", "corner kick"},
    "foul": {"foul", "falta", "ãƒ•ã‚¡ã‚¦ãƒ«", "íŒŒìš¸"},
    "yellow": {"yellow", "amarelo", "ã‚¤ã‚¨ãƒ­ãƒ¼", "ì˜ë¡œìš°", "ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰", "yellow card"},
    "red": {"red", "vermelho", "ãƒ¬ãƒƒãƒ‰", "ë ˆë“œ", "ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰", "red card"},
    "shoot": {"shoot", "shot", "ã‚·ãƒ¥ãƒ¼ãƒˆ", "ìŠ›", "chute"},
    "freekick": {"free kick", "freekick", "ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯", "í”„ë¦¬í‚¥"},
    "goalkeeper": {"goalkeeper", "goalie", "gk", "ã‚´ãƒ¼ãƒ«ã‚­ãƒ¼ãƒ‘ãƒ¼", "ê³¨í‚¤í¼", "ã‚­ãƒ¼ãƒ‘ãƒ¼"},
    
    # é‡çƒé–¢é€£
    "baseball": {"baseball", "beisebol", "é‡çƒ", "ì•¼êµ¬"},
    "homerun": {"homerun", "home", "ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³", "í™ˆëŸ°", "æœ¬å¡æ‰“", "homer"},
    "strike": {"strike", "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯", "ìŠ¤íŠ¸ë¼ì´í¬"},
    "ball": {"ball", "ãƒœãƒ¼ãƒ«", "ë³¼"},
    "pitcher": {"pitcher", "ãƒ”ãƒƒãƒãƒ£ãƒ¼", "íˆ¬ìˆ˜", "æŠ•æ‰‹"},
    "batter": {"batter", "ãƒãƒƒã‚¿ãƒ¼", "íƒ€ì", "æ‰“è€…"},
    "hit": {"hit", "ãƒ’ãƒƒãƒˆ", "ì•ˆíƒ€", "å®‰æ‰“"},
    "run": {"run", "ãƒ©ãƒ³", "ë“ì ", "ç‚¹"},
    
    # ãƒãƒ¼ãƒ åï¼ˆã‚µãƒƒã‚«ãƒ¼ï¼‰
    "real madrid": {"real madrid", "real", "madrid", "ãƒ¬ã‚¢ãƒ«ãƒãƒ‰ãƒªãƒ¼ãƒ‰", "ë ˆì•Œ ë§ˆë“œë¦¬ë“œ", "merengues"},
    "barcelona": {"barcelona", "barÃ§a", "barca", "ãƒãƒ«ã‚»ãƒ­ãƒŠ", "ë°”ë¥´ì…€ë¡œë‚˜", "blaugrana"},
    "man united": {"manchester united", "man utd", "man united", "united", "ãƒãƒ³ãƒã‚§ã‚¹ã‚¿ãƒ¼ãƒ¦ãƒŠã‚¤ãƒ†ãƒƒãƒ‰"},
    "liverpool": {"liverpool", "lfc", "ãƒªãƒãƒ—ãƒ¼ãƒ«", "reds"},
    "bayern": {"bayern", "bayern munich", "ãƒã‚¤ã‚¨ãƒ«ãƒ³", "fcb"},
    "psg": {"psg", "paris", "ãƒ‘ãƒªã‚µãƒ³ã‚¸ã‚§ãƒ«ãƒãƒ³", "paris saint germain"},
    
    # ãƒãƒ¼ãƒ åï¼ˆé‡çƒï¼‰
    "yankees": {"yankees", "new york yankees", "ãƒ¤ãƒ³ã‚­ãƒ¼ã‚¹", "ny yankees"},
    "dodgers": {"dodgers", "la dodgers", "los angeles dodgers", "ãƒ‰ã‚¸ãƒ£ãƒ¼ã‚¹"},
    "red sox": {"red sox", "boston red sox", "ãƒ¬ãƒƒãƒ‰ã‚½ãƒƒã‚¯ã‚¹"},
    "giants": {"giants", "sf giants", "san francisco giants", "ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„"},
    
    # ä¸€èˆ¬
    "win": {"win", "vitÃ³ria", "å‹ã¡", "å‹åˆ©", "ìŠ¹ë¦¬", "victory", "won"},
    "lose": {"lose", "derrota", "è² ã‘", "æ•—åŒ—", "íŒ¨ë°°", "lost", "defeat"},
    "draw": {"draw", "empate", "å¼•ãåˆ†ã‘", "ë¬´ìŠ¹ë¶€", "tie"},
    "score": {"score", "placar", "ã‚¹ã‚³ã‚¢", "å¾—ç‚¹", "ì ìˆ˜"},
    "match": {"match", "partida", "è©¦åˆ", "ê²½ê¸°", "jogo", "game"},
    "player": {"player", "é¸æ‰‹", "jogador", "ì„ ìˆ˜"},
    "coach": {"coach", "manager", "ç›£ç£", "ê°ë…", "tÃ©cnico"},
    "referee": {"referee", "ref", "å¯©åˆ¤", "ì‹¬íŒ", "Ã¡rbitro"},
    "fan": {"fan", "supporter", "ãƒ•ã‚¡ãƒ³", "íŒ¬", "torcedor"},
    "stadium": {"stadium", "ã‚¹ã‚¿ã‚¸ã‚¢ãƒ ", "ê²½ê¸°ì¥", "estÃ¡dio"},
    
    # è©¦åˆãƒ•ã‚§ãƒ¼ã‚º
    "halftime": {"halftime", "half time", "ãƒãƒ¼ãƒ•ã‚¿ã‚¤ãƒ ", "í•˜í”„íƒ€ì„"},
    "fulltime": {"fulltime", "full time", "ãƒ•ãƒ«ã‚¿ã‚¤ãƒ ", "í’€íƒ€ì„"},
    "overtime": {"overtime", "extra time", "å»¶é•·", "ì—°ì¥"},
}

def normalize_with_synonyms(word: str) -> str:
    """
    å¤šè¨€èªé¡ç¾©èªè¾æ›¸ã‚’ä½¿ç”¨ã—ã¦å˜èªã‚’æ­£è¦åŒ–

    Args:
        word: æ­£è¦åŒ–ã™ã‚‹å˜èª

    Returns:
        æ­£è¦åŒ–å¾Œã®å˜èªï¼ˆé¡ç¾©èªãŒã‚ã‚‹å ´åˆã¯ä»£è¡¨èªã€ãªã„å ´åˆã¯å°æ–‡å­—åŒ–ï¼‰
    """
    word_lower = word.lower().strip()
    
    # è¾æ›¸ã‹ã‚‰ä»£è¡¨èªã‚’æ¤œç´¢
    for canonical, synonyms in MULTILINGUAL_SYNONYMS.items():
        if word_lower in synonyms:
            return canonical
    
    # è¾æ›¸ã«ãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
    return word_lower

def normalize_term(word: str) -> str:
    """
    ç”¨èªã‚’æ­£è¦åŒ–ã—ã¦ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚

    æ”¹å–„å†…å®¹ï¼š
    1. ç¹°ã‚Šè¿”ã—æ–‡å­—ã®æ­£è¦åŒ–ï¼ˆ"goalllll" â†’ "goal"ï¼‰
    2. å¤§æ–‡å­—å°æ–‡å­—ã®çµ±ä¸€
    3. æœ€å°é•·ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ1æ–‡å­—ã®å˜èªã‚’é™¤å¤–ï¼‰
    4. å¤šè¨€èªé¡ç¾©èªã®çµ±ä¸€ï¼ˆ"goal" = "ã‚´ãƒ¼ãƒ«"ï¼‰
    4. æ•°å­—ã®ã¿ã®å˜èªã‚’é™¤å¤–
    5. ç‰¹æ®Šæ–‡å­—ã®é™¤å»
    6. ç”¨èªãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã®é©ç”¨
    """
    if not isinstance(word, str):
        return word
    
    # å°æ–‡å­—åŒ–
    w = word.lower().strip()
    
    # ç©ºæ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯
    if not w:
        return ""
    
    # æ•°å­—ã®ã¿ã®å˜èªã‚’é™¤å¤–
    if w.isdigit():
        return ""
    
    # ç‰¹æ®Šæ–‡å­—ã®ã¿ã®å˜èªã‚’é™¤å¤–ï¼ˆçµµæ–‡å­—ã¯é™¤ãï¼‰
    import re
    if re.match(r'^[^\w\s]+$', w) and not any(ord(c) > 0x1F600 for c in w):
        return ""
    
    # ç¹°ã‚Šè¿”ã—æ–‡å­—ã®æ­£è¦åŒ–ï¼ˆ3æ–‡å­—ä»¥ä¸Šã®ç¹°ã‚Šè¿”ã—ã‚’2æ–‡å­—ã«ï¼‰
    # "goalllllll" â†’ "goal", "kkkkkk" â†’ "kk"
    w = re.sub(r'(.)\1{2,}', r'\1\1', w)
    
    # æœ€å°é•·ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ1æ–‡å­—ã®å˜èªã‚’é™¤å¤–ã€ãŸã ã—æ—¥æœ¬èªã¯é™¤ãï¼‰
    if len(w) == 1:
        # æ—¥æœ¬èªæ–‡å­—ï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ï¼‰ã¯è¨±å¯
        if not (0x3040 <= ord(w) <= 0x309F or  # ã²ã‚‰ãŒãª
                0x30A0 <= ord(w) <= 0x30FF or  # ã‚«ã‚¿ã‚«ãƒŠ
                0x4E00 <= ord(w) <= 0x9FFF):   # æ¼¢å­—
            return ""
    
    # ç”¨èªãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã®é©ç”¨
    w = TERM_MAP.get(w, w)
    
    # å¤šè¨€èªé¡ç¾©èªã®çµ±ä¸€ï¼ˆæ–°è¦è¿½åŠ ï¼‰
    w = normalize_with_synonyms(w)

    # æœ€çµ‚çš„ã«çŸ­ã™ãã‚‹å ´åˆã¯é™¤å¤–
    if len(w) < 2 and not any(0x3040 <= ord(c) <= 0x9FFF for c in w):
        return ""
    
    return w

# -------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ / å‰å‡¦ç†
# -------------------------
def segment_text(text: str) -> str:
    """
    æ—¥æœ¬èªã¨ãã®ä»–ã®æ–‡å­—ãŒæ··åœ¨ã™ã‚‹æ–‡å­—åˆ—ã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥ã™ã‚‹ã€‚

    æ—¥æœ¬èªã«ã¯é€šå¸¸ã‚¹ãƒšãƒ¼ã‚¹ãŒå…¥ã£ã¦ã„ãªã„ãŸã‚ã€
    ã‚³ãƒ¡ãƒ³ãƒˆå…¨ä½“ãŒ1ãƒˆãƒ¼ã‚¯ãƒ³ã«ãªã£ã¦ã—ã¾ã†å•é¡Œã‚’é¿ã‘ã‚‹ãŸã‚ã®ç°¡æ˜“æ‰‹æ³•ã§ã™ã€‚
    æ¼¢å­—ãƒ»ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãŒé€£ç¶šã™ã‚‹éƒ¨åˆ†ã¨ã€ãã‚Œä»¥å¤–ï¼ˆãƒ©ãƒ†ãƒ³æ–‡å­—ã‚„æ•°å­—ãªã©ï¼‰
    ã®å¢ƒç•Œã§ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥ã—ã¾ã™ã€‚
    ã“ã‚Œã¯å³å¯†ãªå½¢æ…‹ç´ è§£æã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²ã‚’æ”¹å–„ã—ã¾ã™ã€‚
    """
    result = []
    prev_jp: Optional[bool] = None
    for ch in text:
        # æ—¥æœ¬èªï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ï¼‰ã¨ãã‚Œä»¥å¤–ã®å¢ƒç•Œã‚’æ¤œå‡º
        is_jp = (
            ('\u3040' <= ch <= '\u30ff')  # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠ
            or ('\u4e00' <= ch <= '\u9fff')  # æ¼¢å­—
        )
        if prev_jp is None:
            result.append(ch)
        else:
            if is_jp != prev_jp:
                # å¢ƒç•Œã§ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥
                result.append(' ')
            result.append(ch)
        prev_jp = is_jp
    return ''.join(result)
def preprocess_text(text: str) -> str:
    """
    ã‚³ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ã‚’è¡Œã†ã€‚

    - URL, ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³, ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’é™¤å»
    - çµµæ–‡å­—ã®ä¸€éƒ¨ã‚’è‹±å˜èªã«ç½®ãæ›ãˆ
    - è¨˜å·ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›ã—ã€è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã«
    - æ—¥æœ¬èªã¨ãã®ä»–ã®æ–‡å­—ã®é–“ã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥ï¼ˆç°¡æ˜“åˆ†å‰²ï¼‰
    - å°æ–‡å­—åŒ–
    """
    if not isinstance(text, str):
        return ""
    # URLã‚„ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãªã©ã®é™¤å»
    text = re.sub(r"http\S+|www\.\S+", " ", text)
    text = re.sub(r"@\w+", " ", text)
    text = re.sub(r"#\w+", " ", text)
    # ä¸€éƒ¨çµµæ–‡å­—ã‚’å˜èªã«å¤‰æ›
    text = (
        text.replace("ğŸ˜‚", " laugh ")
            .replace("ğŸ˜­", " cry ")
            .replace("ğŸ‘", " clap ")
            .replace("ğŸ”¥", " fire ")
    )
    # è¨˜å·ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«
    text = re.sub(r"[^\w\s\u4E00-\u9FFF\u3040-\u30FF\uAC00-\uD7AF]", " ", text)
    # æ—¥æœ¬èªã¨ãã®ä»–ã®æ–‡å­—åˆ—å¢ƒç•Œã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥
    text = segment_text(text)
    # è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’ä¸€ã¤ã«ã€å‰å¾Œã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤ã—å°æ–‡å­—åŒ–
    text = re.sub(r"\s+", " ", text).strip().lower()
    return text

# -------------------------
# è¨€èªãƒ»çµµæ–‡å­—é–¢é€£ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -------------------------
def detect_lang_safe(text: str) -> str:
    """langdetect ã®ä¾‹å¤–ã‚’æ¡ã‚Šã¤ã¶ã—ã¦è¨€èªã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™"""
    try:
        return detect(text)
    except Exception:
        return "unk"

def is_emoji(char: str) -> bool:
    """ç°¡æ˜“çš„ãªçµµæ–‡å­—åˆ¤å®šã€‚Unicode ã®çµµæ–‡å­—ãƒ–ãƒ­ãƒƒã‚¯ã«å±ã™ã‚‹ã‹ã§åˆ¤å®š"""
    cp = ord(char)
    # çµµæ–‡å­—ã¯å¤šæ§˜ã ãŒã€ä»¥ä¸‹ã®ç¯„å›²ã‚’ã‚«ãƒãƒ¼ã™ã‚‹
    return (
        0x1F600 <= cp <= 0x1F64F  # emoticons
        or 0x1F300 <= cp <= 0x1F5FF  # symbols & pictographs
        or 0x1F680 <= cp <= 0x1F6FF  # transport & map symbols
        or 0x2600 <= cp <= 0x26FF    # miscellaneous symbols
        or 0x2700 <= cp <= 0x27BF    # dingbats
        or 0x1F1E6 <= cp <= 0x1F1FF  # flags
    )

def compute_language_distribution(texts: List[str]) -> Dict[str, int]:
    """ã‚³ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã‹ã‚‰è¨€èªåˆ¥ä»¶æ•°ã‚’æ•°ãˆã‚‹"""
    counter = defaultdict(int)
    for txt in texts:
        lang = detect_lang_safe(txt)
        counter[lang] += 1
    return counter

def compute_emoji_ratio(texts: List[str]) -> float:
    """ã‚³ãƒ¡ãƒ³ãƒˆå†…ã®çµµæ–‡å­—æ¯”ç‡ã‚’è¨ˆç®—ï¼ˆæ–‡å­—æ•°ã§ã¯ãªãçµµæ–‡å­—æ•°/å˜èªæ•°ï¼‰"""
    total_tokens = 0
    emoji_count = 0
    for txt in texts:
        tokens = txt.split()
        total_tokens += len(tokens)
        for ch in txt:
            if is_emoji(ch):
                emoji_count += 1
    return float(emoji_count) / max(total_tokens, 1)

def js_distance_distribution(counter_a: Dict[str, int], counter_b: Dict[str, int]) -> float:
    """è¨€èªåˆ†å¸ƒã‚„ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã® Jensenâ€“Shannon è·é›¢ã‚’è¨ˆç®—"""
    keys = set(counter_a.keys()) | set(counter_b.keys())
    pa = np.array([counter_a.get(k, 0) for k in keys], dtype=float)
    pb = np.array([counter_b.get(k, 0) for k in keys], dtype=float)
    return js_distance(pa, pb)

# -------------------------
# æ„Ÿæƒ…ãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆç°¡æ˜“ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰
# -------------------------
EN_POS = set([
    "good","great","amazing","awesome","nice","love","wow","goal","score","win","gg","goat","clutch",
])
EN_NEG = set([
    "bad","terrible","worst","lose","miss","wtf","hate","boo","noob","trash","rigged",
])
JA_POS = set(["ã™ã”ã„","æœ€é«˜","ä¸Šæ‰‹ã„","ã†ã¾ã„","å‹ã¡","å‹ã£ãŸ","ãƒŠã‚¤ã‚¹","ç¥","ã‚„ã£ãŸ","ã„ã„ã­","è‰","GG"])
JA_NEG = set(["æœ€æ‚ª","ã²ã©ã„","ä¸‹æ‰‹","è² ã‘","è² ã‘ãŸ","ãƒ€ãƒ¡","å«Œã„","ãã","ã‚¯ã‚½","ãƒ–ãƒ¼ã‚¤ãƒ³ã‚°"])

def _count_emoji(text: str) -> int:
    return sum(1 for ch in text if is_emoji(ch))

def compute_sentiment_metrics(texts: List[str]) -> Dict[str, float]:
    """æ¥µæ€§/è¦šé†’åº¦ã®ç°¡æ˜“æŒ‡æ¨™ã‚’è¿”ã™ã€‚
    polarity: [-1,1]ï¼ˆãƒã‚¸-ãƒã‚¬ï¼‰
    pos_ratio/neg_ratio: èªå½™æ¯”ç‡
    arousal: æ„Ÿæƒ…å–šèµ·ã®ä»£ç†ï¼ˆ! ã¨ çµµæ–‡å­—å¯†åº¦ï¼‰
    """
    pos = neg = 0
    tokens_total = 0
    exclam = 0
    emoji_ct = 0
    for txt in texts:
        if not isinstance(txt, str):
            continue
        t = txt.lower()
        tokens = t.split()
        tokens_total += len(tokens)
        exclam += t.count("!")
        emoji_ct += _count_emoji(txt)
        # ç°¡æ˜“æ¥µæ€§
        for w in tokens:
            if w in EN_POS:
                pos += 1
            if w in EN_NEG:
                neg += 1
        # æ—¥æœ¬èªã¯ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§ãªã„ã“ã¨ãŒå¤šã„ã®ã§æ–‡å­—åˆ—åŒ…å«ã§åˆ¤å®š
        for w in JA_POS:
            if w in txt:
                pos += 1
        for w in JA_NEG:
            if w in txt:
                neg += 1
    s_total = max(pos + neg, 1)
    polarity = float((pos - neg) / s_total)
    pos_ratio = float(pos / max(tokens_total, 1))
    neg_ratio = float(neg / max(tokens_total, 1))
    arousal = float((exclam + emoji_ct) / max(tokens_total, 1))
    return {
        "polarity": polarity,
        "pos_ratio": pos_ratio,
        "neg_ratio": neg_ratio,
        "arousal": arousal,
    }

def compute_style_profile(texts: List[str]) -> Dict[str, float]:
    letters = 0
    upper = 0
    tokens = 0
    uniq = set()
    exclam = ques = 0
    urls = mentions = 0
    for txt in texts:
        if not isinstance(txt, str):
            continue
        t = txt
        tokens_list = t.split()
        tokens += len(tokens_list)
        for w in tokens_list:
            uniq.add(w)
            if w.startswith("http://") or w.startswith("https://"):
                urls += 1
            if w.startswith("@"):
                mentions += 1
        exclam += t.count("!")
        ques += t.count("?")
        letters += sum(1 for ch in t if ch.isalpha())
        upper += sum(1 for ch in t if ch.isupper())
    avg_len = float(tokens / max(len(texts), 1))
    unique_ratio = float(len(uniq) / max(tokens, 1))
    upper_ratio = float(upper / max(letters, 1))
    exclam_ratio = float(exclam / max(tokens, 1))
    ques_ratio = float(ques / max(tokens, 1))
    url_ratio = float(urls / max(tokens, 1))
    mention_ratio = float(mentions / max(tokens, 1))
    return {
        "avg_len": avg_len,
        "unique_ratio": unique_ratio,
        "upper_ratio": upper_ratio,
        "exclam_ratio": exclam_ratio,
        "ques_ratio": ques_ratio,
        "url_ratio": url_ratio,
        "mention_ratio": mention_ratio,
    }

def style_distance(p: Dict[str, float], q: Dict[str, float]) -> float:
    """ã‚¹ã‚¿ã‚¤ãƒ«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é–“ã®å¹³å‡çµ¶å¯¾å·®"""
    keys = sorted(set(p.keys()) & set(q.keys()))
    if not keys:
        return 0.0
    diffs = [abs(float(p[k]) - float(q[k])) for k in keys]
    return float(np.mean(diffs))

def read_csv_any(path: str) -> pd.DataFrame:
    encodings = ["utf-8", "utf-8-sig", "cp932", "iso-8859-1"]
    for enc in encodings:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            pass
    return pd.read_csv(path, engine="python", on_bad_lines="skip")

def parse_country_from_filename(path: str) -> str:
    name = os.path.basename(path)
    m = re.findall(r"(japan|japanese|jpn|india|indian|dominican|usa|korea|korean|mexico|taiwan|china|chinese|france)", name.lower())
    if m:
        return m[-1].capitalize()
    if re.search(r"[\u3040-\u30ff\u4e00-\u9faf]", name):
        return "Japan"
    return "Unknown"

# -------------------------
# ãƒˆãƒ”ãƒƒã‚¯çµ±åˆãƒ»æ™‚ç³»åˆ—ãƒ»ãƒ”ãƒ¼ã‚¯
# -------------------------
def extract_ngram_topics_direct(comments: List[str], top_k: int = 30) -> List[str]:
    """
    ã€æ–°æ©Ÿèƒ½ã€‘ç‹¬è‡ªN-gramæŠ½å‡ºï¼ˆBERTopicã‚’ãƒã‚¤ãƒ‘ã‚¹ï¼‰
    
    BERTopicã®å†…éƒ¨å‡¦ç†ã§N-gramãƒ•ãƒ¬ãƒ¼ã‚ºãŒå˜èªã«åˆ†è§£ã•ã‚Œã‚‹å•é¡Œã‚’å›é¿ã—ã€
    TfidfVectorizerã§ç›´æ¥N-gramã‚’æŠ½å‡ºã—ã¦ãƒˆãƒ”ãƒƒã‚¯èªã¨ã™ã‚‹ã€‚
    
    ç›®çš„:
    - "Real Madrid", "penalty kick"ç­‰ã®ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ãã®ã¾ã¾æŠ½å‡º
    - topic_jaccard=0ãŒ82% â†’ 40-50%ã¸ã®æ”¹å–„ã‚’ç›®æŒ‡ã™
    
    Args:
        comments: ã‚³ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ ['comment1', 'comment2', ...]
        top_k: æŠ½å‡ºã™ã‚‹ä¸Šä½N-gramæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30ï¼‰
        
    Returns:
        list: é‡è¦ãªN-gramã®ãƒªã‚¹ãƒˆ ['Real Madrid', 'penalty kick', 'goal', ...]
        
    å®Ÿè£…ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :
    1. TfidfVectorizerã§1-gram, 2-gram, 3-gramã‚’æŠ½å‡º
    2. TF-IDFã‚¹ã‚³ã‚¢ã®åˆè¨ˆã§ã‚½ãƒ¼ãƒˆ
    3. ä¸Šä½top_kå€‹ã‚’è¿”ã™
    """
    if not comments or len(comments) < 2:
        return []
    
    try:
        # TfidfVectorizer ã§N-gramã‚’æŠ½å‡º
        vectorizer = TfidfVectorizer(
            ngram_range=(1, 3),       # 1-gram, 2-gram, 3-gram
            max_features=3000,         # æœ€å¤§3000å€‹ã®ç‰¹å¾´ï¼ˆStep 1ã§æ‹¡å¼µï¼‰
            max_df=1.0,                # 100%å‡ºç¾ã™ã‚‹èªã‚‚å«ã‚ã‚‹ï¼ˆPhase 1.5: å°è¦æ¨¡ã‚¤ãƒ™ãƒ³ãƒˆå¯¾å¿œï¼‰
            min_df=1,                  # æœ€ä½1å›å‡ºç¾ã™ã‚‹èªã®ã¿ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆæ•°ãŒå°‘ãªã„ã‚¤ãƒ™ãƒ³ãƒˆå¯¾å¿œï¼‰
            token_pattern=r"(?u)\b\w+\b",
            lowercase=True,
            # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¯é™¤å¤–ã—ãªã„ï¼ˆã‚¹ãƒãƒ¼ãƒ„ç”¨èªã‚’ä¿æŒï¼‰
        )
        
        # TF-IDFãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
        X = vectorizer.fit_transform(comments)
        
        # å…¨ã‚³ãƒ¡ãƒ³ãƒˆã§ã®TF-IDFã‚¹ã‚³ã‚¢ã®åˆè¨ˆã‚’è¨ˆç®—
        scores = np.asarray(X.sum(axis=0)).flatten()
        
        # ã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        top_indices = scores.argsort()[-top_k:][::-1]
        
        # ç‰¹å¾´èªï¼ˆN-gramï¼‰ã‚’å–å¾—
        feature_names = vectorizer.get_feature_names_out()
        top_ngrams = [feature_names[i] for i in top_indices]
        
        # ===== Noise Filteringçµ±åˆ =====
        # N-gramã‹ã‚‰ãƒã‚¤ã‚ºã‚’é™¤å»
        top_ngrams_filtered = NOISE_FILTER.filter_ngrams(top_ngrams)
        removed_count = len(top_ngrams) - len(top_ngrams_filtered)
        if removed_count > 0:
            print(f"  [N-gram Filter] Removed {removed_count}/{len(top_ngrams)} noise n-grams")
        top_ngrams = top_ngrams_filtered
        
        # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆæœ€åˆã®5å€‹ã®ã¿ï¼‰
        if len(top_ngrams) > 0:
            print(f"  [N-gramæŠ½å‡º] Top 5: {top_ngrams[:5]}")
        
        return top_ngrams
        
    except Exception as e:
        print(f"  [WARNING] N-gramæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜èªé »åº¦ãƒ™ãƒ¼ã‚¹
        all_words = []
        for comment in comments:
            words = comment.lower().split()
            all_words.extend(words)
        word_counts = Counter(all_words)
        return [word for word, count in word_counts.most_common(top_k)]


def build_topic_model(embedding_model: SentenceTransformer) -> BERTopic:
    # ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã®ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
    # CountVectorizer ã®ç‰¹å¾´æ•°ã‚’å¢—ã‚„ã—ã€å˜ä¸€å‡ºç¾èªã‚‚å¯¾è±¡ã«å«ã‚ã‚‹
    # ã€é‡è¦ã€‘N-gramã‚’æœ‰åŠ¹åŒ–: 1-gram, 2-gram, 3-gramã‚’æŠ½å‡º
    # ã“ã‚Œã«ã‚ˆã‚Š"Real Madrid", "penalty kick", "World Cup final"ç­‰ã®ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æ¤œå‡º
    vectorizer_model = CountVectorizer(
        token_pattern=r"(?u)\b\w+\b",
        max_features=8000,  # 6000â†’8000ã«å¢—åŠ ï¼ˆN-gramå¯¾å¿œï¼‰
        min_df=1,
        ngram_range=(1, 3),  # ã€æ–°æ©Ÿèƒ½ã€‘1-gram, 2-gram, 3-gramã‚’æŠ½å‡º
        max_df=1.0  # 100%å‡ºç¾ã™ã‚‹èªã‚‚å«ã‚ã‚‹ï¼ˆPhase 1.5: å°è¦æ¨¡ã‚¤ãƒ™ãƒ³ãƒˆå¯¾å¿œï¼‰
    )
    # UMAP ã®æ¬¡å…ƒæ•°ã¨è¿‘å‚æ•°ã‚’å¢—ã‚„ã—ã€é«˜æ¬¡å…ƒåŸ‹ã‚è¾¼ã¿ã‚’ã‚ˆã‚Šè©³ç´°ã«è¡¨ç¾ã™ã‚‹
    umap_model = UMAP(n_components=10, n_neighbors=30, min_dist=0.00, metric="cosine", random_state=42)
    # HDBSCAN ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚ºã¨ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å°ã•ãè¨­å®šã—ã€å°ã•ãªã‚¤ãƒ™ãƒ³ãƒˆã‚‚æ¤œå‡ºã—ã‚„ã™ãã™ã‚‹
    hdbscan_model = HDBSCAN(min_cluster_size=10, min_samples=2, metric="euclidean",
                            cluster_selection_method="eom", prediction_data=True, core_dist_n_jobs=1)
    representation_model = MaximalMarginalRelevance(diversity=0.5)
    # ã‚¹ãƒãƒ¼ãƒ„å®Ÿæ³ã«é–¢é€£ã™ã‚‹ã‚ˆã‚Šå¤šæ§˜ãªç¨®ãƒˆãƒ”ãƒƒã‚¯ã‚’å«ã‚ã‚‹
    seed_topic_list = [
        ["pitch", "strike", "ball", "fastball", "slider", "pitching"],
        ["bat", "homer", "home run", "slugger", "batting"],
        ["defense", "catch", "outfield", "infield", "double play"],
        ["umpire", "referee", "call", "review", "challenge"],
        ["injury", "hurt", "rehab", "out"],
        ["weather", "rain", "delay"],
        ["cheer", "chant", "song", "boo", "applause", "clap", "support"],
        ["strategy", "tactics", "lineup", "substitution", "change"],
        # ã‚µãƒƒã‚«ãƒ¼ãƒ»é‡çƒç­‰å…±é€š: å¾—ç‚¹ã«é–¢ã™ã‚‹ç¨®
        ["goal", "score", "goalkeeper", "penalty", "shoot", "free kick", "corner", "offside"],
        # ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰ãƒ»ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰ãƒ»è­¦å‘Š
        ["yellow card", "warning", "foul", "penalty", "fine"],
        ["red card", "ejection", "sent off", "expulsion"],
        # äº¤ä»£ãƒ»é¸æ‰‹äº¤ä»£
        ["substitution", "sub", "change player", "replace", "äº¤ä»£"],
    ]
    return BERTopic(
        embedding_model=embedding_model,
        vectorizer_model=vectorizer_model,
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        representation_model=representation_model,
        calculate_probabilities=False,
        seed_topic_list=seed_topic_list,
        # ã‚ˆã‚Šå°ã•ãªãƒˆãƒ”ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’è¨±å®¹ã™ã‚‹ã“ã¨ã§ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºã‚’ç´°åˆ†åŒ–
        min_topic_size=5,
        nr_topics=None,
        verbose=False,
    )

def merge_topics(words_by_tid: Dict[int, List[Tuple[str, float]]], threshold: float) -> List[List[int]]:
    """
    ãƒˆãƒ”ãƒƒã‚¯ã‚’Jaccardé¡ä¼¼åº¦ã§ãƒãƒ¼ã‚¸ã™ã‚‹ï¼ˆç²¾åº¦å‘ä¸Šç‰ˆï¼‰
    
    æ”¹å–„å†…å®¹ï¼š
    1. é©å¿œçš„é–¾å€¤ï¼šãƒˆãƒ”ãƒƒã‚¯ã‚µã‚¤ã‚ºã«å¿œã˜ã¦é–¾å€¤ã‚’èª¿æ•´
    2. ç©ºå˜èªã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼šæ­£è¦åŒ–å¾Œã«ç©ºã«ãªã‚‹å˜èªã‚’é™¤å¤–
    3. æœ€å°ãƒˆãƒ”ãƒƒã‚¯ã‚µã‚¤ã‚ºï¼š2å˜èªæœªæº€ã®ãƒˆãƒ”ãƒƒã‚¯ã¯å­¤ç«‹ã•ã›ã‚‹
    """
    tids = [t for t in words_by_tid.keys() if t != -1]
    
    # å„ãƒˆãƒ”ãƒƒã‚¯ã®ä¸Šä½èªã‚»ãƒƒãƒˆã‚’æ­£è¦åŒ–ï¼ˆç”¨èªãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰ã—ã¦ã‹ã‚‰æ¯”è¼ƒ
    sets = {}
    topic_sizes = {}  # æ­£è¦åŒ–å¾Œã®ãƒˆãƒ”ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’è¨˜éŒ²
    
    for t in tids:
        raw_words = [w for w, _ in words_by_tid[t][:10] if isinstance(w, str) and w.strip()]
        # æ­£è¦åŒ–ã—ã¦ç©ºæ–‡å­—åˆ—ã‚’é™¤å¤–
        normalized = {normalize_term(w) for w in raw_words}
        normalized = {w for w in normalized if w}  # ç©ºæ–‡å­—åˆ—é™¤å¤–
        sets[t] = normalized
        topic_sizes[t] = len(normalized)
    
    # Union-Findæ§‹é€ 
    parent: Dict[int, int] = {t: t for t in tids}
    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x
    def union(x, y):
        rx, ry = find(x), find(y)
        if rx != ry: parent[ry] = rx
    
    # é©å¿œçš„é–¾å€¤ã§ãƒãƒ¼ã‚¸
    for i, ti in enumerate(tids):
        for tj in tids[i+1:]:
            sa, sb = sets[ti], sets[tj]
            
            # ç©ºã‚»ãƒƒãƒˆã¾ãŸã¯å°ã•ã™ãã‚‹ãƒˆãƒ”ãƒƒã‚¯ã¯ã‚¹ã‚­ãƒƒãƒ—
            if not sa or not sb or len(sa) < 2 or len(sb) < 2:
                continue
            
            # Jaccardé¡ä¼¼åº¦è¨ˆç®—
            intersection = len(sa & sb)
            union_size = len(sa | sb)
            
            if union_size == 0:
                continue
            
            jac = intersection / union_size
            
            # é©å¿œçš„é–¾å€¤ã®è¨ˆç®—
            # å°ãƒˆãƒ”ãƒƒã‚¯ï¼ˆ2-5å˜èªï¼‰: é–¾å€¤ Ã— 0.7
            # ä¸­ãƒˆãƒ”ãƒƒã‚¯ï¼ˆ6-8å˜èªï¼‰: é–¾å€¤ Ã— 0.85
            # å¤§ãƒˆãƒ”ãƒƒã‚¯ï¼ˆ9-10å˜èªï¼‰: é–¾å€¤ Ã— 1.0
            avg_size = (topic_sizes[ti] + topic_sizes[tj]) / 2
            
            if avg_size <= 5:
                adaptive_threshold = threshold * 0.7  # ã‚ˆã‚Šç·©ã„é–¾å€¤
            elif avg_size <= 8:
                adaptive_threshold = threshold * 0.85
            else:
                adaptive_threshold = threshold
            
            # ãƒãƒ¼ã‚¸åˆ¤å®š
            if jac >= adaptive_threshold:
                union(ti, tj)
    
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    groups = defaultdict(list)
    for t in tids:
        groups[find(t)].append(t)
    
    return list(groups.values())

def compute_topics_over_time(topic_model: BERTopic, docs: List[str], topics: List[int], timestamps: List[pd.Timestamp], nr_bins: int) -> pd.DataFrame:
    return topic_model.topics_over_time(docs=docs, topics=topics, timestamps=timestamps, nr_bins=nr_bins, datetime_format=None)

def build_relative_time_bins(timestamps: pd.Series, nr_bins: int) -> pd.IntervalIndex:
    tmin, tmax = timestamps.min(), timestamps.max()
    edges = pd.date_range(start=tmin, end=tmax, periods=nr_bins + 1)
    return pd.IntervalIndex.from_breaks(edges, closed="left")

def smooth_series(y: np.ndarray, k: int = 3) -> np.ndarray:
    """ç§»å‹•å¹³å‡ã§å¹³æ»‘åŒ–ï¼ˆç«¯ã¯åå°„paddingï¼‰"""
    if k <= 1 or len(y) == 0:
        return y
    pad = k // 2
    ypad = np.pad(y, (pad, pad), mode="reflect")
    ker = np.ones(k) / k
    return np.convolve(ypad, ker, mode="valid")

def local_peaks(y: np.ndarray, n_keep: int = 3) -> List[int]:
    """å˜ç´”ãªå±€æ‰€æœ€å¤§ï¼ˆéš£ã‚ˆã‚Šå¤§ãã„ or åŒç­‰ï¼‰ã‚’æŠ½å‡ºã—ã¦ä¸Šä½n_keep"""
    if len(y) == 0:
        return []
    idxs = []
    for i in range(len(y)):
        l = y[i-1] if i-1 >= 0 else -np.inf
        r = y[i+1] if i+1 < len(y) else -np.inf
        if y[i] >= l and y[i] >= r:
            idxs.append(i)
    # å¼·ã„ãƒ”ãƒ¼ã‚¯é †
    idxs.sort(key=lambda i: y[i], reverse=True)
    return idxs[:n_keep]

# -------------------------
# ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
# -------------------------
class StreamData:
    def __init__(self, file_path: str, country: str, df_valid: pd.DataFrame,
                 topics_valid: List[int], groups: List[List[int]],
                 gid_label: Dict[int, str], group_timeseries: pd.DataFrame,
                 nr_bins: int, group_top_words: Dict[int, List[str]]):
        self.file_path = file_path
        self.country = country
        self.df_valid = df_valid
        self.topics_valid = topics_valid
        self.groups = groups
        self.gid_label = gid_label
        self.group_timeseries = group_timeseries
        self.nr_bins = nr_bins
        self.group_top_words = group_top_words  # {group_id: [str,...]}
        # è¨€èªåˆ—ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ä¿æŒ
        self.languages = df_valid.get("lang") if "lang" in df_valid.columns else None

# -------------------------
# ã‚¹ãƒˆãƒªãƒ¼ãƒ 1æœ¬ã®å‡¦ç†
# -------------------------
def process_stream(csv_file: str, embedding_model: SentenceTransformer,
                   jaccard_th: float, nr_bins: int, topk_plot: int = 10) -> Optional[StreamData]:
    df = read_csv_any(csv_file)
    if df.empty or "message" not in df.columns:
        print(f"Skipping {csv_file}: no message column")
        return None

    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— æ­£è¦åŒ–ï¼ˆTZæ··åœ¨å¯¾ç­–ï¼šã™ã¹ã¦tz-naiveã¸ï¼‰
    if "timestamp" in df.columns and not df["timestamp"].isnull().all():
        ts = pd.to_datetime(df["timestamp"], errors="coerce", utc=True)
        df["timestamp"] = ts.dt.tz_localize(None)
        df = df.dropna(subset=["timestamp"]).copy()
    else:
        df["timestamp"] = pd.date_range(start="2024-01-01", periods=len(df), freq="5S")

    # å‰å‡¦ç†
    df = df.dropna(subset=["message"]).copy()
    df["message_clean"] = df["message"].astype(str).apply(preprocess_text)
    df = df[df["message_clean"].str.len() > 0].copy()
    
    # ===== Noise Filteringçµ±åˆ =====
    # ãƒã‚¤ã‚ºã‚³ãƒ¡ãƒ³ãƒˆé™¤å» (kkkk, wwww, etc.)
    df["is_noise"] = df["message_clean"].apply(NOISE_FILTER.is_noise)
    noise_count = df["is_noise"].sum()
    total_before = len(df)
    df = df[~df["is_noise"]].copy()
    noise_ratio = noise_count / total_before if total_before > 0 else 0
    print(f"  [Noise Filter] Removed {noise_count}/{total_before} comments ({noise_ratio:.1%})")
    
    if df.empty:
        print(f"Skipping {csv_file}: no usable comments after noise filtering")
        return None
    # è¨€èªæ¤œå‡ºï¼ˆå¾Œã§ã‚¹ã‚¿ã‚¤ãƒ«æ¯”è¼ƒã«åˆ©ç”¨ï¼‰
    df["lang"] = df["message_clean"].apply(detect_lang_safe)
    texts = df["message_clean"].tolist()

    # åŸ‹ã‚è¾¼ã¿ & BERTopic
    emb = embedding_model.encode(texts, batch_size=64, show_progress_bar=False, normalize_embeddings=True)
    topic_model = build_topic_model(embedding_model)
    topics, _ = topic_model.fit_transform(texts, embeddings=emb)

    valid_idx = [i for i, t in enumerate(topics) if t != -1]
    if len(valid_idx) < 10:
        print(f"Skipping {csv_file}: too few valid topics")
        return None
    df_valid = df.iloc[valid_idx].reset_index(drop=True)
    topics_valid = [topics[i] for i in valid_idx]

    # ä¸Šä½èª
    topic_info = topic_model.get_topic_info()
    valid_tids = sorted([int(t) for t in topic_info["Topic"].tolist() if int(t) != -1])
    words_by_tid: Dict[int, List[Tuple[str, float]]] = {}
    for tid in valid_tids:
        items = topic_model.get_topic(tid) or []
        items = [(str(w), float(s)) for w, s in items if isinstance(w, str) and str(w).strip()]
        words_by_tid[tid] = items

    # ä¼¼ãƒˆãƒ”ãƒƒã‚¯çµ±åˆ
    groups = merge_topics(words_by_tid, threshold=jaccard_th)
    gid_label, group_top_words = {}, {}
    for gid, members in enumerate(groups):
        # ãƒˆãƒ”ãƒƒã‚¯çµ±åˆå¾Œã®ä»£è¡¨èªã‚’ä½œæˆï¼šé¡ç¾©èªãƒãƒƒãƒ”ãƒ³ã‚°ã§æ­£è¦åŒ–ã—ã€ä¸Šä½ã‚¹ã‚³ã‚¢é †ã«ã‚«ã‚¦ãƒ³ãƒˆ
        counter = Counter()
        for t in members:
            # ãƒˆãƒ”ãƒƒã‚¯é‡è¤‡æ¤œå‡ºæ”¹å–„: 10èª â†’ 20èªã«å¢—åŠ 
            for w, s in words_by_tid.get(t, [])[:20]:
                if isinstance(w, str) and w.strip():
                    norm = normalize_term(w)
                    counter[norm] += float(s)
        # ãƒ©ãƒ™ãƒ«ã¯ä¸Šä½4èªã®ã¾ã¾ï¼ˆå¯èª­æ€§ã®ãŸã‚ï¼‰
        tops = [w for w, _ in counter.most_common(4)]
        # ãƒˆãƒ”ãƒƒã‚¯æ¯”è¼ƒç”¨ã«ã¯ä¸Šä½20èªã‚’ä¿å­˜
        tops_extended = [w for w, _ in counter.most_common(20)]
        group_top_words[gid] = tops_extended  # 20èªã‚’ä¿å­˜
        gid_label[gid] = "ãƒ»".join(tops) if tops else f"group_{gid}"

    # æ™‚ç³»åˆ—ï¼ˆçµ±åˆï¼‰
    tot = compute_topics_over_time(topic_model, df_valid["message_clean"].tolist(),
                                   topics_valid, df_valid["timestamp"].tolist(), nr_bins)
    # raw topic -> group
    raw2g = {}
    for gid, members in enumerate(groups):
        for t in members: raw2g[t] = gid
    tot["Group"] = tot["Topic"].map(raw2g).astype(int)

    sums = tot.groupby("Timestamp")["Frequency"].sum().rename("total")
    df_g = (tot.groupby(["Group", "Timestamp"], as_index=False)["Frequency"].sum()
              .merge(sums, left_on="Timestamp", right_index=True, how="left"))
    df_g["Percentage"] = 100.0 * df_g["Frequency"] / df_g["total"].clip(lower=1)
    df_g = df_g.drop(columns=["total"])

    # å¯è¦–åŒ–: å„é…ä¿¡è€…ã®Top-10æ™‚ç³»åˆ—
    plot_top_groups(df_g, gid_label,
                    out_png=os.path.join(OUT_DIR, "timelines", f"{os.path.basename(csv_file).replace('.csv','')}_timeline.png"),
                    title=f"Topics Over Time (Top-{topk_plot}) : {os.path.basename(csv_file)} [{parse_country_from_filename(csv_file)}]",
                    top_k=topk_plot)

    return StreamData(
        file_path=os.path.basename(csv_file),
        country=parse_country_from_filename(csv_file),
        df_valid=df_valid,
        topics_valid=topics_valid,
        groups=groups,
        gid_label=gid_label,
        group_timeseries=df_g,
        nr_bins=nr_bins,
        group_top_words=group_top_words,
    )

def plot_top_groups(df_g: pd.DataFrame, labels: Dict[int, str], out_png: str, title: str, top_k: int = 10):
    os.makedirs(os.path.dirname(out_png), exist_ok=True)
    order = (df_g.groupby("Group")["Frequency"].sum().sort_values(ascending=False))
    top_groups = order.index.tolist()[:top_k]
    plt.figure(figsize=(12,6))
    for gid in top_groups:
        d = df_g[df_g["Group"] == gid].sort_values("Timestamp")
        if d.empty: continue
        label = labels.get(gid, f"G{gid}")
        if len(label) > 40: label = label[:37] + "..."
        plt.plot(d["Timestamp"], d["Percentage"], marker=".", linewidth=1.2, label=label)
    plt.title(title); plt.xlabel("Time"); plt.ylabel("Percentage of comments")
    plt.legend(ncol=2, fontsize=9, frameon=False); plt.xticks(rotation=45)
    plt.tight_layout(); plt.savefig(out_png, dpi=220); plt.close()
    print(f"Saved timeline: {out_png}")

# -------------------------
# ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡ºãƒ»ç…§åˆãƒ»ãƒ†ã‚­ã‚¹ãƒˆè·é›¢
# -------------------------
def detect_events(stream: StreamData, n_events: int = 5, focus_top: Optional[int] = None) -> List[Dict[str, object]]:
    """
    å„ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆãƒˆãƒ”ãƒƒã‚¯çµ±åˆï¼‰ã®æ™‚ç³»åˆ—ã‹ã‚‰ãƒ”ãƒ¼ã‚¯ã‚¤ãƒ™ãƒ³ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚

    Parameters
    ----------
    stream : StreamData
        1ã¤ã®é…ä¿¡ã®è§£æçµæœã€‚
    n_events : int
        å„ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æŠ½å‡ºã™ã‚‹ãƒ”ãƒ¼ã‚¯ã®æœ€å¤§æ•°ã€‚
    focus_top : int or None
        ãƒˆãƒ”ãƒƒã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç·å‡ºç¾å›æ•°ã®å¤šã„é †ã«é™å®šã™ã‚‹æ•°ã€‚
        ä¾‹: 10 ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®å¤šã„ä¸Šä½10ã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿ã‹ã‚‰ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’è¡Œã†ã€‚
        None ã®å ´åˆã¯ã™ã¹ã¦ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å¯¾è±¡ã¨ã™ã‚‹ã€‚

    Returns
    -------
    List[Dict[str, object]]
        æ¤œå‡ºã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆã€‚
    """
    events: List[Dict[str, object]] = []
    # å¯¾è±¡ã‚°ãƒ«ãƒ¼ãƒ—ã®æ±ºå®š
    if focus_top is not None and focus_top > 0:
        sums = stream.group_timeseries.groupby("Group")["Frequency"].sum()
        groups_to_use = sums.sort_values(ascending=False).head(focus_top).index.tolist()
    else:
        groups_to_use = sorted(stream.group_timeseries["Group"].unique())
    # Binning å†ç¾
    bins = build_relative_time_bins(stream.df_valid["timestamp"], stream.nr_bins)
    for gid in groups_to_use:
        gdf = stream.group_timeseries[stream.group_timeseries["Group"] == gid]
        if gdf.empty:
            continue
        # Bin assignment
        bin_edges = list(zip(bins.left, bins.right))
        counts = np.zeros(len(bins), dtype=float)
        for _, r in gdf.iterrows():
            ts = r["Timestamp"]
            b = None
            for bi, (lft, rgt) in enumerate(bin_edges):
                if ts >= lft and ts < rgt:
                    b = bi
                    break
            if b is None:
                centers = np.array([iv.left.value for iv in bins], dtype=np.int64)
                b = int(np.argmin(np.abs(centers - int(ts.value))))
            counts[b] += float(r["Frequency"])
        # å¹³æ»‘åŒ–â†’ãƒ”ãƒ¼ã‚¯æŠ½å‡º
        y = smooth_series(counts, k=5)
        peak_idx = local_peaks(y, n_keep=n_events)
        for b in peak_idx:
            events.append({
                "group_id": int(gid),
                "bin_id": int(b),
                "peak_time": bins[b].left,
                "top_words": stream.group_top_words.get(gid, []),
                "label": stream.gid_label.get(gid, f"group_{gid}")
            })
    
    # ===== Noise Filteringçµ±åˆ =====
    # ã‚¤ãƒ™ãƒ³ãƒˆã‚’å“è³ªã‚¹ã‚³ã‚¢ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    events_before = len(events)
    events_with_quality = []
    for event in events:
        quality = NOISE_FILTER.score_topic_quality(event['top_words'])
        event['quality_score'] = quality
        if quality >= 0.3:  # æœ€å°å“è³ªé–¾å€¤
            events_with_quality.append(event)
    
    removed_events = events_before - len(events_with_quality)
    if removed_events > 0:
        print(f"  [Event Filter] Removed {removed_events}/{events_before} low-quality events")
    
    return events_with_quality

def match_events_across_streams(
    events_by_stream: Dict[str, List[Dict[str, object]]],
    word_th: float,
    time_th: int,
    embed_th: Optional[float] = None,
) -> Dict[Tuple[str, int], int]:
    """
    å¤šæ•°ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒšã‚¢ãƒ¯ã‚¤ã‚ºã§æ¯”è¼ƒã—ã€é¡ä¼¼ã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã‚’åŒä¸€IDã«çµ±åˆã™ã‚‹ã€‚

    æ¯”è¼ƒã«ã¯ä»¥ä¸‹ã®æ¡ä»¶ã‚’ä½¿ç”¨ã™ã‚‹ï¼š
      1. Top-wordé›†åˆã®Jaccardé¡ä¼¼åº¦ãŒ `word_th` ä»¥ä¸Šã€‚
      2. ç™ºç”ŸbinãŒ `time_th` ä»¥ä¸‹ã®å·®ã§è¿‘æ¥ã—ã¦ã„ã‚‹ã€‚
      3. ã‚¤ãƒ™ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå¹³å‡ã‚³ãƒ¡ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãŒ `embed_th` ä»¥ä¸Š
         ï¼ˆ`embed_th` ãŒ None ã®å ´åˆã¯ã“ã®æ¡ä»¶ã‚’ç„¡è¦–ï¼‰ã€‚

    ã“ã‚Œã«ã‚ˆã‚Šå˜ç´”ãªJaccardã¨æ™‚é–“ã ã‘ã§ãªãã€èªå¥ãŒç•°ãªã‚‹è¨€èªã§ã‚‚å†…å®¹ãŒè¿‘ã„å ´åˆã‚’æ‹¾ãˆã‚‹ã€‚

    Parameters
    ----------
    events_by_stream : dict
        ã‚¹ãƒˆãƒªãƒ¼ãƒ ã”ã¨ã«æŠ½å‡ºã—ãŸã‚¤ãƒ™ãƒ³ãƒˆã‚’æ ¼ç´ã—ãŸè¾æ›¸ã€‚
        å„ã‚¤ãƒ™ãƒ³ãƒˆdictã«ã¯ 'top_words', 'bin_id', 'embedding' (optional) ãªã©ãŒå«ã¾ã‚Œã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
    word_th : float
        ãƒˆãƒ”ãƒƒã‚¯ã®ä¸Šä½èªé›†åˆã§è¨ˆç®—ã—ãŸJaccardé¡ä¼¼åº¦ã®é–¾å€¤ã€‚
    time_th : int
        ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿbinã®å·®ã®è¨±å®¹ç¯„å›²ã€‚
    embed_th : float or None
        ã‚¤ãƒ™ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦é–¾å€¤ã€‚Noneã®ã¨ãã¯ãƒã‚§ãƒƒã‚¯ã—ãªã„ã€‚

    Returns
    -------
    dict
        (stream_key, event_index) -> unified_event_id ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    """
    items = []
    # Flatten events with their stream key and index
    for key, evts in events_by_stream.items():
        for i, evt in enumerate(evts):
            items.append((key, i, evt))
    # DSU setup
    parent: Dict[Tuple[str,int], Tuple[str,int]] = {(k,i):(k,i) for k,i,_ in items}
    def find(x: Tuple[str,int]) -> Tuple[str,int]:
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x
    def union(x: Tuple[str,int], y: Tuple[str,int]) -> None:
        rx, ry = find(x), find(y)
        if rx != ry:
            parent[ry] = rx
    # Pairwise comparisons
    debug_count = 0
    debug_match_count = 0
    DEBUG_VERBOSE = False  # è©³ç´°ãªãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã‚’æœ‰åŠ¹åŒ–ã™ã‚‹å ´åˆã¯True
    for a in range(len(items)):
        ka, ia, ea = items[a]
        for b in range(a+1, len(items)):
            kb, ib, eb = items[b]
            # åŒä¸€ã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ã‚¤ãƒ™ãƒ³ãƒˆã¯ãƒãƒ¼ã‚¸ã—ãªã„
            if ka == kb:
                continue
            
            debug_count += 1
            
            # DEBUG: æœ€åˆã®æ•°ãƒšã‚¢ã‚’è©³ç´°ã«è¨˜éŒ²ï¼ˆDEBUG_VERBOSEãŒTrueã®å ´åˆã®ã¿ï¼‰
            if DEBUG_VERBOSE and debug_count <= 5:
                print(f"[DEBUG] Pair {debug_count}: {os.path.basename(ka)} event{ia} vs {os.path.basename(kb)} event{ib}")
            
            # Time proximity check first (fastest)
            bin_diff = abs(int(ea.get("bin_id", -1)) - int(eb.get("bin_id", -1)))
            if DEBUG_VERBOSE and debug_count <= 5:
                print(f"  - Time bin difference: {bin_diff} (threshold: {time_th})")
            if bin_diff > time_th:
                if DEBUG_VERBOSE and debug_count <= 5:
                    print(f"  - SKIP: Time difference too large")
                continue
            
            # ===ç²¾åº¦å‘ä¸Š: æœ€å°ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãƒã‚§ãƒƒã‚¯===
            # å„ã‚¤ãƒ™ãƒ³ãƒˆãŒååˆ†ãªã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’æŒã£ã¦ã„ã‚‹ã‹ç¢ºèªï¼ˆå½é™½æ€§å‰Šæ¸›ï¼‰
            min_comments_threshold = 5  # æœ€å°5ã‚³ãƒ¡ãƒ³ãƒˆ
            
            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®æ¨å®šï¼ˆtop_wordsã®æ•°ã‚’ãƒ—ãƒ­ã‚­ã‚·ã¨ã—ã¦ä½¿ç”¨ï¼‰
            # ã¾ãŸã¯ã€å¾Œã§ extract_event_comments ã‚’å‘¼ã³å‡ºã—ã¦ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½
            # ã“ã“ã§ã¯ç°¡æ˜“çš„ã« top_words ãŒå­˜åœ¨ã™ã‚‹ã‹ã§åˆ¤å®š
            has_sufficient_data_a = len(ea.get("top_words", [])) >= 3
            has_sufficient_data_b = len(eb.get("top_words", [])) >= 3
            
            if not has_sufficient_data_a or not has_sufficient_data_b:
                if DEBUG_VERBOSE and debug_count <= 5:
                    print(f"  - SKIP: Insufficient topic data (A:{len(ea.get('top_words', []))}, B:{len(eb.get('top_words', []))})")
                continue
            
            # Embedding similarity check (if enabled, this is primary matching method)
            if embed_th is not None:
                emb_a = ea.get("embedding")
                emb_b = eb.get("embedding")
                # ã©ã¡ã‚‰ã‹æ¬ å¦‚ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
                if emb_a is None or emb_b is None:
                    if DEBUG_VERBOSE and debug_count <= 5:
                        print(f"  - SKIP: Missing embedding")
                    continue
                # æ­£è¦åŒ–æ¸ˆã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
                num = float(np.dot(emb_a, emb_b))
                if DEBUG_VERBOSE and debug_count <= 5:
                    print(f"  - Embedding similarity: {num:.4f} (threshold: {embed_th})")
                # æ—¢ã«normalize_embeddings=Trueã§ç”Ÿæˆã—ã¦ã„ã‚‹ã®ã§normã¯â‰ˆ1
                if num < embed_th:
                    if DEBUG_VERBOSE and debug_count <= 5:
                        print(f"  - SKIP: Embedding similarity too low")
                    continue
                # Embedding check passed, now check Jaccard (if both pass = stronger match)
            
            # Jaccard on top words (secondary check, or primary if embed_th is None)
            sa_raw = ea.get("top_words", [])
            sb_raw = eb.get("top_words", [])
            sa = {normalize_term(w) for w in sa_raw if isinstance(w, str) and w.strip()}
            sb = {normalize_term(w) for w in sb_raw if isinstance(w, str) and w.strip()}
            
            if DEBUG_VERBOSE and debug_count <= 5:
                print(f"  - top_words_A: {sa_raw}")
                print(f"  - top_words_B: {sb_raw}")
                print(f"  - normalized_A: {sa}")
                print(f"  - normalized_B: {sb}")
            
            # Jaccard similarity of normalized sets
            jacc = 0.0
            if sa or sb:
                jacc = len(sa & sb) / (len(sa | sb) + 1e-12)
            if DEBUG_VERBOSE and debug_count <= 5:
                print(f"  - Jaccard similarity: {jacc:.4f} (threshold: {word_th})")
            
            # If embedding matching is enabled, Jaccard is optional (just for extra validation)
            # If embedding matching is disabled, Jaccard is required
            if embed_th is None:
                # No embedding check - rely on Jaccard
                if jacc < word_th:
                    if DEBUG_VERBOSE and debug_count <= 5:
                        print(f"  - SKIP: Jaccard too low (no embedding check)")
                    continue
            else:
                # Embedding check already passed - Jaccard is just for logging
                if DEBUG_VERBOSE and debug_count <= 5:
                    if jacc >= word_th:
                        print(f"  - Jaccard also passed (strong match)")
                    else:
                        print(f"  - Jaccard low but embedding passed (semantic match)")
            
            # All conditions satisfied â†’ union
            debug_match_count += 1
            if DEBUG_VERBOSE and debug_count <= 5:
                print(f"  - âœ“ MATCHED!")
            union((ka, ia), (kb, ib))
    
    if embed_th is not None:
        print(f"[INFO] Event matching: {debug_match_count} similar events matched (embedding-based, threshold={embed_th})")
    else:
        print(f"[INFO] Event matching: {debug_match_count} similar events matched (Jaccard-based, threshold={word_th})")
    if DEBUG_VERBOSE:
        print(f"[DEBUG] Total pairs compared: {debug_count}, Matched: {debug_match_count}")
    # Assign unified IDs
    root2id: Dict[Tuple[str,int], int] = {}
    event_map: Dict[Tuple[str,int], int] = {}
    nxt = 0
    for k,i,_ in items:
        r = find((k,i))
        if r not in root2id:
            root2id[r] = nxt
            nxt += 1
        event_map[(k,i)] = root2id[r]
    return event_map

def extract_event_comments(stream: StreamData, event: Dict[str, object], peak_pad: int) -> Tuple[List[str], List[str]]:
    """
    ã‚¤ãƒ™ãƒ³ãƒˆã«è©²å½“ã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆã¨ãã®è¨€èªãƒªã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚
    æˆ»ã‚Šå€¤ã¯ (ã‚³ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ, è¨€èªã®ãƒªã‚¹ãƒˆ)
    """
    gid, bin_id = event["group_id"], int(event["bin_id"])
    bins = build_relative_time_bins(stream.df_valid["timestamp"], stream.nr_bins)
    raw2g: Dict[int, int] = {}
    for g_id, members in enumerate(stream.groups):
        for t in members:
            raw2g[t] = g_id
    low, high = max(0, bin_id - peak_pad), min(stream.nr_bins - 1, bin_id + peak_pad)
    comments: List[str] = []
    langs: List[str] = []
    # 1:1å¯¾å¿œã®ãŸã‚ topics_valid ã¯ df_valid ã¨åŒã˜é †
    for i, row in stream.df_valid.iterrows():
        topic_id = stream.topics_valid[i]
        if topic_id == -1 or raw2g.get(topic_id, -1) != gid:
            continue
        ts = row["timestamp"]
        b = None
        for bi, iv in enumerate(bins):
            if ts >= iv.left and ts < iv.right:
                b = bi
                break
        if b is None:
            centers = np.array([iv.left.value for iv in bins], dtype=np.int64)
            b = int(np.argmin(np.abs(centers - int(ts.value))))
        if low <= b <= high:
            comments.append(row["message_clean"])
            # è¨€èªåˆ—ãŒã‚ã‚Œã°å–å¾—
            lang = row.get("lang") if isinstance(row, pd.Series) else None
            if lang is None and stream.languages is not None and i < len(stream.languages):
                lang = stream.languages.iloc[i]
            langs.append(lang if isinstance(lang, str) else "unk")
    return comments, langs

def js_distance(p: np.ndarray, q: np.ndarray) -> float:
    """
    Jensen-Shannonè·é›¢ã‚’è¨ˆç®—ï¼ˆç²¾åº¦å‘ä¸Šç‰ˆï¼‰
    
    æ”¹å–„å†…å®¹ï¼š
    1. NaN/Inf ãƒã‚§ãƒƒã‚¯
    2. ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«å‡¦ç†
    3. Laplace smoothing
    """
    p = p.astype(float)
    q = q.astype(float)
    
    # ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ãƒã‚§ãƒƒã‚¯
    p_sum = p.sum()
    q_sum = q.sum()
    
    if p_sum == 0 or q_sum == 0:
        # ã©ã¡ã‚‰ã‹ãŒç©ºã®å ´åˆã¯æœ€å¤§è·é›¢ã‚’è¿”ã™
        return 1.0
    
    # æ­£è¦åŒ–ï¼ˆLaplace smoothingé©ç”¨ï¼‰
    smoothing = 1e-10
    p = (p + smoothing) / (p_sum + smoothing * len(p))
    q = (q + smoothing) / (q_sum + smoothing * len(q))
    
    # ä¸­ç‚¹åˆ†å¸ƒ
    m = 0.5 * (p + q)
    
    # KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹è¨ˆç®—ï¼ˆæ•°å€¤å®‰å®šæ€§å‘ä¸Šï¼‰
    with np.errstate(divide='ignore', invalid='ignore'):
        kl_pm = np.sum(np.where(p > 0, p * np.log(p / m), 0.0))
        kl_qm = np.sum(np.where(q > 0, q * np.log(q / m), 0.0))
    
    # NaN/Infãƒã‚§ãƒƒã‚¯
    if np.isnan(kl_pm) or np.isinf(kl_pm):
        kl_pm = 0.0
    if np.isnan(kl_qm) or np.isinf(kl_qm):
        kl_qm = 0.0
    
    # JSè·é›¢
    js_div = 0.5 * (kl_pm + kl_qm)
    
    # è² ã®å€¤ã‚„NaNã®å‡¦ç†
    if js_div < 0 or np.isnan(js_div):
        return 0.0
    
    js_dist = np.sqrt(js_div)
    
    # æœ€çµ‚ãƒã‚§ãƒƒã‚¯
    if np.isnan(js_dist) or np.isinf(js_dist):
        return 1.0
    
    return float(min(js_dist, 1.0))  # [0, 1]ã«åˆ¶é™

def compute_lexical_distance(comments_a: List[str], comments_b: List[str], top_n: int = 1000) -> float:
    """
    ã‚³ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆåŒå£«ã®èªå½™åˆ†å¸ƒå·®ï¼ˆJensenâ€“Shannonè·é›¢ï¼‰ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆç²¾åº¦å‘ä¸Šç‰ˆï¼‰
    
    æ”¹å–„å†…å®¹ï¼š
    1. æœ€å°ãƒ‡ãƒ¼ã‚¿æ•°ãƒã‚§ãƒƒã‚¯
    2. ç©ºæ–‡å­—åˆ—é™¤å¤–ã®å¾¹åº•
    3. èªå½™ã®å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯
    """
    # ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ä½œæˆï¼ˆæ”¹å–„ç‰ˆæ­£è¦åŒ–ã‚’ä½¿ç”¨ï¼‰
    ca, cb = Counter(), Counter()
    
    for txt in comments_a:
        if not isinstance(txt, str):
            continue
        for w in txt.split():
            norm = normalize_term(w)
            if norm:  # ç©ºæ–‡å­—åˆ—é™¤å¤–
                ca[norm] += 1
    
    for txt in comments_b:
        if not isinstance(txt, str):
            continue
        for w in txt.split():
            norm = normalize_term(w)
            if norm:  # ç©ºæ–‡å­—åˆ—é™¤å¤–
                cb[norm] += 1
    
    # æœ€å°ãƒ‡ãƒ¼ã‚¿æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆç²¾åº¦å‘ä¸Šãƒ»èª¿æ•´æ¸ˆã¿ï¼‰
    min_words_threshold = 7  # å„å´æœ€ä½7å˜èªï¼ˆ10ã‹ã‚‰ç·©å’Œï¼‰
    if len(ca) < min_words_threshold or len(cb) < min_words_threshold:
        # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å ´åˆã¯æœ€å¤§è·é›¢ã‚’è¿”ã™
        return 1.0
    
    # èªå½™ã®çµ„ã¿åˆã‚ã›
    combined = ca + cb
    
    # ä¸Šä½å˜èªã‚’æœ€å¤§ top_n ã¾ã§
    vocab = [w for w, _ in combined.most_common(top_n) if w]  # ç©ºæ–‡å­—åˆ—é™¤å¤–
    
    if not vocab or len(vocab) < 5:  # æœ€ä½5å˜èªå¿…è¦
        return 1.0
    
    # ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ
    va = np.array([ca.get(w, 0) for w in vocab], dtype=float)
    vb = np.array([cb.get(w, 0) for w in vocab], dtype=float)
    
    # JSè·é›¢è¨ˆç®—ï¼ˆæ”¹å–„ç‰ˆã‚’ä½¿ç”¨ï¼‰
    return js_distance(va, vb)

# -------------------------
# Event-to-Event Comparisonï¼ˆã‚¤ãƒ™ãƒ³ãƒˆé–“é¡ä¼¼åº¦è¨ˆç®—ï¼‰
# -------------------------
def aggregate_event_representation(evts_dict: Dict[str, Dict[str, object]], 
                                   streams: Dict[str, 'StreamData'], 
                                   peak_pad: int) -> Dict[str, object]:
    """
    è¤‡æ•°é…ä¿¡è€…ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’çµ±åˆã—ã¦1ã¤ã®è¡¨ç¾ã‚’ä½œæˆ
    
    Parameters:
    - evts_dict: {stream_key: event_dict}
    - streams: {stream_key: StreamData}
    - peak_pad: ã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡ºã® padding
    
    Returns:
    - çµ±åˆã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆè¡¨ç¾ï¼ˆembedding, comments, topics, ãªã©ï¼‰
    """
    all_comments = []
    all_embeddings = []
    all_topics = set()
    bin_ids = []
    
    for stream_key, evt in evts_dict.items():
        comments, _ = extract_event_comments(streams[stream_key], evt, peak_pad)
        all_comments.extend(comments)
        
        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        if evt.get("embedding") is not None:
            all_embeddings.append(evt["embedding"])
        
        # ãƒˆãƒ”ãƒƒã‚¯èª
        top_words = evt.get("top_words", [])
        all_topics.update([normalize_term(w) for w in top_words if isinstance(w, str)])
        
        # æ™‚é–“æƒ…å ±
        bin_ids.append(int(evt.get("bin_id", -1)))
    
    # å¹³å‡åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
    if all_embeddings:
        aggregated_embedding = np.mean(all_embeddings, axis=0)
        # æ­£è¦åŒ–
        norm = np.linalg.norm(aggregated_embedding)
        if norm > 0:
            aggregated_embedding = aggregated_embedding / norm
    else:
        aggregated_embedding = None
    
    # å¹³å‡æ™‚é–“bin
    avg_bin = int(np.mean(bin_ids)) if bin_ids else -1
    
    # æ–°æ©Ÿèƒ½: æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå„binã”ã¨ã®ã‚³ãƒ¡ãƒ³ãƒˆæ•°ï¼‰
    # ã‚¤ãƒ™ãƒ³ãƒˆå‰å¾Œã®ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’é…åˆ—ã¨ã—ã¦å–å¾—
    comment_counts_per_bin = []
    for stream_key, evt in evts_dict.items():
        bin_id = int(evt.get("bin_id", -1))
        if bin_id >= 0 and stream_key in streams:
            stream_data = streams[stream_key]
            # ã‚¤ãƒ™ãƒ³ãƒˆå‰å¾ŒÂ±peak_padç¯„å›²ã®ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—
            counts = []
            
            # group_timeseriesã‹ã‚‰Timestampã§ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            df_ts = stream_data.group_timeseries.copy()
            if 'Timestamp' in df_ts.columns:
                df_ts = df_ts.sort_values('Timestamp')
                # å„ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆbinï¼‰ã”ã¨ã®ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’é›†è¨ˆ
                bin_frequencies = df_ts.groupby('Timestamp')['Frequency'].sum()
                
                for offset in range(-peak_pad, peak_pad + 1):
                    target_bin = bin_id + offset
                    if 0 <= target_bin < len(bin_frequencies):
                        counts.append(int(bin_frequencies.iloc[target_bin]))
                    else:
                        counts.append(0)
            
            if counts:
                comment_counts_per_bin.extend(counts)
    
    # è¤‡æ•°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®å ´åˆã¯å¹³å‡ã‚’å–ã‚‹
    if comment_counts_per_bin and len(evts_dict) > 1:
        bins_per_stream = (2 * peak_pad + 1)
        averaged_counts = []
        for i in range(bins_per_stream):
            bin_values = [comment_counts_per_bin[j * bins_per_stream + i] 
                         for j in range(len(evts_dict)) 
                         if j * bins_per_stream + i < len(comment_counts_per_bin)]
            if bin_values:
                averaged_counts.append(np.mean(bin_values))
        comment_counts_per_bin = averaged_counts

    return {
        "embedding": aggregated_embedding,
        "comments": all_comments,
        "topics": all_topics,
        "num_streams": len(evts_dict),
        "stream_keys": list(evts_dict.keys()),
        "avg_bin_id": avg_bin,
        "num_comments": len(all_comments),
        "comment_counts_per_bin": comment_counts_per_bin  # æ–°æ©Ÿèƒ½ï¼šæ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³
    }

def compute_event_to_event_similarity(event_A: Dict[str, object], 
                                      event_B: Dict[str, object]) -> Dict[str, float]:
    """
    2ã¤ã®ã‚¤ãƒ™ãƒ³ãƒˆé–“ã®é¡ä¼¼åº¦ã‚’è¤‡æ•°ã®æŒ‡æ¨™ã§è¨ˆç®—

    Returns:
    - embedding_similarity: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
    - topic_jaccard: ãƒˆãƒ”ãƒƒã‚¯èªã®Jaccardä¿‚æ•°
    - lexical_similarity: ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹ã®JSè·é›¢ãƒ™ãƒ¼ã‚¹é¡ä¼¼åº¦ï¼ˆ1-JSï¼‰
    - combined_score: ç·åˆã‚¹ã‚³ã‚¢ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
    - context_penalty: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸ä¸€è‡´ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆ0.0-1.0ã€1.0=ä¸€è‡´ï¼‰
    """
    # 0. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¤œè¨¼ï¼ˆã‚¹ãƒãƒ¼ãƒ„ç¨®åˆ¥ãªã©ï¼‰
    # ä¿®æ­£: topics set ã§ã¯ãªã comments ã®ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ï¼ˆè¤‡åˆèªæ¤œå‡ºã®ãŸã‚ï¼‰
    comments_A_str = " ".join(event_A["comments"]).lower() if event_A["comments"] else ""
    comments_B_str = " ".join(event_B["comments"]).lower() if event_B["comments"] else ""
    
    # ãƒˆãƒ”ãƒƒã‚¯èªã‚‚ä½µç”¨ï¼ˆè»½é‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
    topics_A_str = " ".join(event_A["topics"]).lower()
    topics_B_str = " ".join(event_B["topics"]).lower()
    
    # ä¸¡æ–¹ã‚’çµåˆã—ã¦ãƒã‚§ãƒƒã‚¯
    full_text_A = f"{topics_A_str} {comments_A_str}"
    full_text_B = f"{topics_B_str} {comments_B_str}"
    
    # ã‚¹ãƒãƒ¼ãƒ„ç¨®åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å®šç¾©
    # ã‚¹ãƒãƒ¼ãƒ„ç¨®åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ‹¡å¼µç‰ˆãƒ»ç²¾å¯†åŒ–ï¼‰
    # ã‚¹ãƒãƒ¼ãƒ„åˆ¤å®šç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå³æ ¼åŒ–ç‰ˆï¼šç‰¹å¾´çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ï¼‰
    baseball_keywords = [
        # é‡çƒç‰¹æœ‰ã®ç”¨èªã®ã¿ï¼ˆä¸€èˆ¬çš„ã™ãã‚‹å˜èªã‚’é™¤å¤–ï¼‰
        "ãƒ”ãƒƒãƒãƒ£ãƒ¼", "ãƒãƒƒã‚¿ãƒ¼", "ãƒ›ãƒ¼ãƒ ãƒ©ãƒ³", "æŠ•æ‰‹", "æ‰“è€…", "æŠ•çƒ",
        "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯", "ãƒœãƒ¼ãƒ«", "ã‚¤ãƒ‹ãƒ³ã‚°", "ç”²å­åœ’", "åºƒé™µ",
        "pitcher", "homerun", "batter", "strike", "inning", "baseball",
        "yankees", "dodgers", "mlb"
    ]
    
    soccer_keywords = [
        # ã‚µãƒƒã‚«ãƒ¼ç‰¹æœ‰ã®ç”¨èªã®ã¿ï¼ˆè¨€åŠã ã‘ã®å˜èªã‚’é™¤å¤–ï¼‰
        "ã‚ªãƒ•ã‚µã‚¤ãƒ‰", "ãƒ•ãƒªãƒ¼ã‚­ãƒƒã‚¯", "ã‚´ãƒ¼ãƒ«ã‚­ãƒ¼ãƒ‘ãƒ¼", "ãƒ‰ãƒªãƒ–ãƒ«",
        "pkæˆ¦", "å»¶é•·æˆ¦", "ãƒãƒ¼ãƒ•ã‚¿ã‚¤ãƒ ", "ã‚³ãƒ¼ãƒŠãƒ¼ã‚­ãƒƒã‚¯",
        "offside", "free kick", "goalkeeper", "penalty kick",
        "real madrid", "barcelona", "man united", "liverpool"
    ]
    
    # å„ã‚¤ãƒ™ãƒ³ãƒˆãŒã©ã®ã‚¹ãƒãƒ¼ãƒ„ã‹åˆ¤å®šï¼ˆè¤‡åˆèªã‚‚æ¤œå‡ºå¯èƒ½ï¼‰
    is_baseball_A = any(kw in full_text_A for kw in baseball_keywords)
    is_soccer_A = any(kw in full_text_A for kw in soccer_keywords)
    is_baseball_B = any(kw in full_text_B for kw in baseball_keywords)
    is_soccer_B = any(kw in full_text_B for kw in soccer_keywords)
    
    # ç•°ãªã‚‹ã‚¹ãƒãƒ¼ãƒ„åŒå£«ã®å ´åˆã€context_penalty = 0.3ï¼ˆé¡ä¼¼åº¦ã‚’å¤§å¹…ã«ä¸‹ã’ã‚‹ï¼‰
    # ã€é‡è¦ã€‘ä¸¡æ–¹ã¨ã‚‚ä¸¡æ–¹ã®ã‚¹ãƒãƒ¼ãƒ„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€å ´åˆã¯é©ç”¨ã—ãªã„ï¼ˆæ›–æ˜§ãªã‚±ãƒ¼ã‚¹ï¼‰
    context_penalty = 1.0
    
    # XORè«–ç†: ç‰‡æ–¹ã ã‘ãŒãã®ã‚¹ãƒãƒ¼ãƒ„ã®å ´åˆã®ã¿ãƒšãƒŠãƒ«ãƒ†ã‚£
    is_pure_baseball_A = is_baseball_A and not is_soccer_A
    is_pure_soccer_A = is_soccer_A and not is_baseball_A
    is_pure_baseball_B = is_baseball_B and not is_soccer_B
    is_pure_soccer_B = is_soccer_B and not is_baseball_B
    
    if (is_pure_baseball_A and is_pure_soccer_B) or (is_pure_soccer_A and is_pure_baseball_B):
        context_penalty = 0.3  # 70%ãƒšãƒŠãƒ«ãƒ†ã‚£
        # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆç•°ãªã‚‹ã‚¹ãƒãƒ¼ãƒ„æ¤œå‡ºæ™‚ã®ã¿ï¼‰
        # event_A, event_B ã‹ã‚‰ ID ã‚’å–å¾—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        evt_a_id = event_A.get("event_id", "?")
        evt_b_id = event_B.get("event_id", "?")
        print(f"[CONTEXT] Different sports detected! (Event {evt_a_id} vs {evt_b_id})")
        print(f"  Event A: Baseball={is_baseball_A}, Soccer={is_soccer_A}")
        print(f"  Event B: Baseball={is_baseball_B}, Soccer={is_soccer_B}")
        print(f"  Comments A sample: {comments_A_str[:60] if comments_A_str else 'N/A'}...")
        print(f"  Comments B sample: {comments_B_str[:60] if comments_B_str else 'N/A'}...")
        print(f"  Context penalty: {context_penalty}")
    
    # 1. åŸ‹ã‚è¾¼ã¿é¡ä¼¼åº¦
    if event_A["embedding"] is not None and event_B["embedding"] is not None:
        embedding_sim = float(np.dot(event_A["embedding"], event_B["embedding"]))
        # æ—¢ã«æ­£è¦åŒ–æ¸ˆã¿ãªã®ã§dotãŒã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’é©ç”¨
        embedding_sim *= context_penalty
    else:
        embedding_sim = None

    # 2. ãƒˆãƒ”ãƒƒã‚¯Jaccardï¼ˆåŒç¾©èªæ­£è¦åŒ– + TF-IDFé‡ã¿ä»˜ã‘é©ç”¨ï¼‰
    topics_A = event_A["topics"]
    topics_B = event_B["topics"]
    if topics_A or topics_B:
        # åŒç¾©èªæ­£è¦åŒ–ã‚’é©ç”¨ã—ã¦ãƒˆãƒ”ãƒƒã‚¯ã‚’çµ±ä¸€
        normalized_A = {}  # {topic: count} ã®è¾æ›¸å½¢å¼ã«å¤‰æ›´
        for topic in topics_A:
            normalized = normalize_with_synonyms(topic)
            normalized_A[normalized] = normalized_A.get(normalized, 0) + 1
        
        normalized_B = {}
        for topic in topics_B:
            normalized = normalize_with_synonyms(topic)
            normalized_B[normalized] = normalized_B.get(normalized, 0) + 1
        
        # TF-IDFé¢¨ã®é‡ã¿ä»˜ãJaccardä¿‚æ•°ã‚’è¨ˆç®—
        # é »åº¦ã®å°‘ãªã„ï¼ˆé‡è¦ãªï¼‰å˜èªã«é«˜ã„é‡ã¿ã‚’ä»˜ä¸
        all_topics = set(normalized_A.keys()) | set(normalized_B.keys())
        
        if len(all_topics) > 0:
            # å„ãƒˆãƒ”ãƒƒã‚¯ã®IDFé¢¨ã‚¹ã‚³ã‚¢ï¼ˆå‡ºç¾å›æ•°ã®é€†æ•°ï¼‰
            weighted_intersection = 0.0
            weighted_union = 0.0
            
            for topic in all_topics:
                count_A = normalized_A.get(topic, 0)
                count_B = normalized_B.get(topic, 0)
                
                # é‡ã¿: å‡ºç¾å›æ•°ãŒå°‘ãªã„ã»ã©é‡è¦ï¼ˆæœ€å°1ã€æœ€å¤§5ï¼‰
                weight = min(5.0, 1.0 / (min(count_A, count_B) + 0.1)) if count_A > 0 and count_B > 0 else 1.0
                
                if count_A > 0 and count_B > 0:
                    weighted_intersection += weight
                if count_A > 0 or count_B > 0:
                    weighted_union += weight
            
            topic_jaccard = weighted_intersection / weighted_union if weighted_union > 0 else 0.0
        else:
            topic_jaccard = 0.0
    else:
        topic_jaccard = 0.0    # 3. èªå½™é¡ä¼¼åº¦ï¼ˆJSè·é›¢ãƒ™ãƒ¼ã‚¹ï¼‰
    if event_A["comments"] and event_B["comments"]:
        lex_dist = compute_lexical_distance(event_A["comments"], event_B["comments"])
        lexical_sim = max(0.0, 1.0 - lex_dist)
    else:
        lexical_sim = 0.0
    
    # 4. æ™‚é–“è¿‘æ¥æ€§ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ã€é¡ä¼¼åº¦ã§ã¯ãªãè¿½åŠ æƒ…å ±ï¼‰
    time_diff = abs(event_A["avg_bin_id"] - event_B["avg_bin_id"])
    
    # 5. æ™‚é–“çš„ç›¸é–¢åˆ†æï¼ˆæ–°æ©Ÿèƒ½ï¼šã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ç›¸é–¢ï¼‰
    temporal_correlation = 0.0
    if "comment_counts_per_bin" in event_A and "comment_counts_per_bin" in event_B:
        counts_A = np.array(event_A["comment_counts_per_bin"])
        counts_B = np.array(event_B["comment_counts_per_bin"])
        if len(counts_A) > 1 and len(counts_B) > 1 and len(counts_A) == len(counts_B):
            try:
                # Pearsonç›¸é–¢ä¿‚æ•°ã‚’numpyã§è¨ˆç®—ï¼ˆscipyä¸è¦ï¼‰
                mean_A = np.mean(counts_A)
                mean_B = np.mean(counts_B)
                std_A = np.std(counts_A)
                std_B = np.std(counts_B)
                
                if std_A > 0 and std_B > 0:
                    correlation = np.mean((counts_A - mean_A) * (counts_B - mean_B)) / (std_A * std_B)
                    # ç›¸é–¢ãŒ0.3ä»¥ä¸Šã®å ´åˆã®ã¿æ¡ç”¨ï¼ˆä¸­ç¨‹åº¦ä»¥ä¸Šã®ç›¸é–¢ï¼‰
                    if not np.isnan(correlation) and correlation > 0.3:
                        temporal_correlation = correlation
            except:
                temporal_correlation = 0.0
    
    # 6. ã‚¤ãƒ™ãƒ³ãƒˆä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆæ–°æ©Ÿèƒ½ï¼šè¤‡æ•°æŒ‡æ¨™ã‹ã‚‰ã®ç·åˆè©•ä¾¡ï¼‰
    confidence_score = 0.0
    confidence_factors = []
    
    # Factor 1: ã‚³ãƒ¡ãƒ³ãƒˆæ•°ï¼ˆå¤šã„ã»ã©ä¿¡é ¼æ€§é«˜ã„ï¼‰
    total_comments_A = len(event_A.get("comments", []))
    total_comments_B = len(event_B.get("comments", []))
    comment_factor = min(1.0, (total_comments_A + total_comments_B) / 200.0)
    confidence_factors.append(comment_factor)
    
    # Factor 2: ãƒˆãƒ”ãƒƒã‚¯ã®æ˜ç¢ºæ€§ï¼ˆãƒˆãƒ”ãƒƒã‚¯æ•°ãŒé©åº¦ã«ã‚ã‚‹ã»ã©ä¿¡é ¼æ€§é«˜ã„ï¼‰
    topic_count_A = len(event_A.get("topics", set()))
    topic_count_B = len(event_B.get("topics", set()))
    topic_factor = min(1.0, (topic_count_A + topic_count_B) / 20.0)
    confidence_factors.append(topic_factor)
    
    # Factor 3: è¤‡æ•°æŒ‡æ¨™ã®ä¸€è‡´åº¦ï¼ˆembedding, topic, lexicalãŒå…¨ã¦é«˜ã„ã»ã©ä¿¡é ¼æ€§é«˜ã„ï¼‰
    consistency_scores = []
    if embedding_sim is not None:
        consistency_scores.append(embedding_sim)
    if topic_jaccard > 0:
        consistency_scores.append(topic_jaccard)
    if lexical_sim > 0:
        consistency_scores.append(lexical_sim)
    
    if len(consistency_scores) >= 2:
        # å„ã‚¹ã‚³ã‚¢ã®æ¨™æº–åå·®ãŒå°ã•ã„ã»ã©ä¸€è‡´åº¦ãŒé«˜ã„
        consistency_factor = 1.0 - min(1.0, np.std(consistency_scores) / 0.5)
        confidence_factors.append(consistency_factor)
    
    # Factor 4: æ™‚é–“çš„ç›¸é–¢ï¼ˆé«˜ã„ã»ã©ä¿¡é ¼æ€§é«˜ã„ï¼‰
    if temporal_correlation > 0.3:
        confidence_factors.append(temporal_correlation)
    
    # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã¯å…¨Factorã®å¹³å‡
    if confidence_factors:
        confidence_score = np.mean(confidence_factors)
    
    # 7. ç·åˆã‚¹ã‚³ã‚¢ï¼ˆé‡ã¿ä»˜ãå¹³å‡ + æ™‚é–“çš„ç›¸é–¢ã®ãƒœãƒ¼ãƒŠã‚¹ï¼‰
    # ã€æ”¹å–„ã€‘ç‹¬è‡ªN-gramæŠ½å‡ºã«ã‚ˆã‚Štopic_jaccardãŒå‘ä¸Šã—ãŸãŸã‚ã€ãƒˆãƒ”ãƒƒã‚¯ã®é‡ã¿ã‚’å¢—åŠ 
    # Before: embedding 0.5 : lexical 0.3 : topic 0.2
    # After:  embedding 0.4 : lexical 0.2 : topic 0.4 (ãƒˆãƒ”ãƒƒã‚¯ã‚’é‡è¦–)
    # Phase 2: embedding 0.3 : lexical 0.15 : topic 0.55 (å¤±æ•—: Topicé‡è¦–ã§å…¨ä½“ãŒæ‚ªåŒ–)
    # Phase 3: embedding 0.7 : lexical 0.1 : topic 0.2 (æœ€é©åŒ–: çµ±è¨ˆçš„æ¤œè¨¼æ¸ˆã¿, p<0.001)
    if embedding_sim is not None:
        combined_score = embedding_sim * 0.70 + lexical_sim * 0.10 + topic_jaccard * 0.20
        main_similarity = embedding_sim
    else:
        # åŸ‹ã‚è¾¼ã¿ãŒãªã„å ´åˆã¯ã€ãƒˆãƒ”ãƒƒã‚¯ã¨èªå½™ã‚’åŒç­‰ã«æ‰±ã†
        combined_score = lexical_sim * 0.5 + topic_jaccard * 0.5
        main_similarity = lexical_sim
    
    # æ™‚é–“çš„ç›¸é–¢ãŒé«˜ã„å ´åˆã€combined_scoreã«ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæ”¹å–„: æœ€å¤§+15%ï¼‰
    if temporal_correlation > 0.5:
        bonus_factor = 1.0 + temporal_correlation * 0.15  # 0.10 â†’ 0.15ã«å¢—åŠ 
        combined_score = min(1.0, combined_score * bonus_factor)
    elif temporal_correlation > 0.7:
        # éå¸¸ã«é«˜ã„ç›¸é–¢ã®å ´åˆã€ã•ã‚‰ã«ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæœ€å¤§+25%ï¼‰
        bonus_factor = 1.0 + temporal_correlation * 0.25
        combined_score = min(1.0, combined_score * bonus_factor)

    return {
        "embedding_similarity": embedding_sim,
        "topic_jaccard": topic_jaccard,
        "lexical_similarity": lexical_sim,
        "combined_score": combined_score,
        "main_similarity": main_similarity,
        "time_diff_bins": time_diff,
        "context_penalty": context_penalty,  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¤œè¨¼çµæœ
        "temporal_correlation": temporal_correlation,  # æ–°æ©Ÿèƒ½ï¼šæ™‚é–“çš„ç›¸é–¢
        "confidence_score": confidence_score  # æ–°æ©Ÿèƒ½ï¼šä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    }

def generate_event_similarity_matrix(events_by_sim_id: Dict[int, Dict[str, Dict[str, object]]], 
                                     streams: Dict[str, 'StreamData'], 
                                     peak_pad: int) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    å…¨ã‚¤ãƒ™ãƒ³ãƒˆãƒšã‚¢ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã¨ãƒšã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    
    Returns:
    - similarity_matrix_df: NÃ—Né¡ä¼¼åº¦è¡Œåˆ—
    - event_pairs_df: ãƒšã‚¢ã”ã¨ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿
    """
    # å„ã‚¤ãƒ™ãƒ³ãƒˆã®çµ±åˆè¡¨ç¾ã‚’ä½œæˆ
    event_representations = {}
    event_labels = {}
    
    for sim_id, evts_dict in events_by_sim_id.items():
        if len(evts_dict) < 2:  # 2é…ä¿¡è€…ä»¥ä¸ŠãŒå‚åŠ ã—ã¦ã„ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã®ã¿
            continue
        
        event_representations[sim_id] = aggregate_event_representation(
            evts_dict, streams, peak_pad
        )

        # ãƒ©ãƒ™ãƒ«ä½œæˆï¼ˆN-gramãƒˆãƒ”ãƒƒã‚¯èªã‚’ä½¿ç”¨ + ã‚¹ãƒˆãƒªãƒ¼ãƒ åï¼‰
        # ã€ä¿®æ­£ã€‘å„ã‚¤ãƒ™ãƒ³ãƒˆã®N-gramãƒˆãƒ”ãƒƒã‚¯ã‹ã‚‰ä¸Šä½èªã‚’æŠ½å‡º
        all_topics = []
        for evt in evts_dict.values():
            evt_topics = evt.get("topics", [])
            if evt_topics:
                # å„ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰ä¸Šä½3èªã‚’æŠ½å‡º
                all_topics.extend(evt_topics[:3])
        
        # é‡è¤‡ã‚’é™¤å»ã—ã¤ã¤é †åºã‚’ä¿æŒï¼ˆå‡ºç¾é †ï¼‰
        seen = set()
        unique_topics = []
        for t in all_topics:
            if t not in seen:
                seen.add(t)
                unique_topics.append(t)
        
        # ãƒ©ãƒ™ãƒ«ä½œæˆ: ä¸Šä½3-5èªã‚’çµåˆ
        if unique_topics:
            label = "ãƒ»".join(unique_topics[:5])
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤ã„ãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨
            first_evt = list(evts_dict.values())[0]
            label = first_evt.get("label", f"Event_{sim_id}")
        
        # å‚åŠ ã‚¹ãƒˆãƒªãƒ¼ãƒ åã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å‰Šé™¤ã—ã¦ãƒ™ãƒ¼ã‚¹åã®ã¿ï¼‰
        stream_names = sorted([os.path.basename(k).replace(".csv", "") for k in evts_dict.keys()])
        stream_suffix = f" ({', '.join(stream_names)})"
        
        # ãƒˆãƒ”ãƒƒã‚¯ãƒ©ãƒ™ãƒ«ã‚’çŸ­ç¸®ï¼ˆæœ€å¤§35æ–‡å­—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒ åã®ä½™è£•ã‚’ç¢ºä¿ï¼‰
        if len(label) > 35:
            label = label[:32] + "..."
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ åã‚’è¿½åŠ 
        label_with_stream = label + stream_suffix
        event_labels[sim_id] = label_with_stream    # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºé¡ä¼¼åº¦è¨ˆç®—
    event_ids = sorted(event_representations.keys())
    n = len(event_ids)
    
    if n == 0:
        # ã‚¤ãƒ™ãƒ³ãƒˆãŒãªã„å ´åˆã¯ç©ºã®DataFrameã‚’è¿”ã™
        return pd.DataFrame(), pd.DataFrame()
    
    similarity_matrix = np.zeros((n, n))
    event_pairs = []
    
    for i in range(n):
        for j in range(i+1, n):
            event_A_id = event_ids[i]
            event_B_id = event_ids[j]
            
            event_A = event_representations[event_A_id]
            event_B = event_representations[event_B_id]
            
            sim_scores = compute_event_to_event_similarity(event_A, event_B)
            
            # ä»£è¡¨é¡ä¼¼åº¦
            main_sim = sim_scores["main_similarity"]
            
            similarity_matrix[i, j] = main_sim
            similarity_matrix[j, i] = main_sim
            
            # è©³ç´°ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            event_pairs.append({
                "event_A_id": event_A_id,
                "event_B_id": event_B_id,
                "event_A_label": event_labels[event_A_id],
                "event_B_label": event_labels[event_B_id],
                "event_A_streams": event_A["num_streams"],
                "event_B_streams": event_B["num_streams"],
                "event_A_comments": event_A["num_comments"],
                "event_B_comments": event_B["num_comments"],
                "embedding_similarity": sim_scores["embedding_similarity"],
                "topic_jaccard": sim_scores["topic_jaccard"],
                "lexical_similarity": sim_scores["lexical_similarity"],
                "combined_score": sim_scores["combined_score"],
                "main_similarity": main_sim,
                "time_diff_bins": sim_scores["time_diff_bins"],
                "context_penalty": sim_scores.get("context_penalty", 1.0),  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¤œè¨¼ã‚¹ã‚³ã‚¢
                "temporal_correlation": sim_scores.get("temporal_correlation", 0.0),  # æ–°æ©Ÿèƒ½ï¼šæ™‚é–“çš„ç›¸é–¢
                "confidence_score": sim_scores.get("confidence_score", 0.0),  # æ–°æ©Ÿèƒ½ï¼šä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
            })
    
    # å¯¾è§’ç·šã¯1.0ï¼ˆè‡ªåˆ†è‡ªèº«ã¨ã®é¡ä¼¼åº¦ï¼‰
    np.fill_diagonal(similarity_matrix, 1.0)
    
    # DataFrameã«å¤‰æ›
    row_labels = [f"E{eid}: {event_labels[eid][:20]}" for eid in event_ids]
    col_labels = [f"E{eid}" for eid in event_ids]
    
    sim_df = pd.DataFrame(
        similarity_matrix,
        index=row_labels,
        columns=col_labels
    )
    
    pairs_df = pd.DataFrame(event_pairs).sort_values("main_similarity", ascending=False)
    
    return sim_df, pairs_df

def save_event_similarity_heatmap(sim_df: pd.DataFrame, out_csv: str, out_png: str):
    """
    ã‚¤ãƒ™ãƒ³ãƒˆé–“é¡ä¼¼åº¦ã® NÃ—N ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä¿å­˜
    """
    if sim_df.empty:
        print("[WARN] Empty similarity matrix, skipping heatmap.")
        return
    
    # CSVä¿å­˜
    os.makedirs(os.path.dirname(out_csv), exist_ok=True)
    sim_df.to_csv(out_csv, encoding="utf-8-sig")
    
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»
    n = len(sim_df)
    figsize = (max(10, n * 0.6), max(8, n * 0.5))
    
    plt.figure(figsize=figsize)
    
    # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—: é»„è‰²ï¼ˆä½é¡ä¼¼ï¼‰â†’ èµ¤ï¼ˆé«˜é¡ä¼¼ï¼‰
    im = plt.imshow(sim_df.values, cmap="YlOrRd", vmin=0, vmax=1, aspect='auto')
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã¨è»¸ãƒ©ãƒ™ãƒ«
    plt.title("Event-to-Event Similarity Matrix\n(ã‚¤ãƒ™ãƒ³ãƒˆé–“é¡ä¼¼åº¦)", fontsize=14, pad=20)
    plt.xlabel("Events", fontsize=12)
    plt.ylabel("Events", fontsize=12)
    
    # è»¸ã®ç›®ç››ã‚Š
    plt.xticks(range(n), sim_df.columns, rotation=45, ha='right', fontsize=9)
    plt.yticks(range(n), sim_df.index, fontsize=9)
    
    # ã‚«ãƒ©ãƒ¼ãƒãƒ¼
    cbar = plt.colorbar(im, fraction=0.046, pad=0.04)
    cbar.set_label("Similarity", rotation=270, labelpad=20, fontsize=11)
    
    # æ•°å€¤ã‚’è¡¨ç¤ºï¼ˆã‚¤ãƒ™ãƒ³ãƒˆãŒå¤šã™ããªã„å ´åˆï¼‰
    if n <= 15:
        for i in range(n):
            for j in range(n):
                val = sim_df.values[i, j]
                color = 'white' if val > 0.5 else 'black'
                plt.text(j, i, f'{val:.2f}', ha='center', va='center', 
                        color=color, fontsize=8)
    
    plt.tight_layout()
    plt.savefig(out_png, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Saved event-to-event similarity heatmap: {out_png}")

# -------------------------
# å¯è¦–åŒ–: ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ»è·é›¢è¡Œåˆ—ãƒ»ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
# -------------------------
def save_csv_and_png_heatmap(df: pd.DataFrame, out_csv: str, out_png: str, title: str):
    os.makedirs(os.path.dirname(out_csv), exist_ok=True)
    # å¸¸ã«å®Œå…¨ãªè¡¨ã‚’CSVã«ä¿å­˜
    df.to_csv(out_csv, index=True, encoding="utf-8-sig")
    # å¯è¦–åŒ–ã¯æ•°å€¤åˆ—ã®ã¿ã«é™å®šã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’é˜²ã
    df_plot = df.select_dtypes(include=[np.number])
    if df_plot is None or df_plot.empty:
        print(f"[WARN] Heatmap skipped for {out_png}: no numeric data to plot")
        return
    # å›³ã‚µã‚¤ã‚ºèª¿æ•´ï¼ˆè«–æ–‡å‘ã‘ã«ã‚„ã‚„åºƒã‚ï¼‰
    fig_width = max(8, 0.6 * len(df_plot.columns) + 3)
    fig_height = max(8, 0.5 * len(df_plot.index) + 3)
    plt.figure(figsize=(fig_width, fig_height))
    vals = df_plot.values.astype(float)
    # presenceï¼ˆ0/1ï¼‰ã®è¦‹ã‚„ã™ã„é…è‰²ã«è‡ªå‹•èª¿æ•´
    unique_vals = np.unique(vals)
    is_binary = set(unique_vals.tolist()) <= {0.0, 1.0}
    if is_binary:
        im = plt.imshow(vals, aspect="auto", cmap="Greens", vmin=0, vmax=1)
    else:
        vmin = float(np.nanmin(vals)) if np.isfinite(vals).any() else 0.0
        vmax = float(np.nanmax(vals)) if np.isfinite(vals).any() else 1.0
        if vmin == vmax:
            vmin, vmax = 0.0, 1.0
        im = plt.imshow(vals, aspect="auto", cmap="viridis", vmin=vmin, vmax=vmax)
    plt.title(title)
    plt.xlabel("Streams")
    plt.ylabel("Events")
    # xè»¸ãƒ©ãƒ™ãƒ«: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã¾ãŸã¯åˆ—åï¼‰ã‚’æ”¹è¡Œä»˜ãã«ã—ã¦é•·ã•ã‚’èª¿æ•´
    x_labels = []
    for c in df_plot.columns:
        s = os.path.basename(str(c))
        if len(s) > 12:
            s = '\n'.join([s[i:i+12] for i in range(0, len(s), 12)])
        x_labels.append(s)
    plt.xticks(range(len(df_plot.columns)), x_labels, rotation=40, ha="right", fontsize=8)
    # yè»¸ãƒ©ãƒ™ãƒ«: ã‚¤ãƒ™ãƒ³ãƒˆãƒ©ãƒ™ãƒ«ãŒé•·ã„å ´åˆã¯æ”¹è¡Œã‚’æŒ¿å…¥ï¼ˆå…ƒã®indexã‚’ãã®ã¾ã¾åˆ©ç”¨ï¼‰
    y_labels = []
    for idx_label in df_plot.index:
        s = str(idx_label)
        if len(s) > 20:
            s = '\n'.join([s[i:i+20] for i in range(0, len(s), 20)])
        y_labels.append(s)
    plt.yticks(range(len(df_plot.index)), y_labels, fontsize=9)
    cb = plt.colorbar(im); cb.set_label("Similarity" if not is_binary else "Presence")
    # ã‚»ãƒ«æ³¨é‡ˆï¼ˆå°ã•ã‚ã®è¡Œåˆ—ã®ã¿ï¼‰
    total_cells = vals.shape[0] * vals.shape[1]
    if total_cells <= 400:
        for i in range(vals.shape[0]):
            for j in range(vals.shape[1]):
                v = vals[i, j]
                if is_binary:
                    if v >= 0.5:
                        plt.text(j, i, "1", ha="center", va="center", color="white", fontsize=9, fontweight="bold")
                else:
                    txt_color = "white" if v > (np.nanmin(vals) + np.nanmax(vals)) / 2 else "black"
                    try:
                        plt.text(j, i, f"{v:.2f}", ha="center", va="center", color=txt_color, fontsize=8)
                    except Exception:
                        pass
    # ã‚°ãƒªãƒƒãƒ‰ç·š
    plt.grid(which='both', color='lightgray', linestyle='-', linewidth=0.3)
    plt.gca().set_xticks(np.arange(-0.5, vals.shape[1], 1), minor=True)
    plt.gca().set_yticks(np.arange(-0.5, vals.shape[0], 1), minor=True)
    plt.grid(which='minor', color='lightgray', linestyle='-', linewidth=0.3)
    plt.gca().tick_params(which='minor', bottom=False, left=False)
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig(out_png, dpi=220)
    plt.close()
    print(f"Saved heatmap: {out_png}")

def _save_matched_summary_table(presence_df: pd.DataFrame, matched_df: pd.DataFrame, out_png: str, top_k: int = 15) -> None:
    """ä¸€è‡´ã‚¤ãƒ™ãƒ³ãƒˆã®æ¦‚è¦ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ PNG ã§ä¿å­˜ã€‚
    åˆ—: Time, Best Pair, Similarity, Label
    """
    try:
        if presence_df is None or presence_df.empty or matched_df is None or matched_df.empty:
            return
        # presence ã® time_label/row_label ã¨ matched ã® (group_id,bin_id) ã‚’çªåˆ
        lef = presence_df[["group_id","bin_id","time_label","row_label"]].drop_duplicates()
        rig = matched_df.copy()
        # best_pair/best_sim ãŒç„¡ã„æ—§ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œ
        if "best_pair" not in rig.columns:
            # best ãƒšã‚¢ã¯ (lex) ãŒæœ€å°ã®ãƒšã‚¢ã‹ã‚‰æ¨å®š
            lex_cols = [c for c in rig.columns if c.endswith("(lex)")]
            if lex_cols:
                best_idx = np.argmin(rig[lex_cols].values, axis=1)
                rig["best_pair"] = [lex_cols[i].replace(" (lex)", "") for i in best_idx]
                rig["best_sim"] = 1.0 - np.take_along_axis(rig[lex_cols].values, best_idx.reshape(-1,1), axis=1).ravel()
        m = pd.merge(rig, lef, on=["group_id","bin_id"], how="left")
        # è¡¨ç¤ºç”¨åˆ—ã«æ•´å½¢
        view = m[["time_label","best_pair","best_sim","label"]].copy()
        view = view.rename(columns={"time_label":"Time","best_pair":"Best Pair","best_sim":"Similarity","label":"Topic"})
        # ä¸¦ã¹æ›¿ãˆï¼ˆSimilarity é™é †ã€Time æ˜‡é †ï¼‰
        if "Similarity" in view.columns:
            view = view.sort_values(["Similarity","Time"], ascending=[False, True])
        else:
            view = view.sort_values(["Time"], ascending=True)
        # ä¸Šä½ã®ã¿
        if len(view) > top_k:
            view = view.head(top_k)
        # æ•°å€¤ã®ä¸¸ã‚
        if "Similarity" in view.columns:
            view["Similarity"] = view["Similarity"].map(lambda x: f"{x:.2f}")
        # ä¿å­˜
        save_df_as_table_png(view, out_png, title="Matched Events Summary (Top)")
    except Exception as e:
        print(f"[WARN] failed to save matched summary: {e}")

def save_png_distance_matrix(mat: pd.DataFrame, out_csv: str, out_png: str, title: str):
    os.makedirs(os.path.dirname(out_csv), exist_ok=True)
    mat.to_csv(out_csv, index=True, encoding="utf-8-sig")
    plt.figure(figsize=(max(6, 0.35*len(mat.columns)+3), max(6, 0.35*len(mat.index)+3)))
    plt.imshow(mat.values, vmin=0, vmax=1)
    plt.title(title); plt.xlabel("Stream"); plt.ylabel("Stream")
    ticks = range(len(mat.columns))
    labels = [os.path.basename(c) for c in mat.columns]
    plt.xticks(ticks, labels, rotation=45, ha="right", fontsize=9)
    plt.yticks(ticks, labels, fontsize=9)
    cb = plt.colorbar(); cb.set_label("Jensenâ€“Shannon distance")
    plt.tight_layout(); plt.savefig(out_png, dpi=220); plt.close()
    print(f"Saved distance matrix: {out_png}")

def save_emoji_timeline_heatmap(df: pd.DataFrame, out_csv: str, out_png: str, title: str):
    """Save an emoji timeline heatmap (time x emoji) per stream.
    df: index=time label (e.g., HH:MM), columns=emoji char, values=counts.
    è»¢ç½®ã—ã¦æ™‚é–“ã‚’æ¨ªè»¸ã€çµµæ–‡å­—ã‚’ç¸¦è»¸ã«ã™ã‚‹ï¼ˆæ¨ªé•·ã§è¦‹ã‚„ã™ã„ï¼‰
    """
    os.makedirs(os.path.dirname(out_csv), exist_ok=True)
    df.to_csv(out_csv, index=True, encoding="utf-8-sig")
    if df.empty:
        print(f"[WARN] Emoji timeline empty: {out_png}")
        return
    
    # è»¢ç½®: æ™‚é–“ã‚’æ¨ªè»¸ã€çµµæ–‡å­—ã‚’ç¸¦è»¸ã«ã™ã‚‹
    df_t = df.T
    
    # å›³ã®ã‚µã‚¤ã‚ºã‚’æ¨ªé•·ã«èª¿æ•´ï¼ˆé«˜ã•ã‚’å°ã•ãã€å¹…ã‚’å¤§ããï¼‰
    fig_width = max(12, 0.4 * len(df_t.columns) + 2)  # æ™‚é–“è»¸ã®æ•°ã«å¿œã˜ã¦æ¨ªå¹…
    fig_height = max(4, 0.3 * len(df_t.index) + 1)     # çµµæ–‡å­—ã®æ•°ã«å¿œã˜ã¦é«˜ã•ï¼ˆå°ã•ã‚ï¼‰
    plt.figure(figsize=(fig_width, fig_height))
    
    vals = df_t.values.astype(float)
    im = plt.imshow(vals, aspect="auto", cmap="magma", interpolation="nearest")
    plt.title(title)
    plt.xlabel("Time", fontsize=11)
    plt.ylabel("Emoji", fontsize=11)
    
    # æ™‚é–“è»¸ï¼ˆæ¨ªè»¸ï¼‰
    plt.xticks(range(len(df_t.columns)), list(df_t.columns), rotation=45, ha="right", fontsize=9)
    
    # çµµæ–‡å­—è»¸ï¼ˆç¸¦è»¸ï¼‰- çµµæ–‡å­—ãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
    emoji_font_prop = None
    try:
        if _emoji_font_path:
            emoji_font_prop = font_manager.FontProperties(fname=_emoji_font_path)
    except Exception:
        pass
    
    if emoji_font_prop:
        plt.yticks(range(len(df_t.index)), list(df_t.index), fontsize=14, fontproperties=emoji_font_prop)
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: çµµæ–‡å­—ãƒ•ã‚©ãƒ³ãƒˆãŒãªã„å ´åˆ
        plt.yticks(range(len(df_t.index)), list(df_t.index), fontsize=14)
    
    cb = plt.colorbar(im)
    cb.set_label("Count", fontsize=10)
    plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()
    print(f"Saved emoji timeline: {out_png}")

def make_wordcloud(texts: List[str], out_png: str):
    from wordcloud import WordCloud
    os.makedirs(os.path.dirname(out_png), exist_ok=True)
    # textsãŒç©ºã€ã‚‚ã—ãã¯æ–‡å­—åˆ—ãŒãªã„å ´åˆã¯ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã—ãªã„
    if not texts:
        print(f"[WARN] Wordcloud skipped for {out_png}: no texts")
        return
    txt = " ".join(texts).strip()
    # ãƒˆãƒ¼ã‚¯ãƒ³ãŒ1å€‹æœªæº€ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
    tokens = txt.split()
    if len(tokens) == 0:
        print(f"[WARN] Wordcloud skipped for {out_png}: no tokens to plot")
        return
    # WordCloud ç”¨ãƒ•ã‚©ãƒ³ãƒˆã‚’è‡ªå‹•é¸æŠã€‚æ—¥æœ¬èªã‚’å«ã‚€å ´åˆã¯ä½¿ç”¨ä¸­ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹ã€‚
    font_path = None
    try:
        if _used_jp_font:
            from matplotlib import font_manager as _fm
            try:
                font_path = _fm.findfont(_used_jp_font, fontext="ttf")
            except Exception:
                font_path = None
    except Exception:
        font_path = None
    # WordCloudç”Ÿæˆ
    wc = WordCloud(
        width=1200,
        height=800,
        background_color="white",
        font_path=font_path if font_path else None,
        collocations=False,
    )
    try:
        wc.generate(txt)
        wc.to_file(out_png)
        print(f"Saved wordcloud: {out_png}")
    except Exception as e:
        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆã¯è­¦å‘Šã®ã¿å‡ºåŠ›
        print(f"[WARN] wordcloud failed for {out_png}: {e}")

# -------------------------
# å¼•æ•°å‡¦ç†
# -------------------------
def parse_args():
    p = argparse.ArgumentParser(description="Compare events across multiple streams for the same match.")
    # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ« or ãƒ•ã‚©ãƒ«ãƒ€ï¼‹ãƒ‘ã‚¿ãƒ¼ãƒ³
    p.add_argument("--files", nargs="+", help="åˆ†æã™ã‚‹CSVã‚’è¤‡æ•°æŒ‡å®šï¼ˆã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼‰")
    p.add_argument("--folder", type=str, help="CSVãŒå…¥ã£ã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€")
    p.add_argument("--pattern", type=str, default="*.csv", help="ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹: *.csvï¼‰")
    # æ—¢å­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    p.add_argument("--time-bins", type=int, default=100, help="æ™‚é–“åˆ†å‰²æ•°ï¼ˆå°ã•ã„ã»ã©é«˜é€Ÿã€å¤§ãã„ã»ã©ç²¾å¯†ï¼‰")
    p.add_argument("--peak-pad", type=int, default=1)
    p.add_argument("--jaccard-th", type=float, default=0.6)
    p.add_argument("--word-match-th", type=float, default=0.05, help="ãƒˆãƒ”ãƒƒã‚¯å˜èªä¸€è‡´åº¦ã®é–¾å€¤ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒãƒ³ã‚°ä½¿ç”¨æ™‚ã¯è£œåŠ©çš„ï¼‰")
    p.add_argument("--time-match-th", type=int, default=15, help="æ™‚é–“å·®ã®è¨±å®¹ç¯„å›²ï¼ˆbinsæ•°ã€å¤§ãã„ã»ã©å¤šããƒãƒƒãƒï¼‰")
    # cross-lingual embedding similarity threshold for event matching (RECOMMENDED for multilingual streams)
    p.add_argument("--embedding-match-th", type=float, default=0.70,
                   help="åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦é–¾å€¤ï¼ˆæ¨å¥¨ï¼š0.70-0.75ã€ç²¾åº¦é‡è¦–ã€å¤šè¨€èªé…ä¿¡ã§ã¯å¿…é ˆã€Noneã§ç„¡åŠ¹åŒ–ï¼‰")
    p.add_argument("--n-events", type=int, default=5)
    p.add_argument("--topk", type=int, default=10, help="æ™‚ç³»åˆ—æç”»ã®ä¸Šä½ã‚°ãƒ«ãƒ¼ãƒ—æ•°")
    p.add_argument("--save-json", action="store_true")
    # å¯è¦–åŒ–ã®åˆ¶é™: é¡ä¼¼åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã«è¡¨ç¤ºã™ã‚‹ä¸€è‡´ã‚¤ãƒ™ãƒ³ãƒˆã®ä¸Šä½ä»¶æ•°ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆé‡ã®å¤šã„é †ï¼‰
    p.add_argument("--top-matched", type=int, default=5,
                   help="matched_event_presence.png ã«è¡¨ç¤ºã™ã‚‹ä¸Šä½ã‚¤ãƒ™ãƒ³ãƒˆæ•°ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆç·æ•°ã®å¤šã„é †ã€0ã§åˆ¶é™ãªã—ï¼‰")
    # ä¸Šä½ã‚°ãƒ«ãƒ¼ãƒ—ã«é™å®šã™ã‚‹æ•°ï¼ˆä¾‹: 10â†’ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãŒå¤šã„ä¸Šä½10ã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿ï¼‰
    p.add_argument("--focus-top", type=int, default=10, help="ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’è¡Œã†å¯¾è±¡ã‚°ãƒ«ãƒ¼ãƒ—æ•°ï¼ˆNoneã®å ´åˆã¯å…¨ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰")
    # Emoji timeline å¯è¦–åŒ–è¨­å®š
    p.add_argument("--emoji-topk", type=int, default=10, help="å„é…ä¿¡ã®çµµæ–‡å­—ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«è¡¨ç¤ºã™ã‚‹ä¸Šä½çµµæ–‡å­—æ•°")
    args = p.parse_args()

    # --folder/--pattern ã‚’ --files ã«å±•é–‹
    if (not args.files) and args.folder:
        from pathlib import Path
        files = sorted(str(p) for p in Path(args.folder).glob(args.pattern))
        if not files:
            raise SystemExit(f"No files matched: {args.folder}/{args.pattern}")
        args.files = files
    if not args.files:
        p.error("either --files or --folder must be provided")
    # ãƒ•ãƒ«ãƒ‘ã‚¹åŒ–
    args.files = [os.path.abspath(f) for f in args.files]
    return args

# -------------------------
# ãƒ¡ã‚¤ãƒ³
# -------------------------
def main():
    args = parse_args()
    embedding_model = SentenceTransformer(EMB_NAME)

    # ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†
    streams: Dict[str, StreamData] = {}
    for csv_file in args.files:
        if not os.path.exists(csv_file):
            print(f"File not found: {csv_file}"); continue
        sd = process_stream(csv_file, embedding_model, args.jaccard_th, args.time_bins, topk_plot=args.topk)
        if sd: streams[csv_file] = sd
    if len(streams) < 2:
        print("Need at least two valid streams to compare events."); return

    # å„ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡ºï¼ˆã‚³ãƒ¡ãƒ³ãƒˆãŒå¤šã„ä¸Šä½ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å„ªå…ˆï¼‰
    events_by_stream: Dict[str, List[Dict[str, object]]] = {}
    for key, sd in streams.items():
        events_by_stream[key] = detect_events(sd, n_events=args.n_events, focus_top=args.focus_top)

    # å„ã‚¤ãƒ™ãƒ³ãƒˆã«ã‚³ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨ç‹¬è‡ªN-gramãƒˆãƒ”ãƒƒã‚¯ã‚’ä»˜ä¸ã™ã‚‹
    # ã¾ãšã¯ã‚¤ãƒ™ãƒ³ãƒˆã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã€å¹³å‡åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—ï¼ˆnormalize_embeddings=Trueã§ã‚ã‚‹ãŸã‚å¹³å‡å¾Œã‚‚å˜ä½é•·ã«å†æ­£è¦åŒ–ï¼‰
    for stream_key, evts in events_by_stream.items():
        for evt in evts:
            try:
                comments, _langs = extract_event_comments(streams[stream_key], evt, args.peak_pad)
                if comments:
                    # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæ—¢å­˜ã®å‡¦ç†ï¼‰
                    vecs = embedding_model.encode(comments, batch_size=32, show_progress_bar=False, normalize_embeddings=True)
                    # 2D array (n_comments x dim)
                    # å¹³å‡ã—ãŸå¾Œã€å†æ­£è¦åŒ–
                    mean_vec = np.mean(vecs, axis=0)
                    norm = np.linalg.norm(mean_vec) + 1e-12
                    mean_vec = mean_vec / norm

                    # ã€æ–°æ©Ÿèƒ½ã€‘ç‹¬è‡ªN-gramæŠ½å‡ºã§ãƒˆãƒ”ãƒƒã‚¯èªã‚’å–å¾—
                    # BERTopicã§ã¯ãªãã€TfidfVectorizerã§ç›´æ¥N-gramãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡º
                    # Phase 1.6: å‹•çš„top_kèª¿æ•´ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆæ•°ã«å¿œã˜ã¦é©å¿œçš„ã«è¨­å®šï¼‰
                    dynamic_top_k = max(5, min(30, len(comments) // 2))  # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®1/2ã€æœ€å°5ã€æœ€å¤§30
                    ngram_topics = extract_ngram_topics_direct(comments, top_k=dynamic_top_k)
                    evt["topics"] = ngram_topics  # N-gramãƒˆãƒ”ãƒƒã‚¯ã‚’ä¿å­˜

                    print(f"  [Event] {os.path.basename(stream_key)} event: {len(comments)} comments, {len(ngram_topics)} topics")
                else:
                    # ã‚³ãƒ¡ãƒ³ãƒˆãŒãªã„å ´åˆã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
                    dim = embedding_model.get_sentence_embedding_dimension()
                    mean_vec = np.zeros(dim, dtype=float)
                    evt["topics"] = []
                evt["embedding"] = mean_vec
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯embeddingã¨topicsã‚’None/ç©ºã«
                print(f"  [ERROR] Failed to process event: {e}")
                evt["embedding"] = None
                evt["topics"] = []
    # å…±é€šã‚¤ãƒ™ãƒ³ãƒˆç…§åˆ
    event_map = match_events_across_streams(
        events_by_stream,
        args.word_match_th,
        args.time_match_th,
        embed_th=args.embedding_match_th,
    )
    if not event_map:
        print("ä¸€è‡´ã™ã‚‹å…±é€šã‚¤ãƒ™ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚é–¾å€¤ï¼ˆ--time-match-th, --word-match-th, --jaccard-thï¼‰ã‚’èª¿æ•´ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    # å…±é€šã‚¤ãƒ™ãƒ³ãƒˆIDã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°
    events_by_id: Dict[int, Dict[str, object]] = defaultdict(lambda: {"streams": {}})
    for stream_key, evts in events_by_stream.items():
        for i, evt in enumerate(evts):
            eid = event_map[(stream_key, i)]
            events_by_id[eid]["label"] = evt["label"]
            events_by_id[eid]["bin_id"] = evt["bin_id"]
            events_by_id[eid]["streams"][stream_key] = evt

    # å„å…±é€šã‚¤ãƒ™ãƒ³ãƒˆã®ã‚³ãƒ¡ãƒ³ãƒˆåé›† & è·é›¢è¨ˆç®—
    results = []
    raw_data = {}
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”¨ï¼šã‚¤ãƒ™ãƒ³ãƒˆÃ—é…ä¿¡è€… è¡Œåˆ—
    stream_names = [os.path.basename(k) for k in streams.keys()]
    event_presence = []
    for eid, info in events_by_id.items():
        # åŒã˜ã‚¤ãƒ™ãƒ³ãƒˆã«å«ã¾ã‚Œã‚‹é…ä¿¡è€…ãŒ1ã¤ã—ã‹ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        # ã€ŒåŒã˜æ™‚é–“å¸¯ãƒ»åŒã˜ãƒˆãƒ”ãƒƒã‚¯ã§ç››ã‚Šä¸ŠãŒã£ãŸã€ã‚‚ã®ã®ã¿æ¯”è¼ƒå¯¾è±¡ã¨ã™ã‚‹ãŸã‚
        if len(info["streams"]) < 2:
            continue
        label = info["label"]; bin_id = int(info["bin_id"])
        # ã‚³ãƒ¡ãƒ³ãƒˆåé›†
        streams_comments: Dict[str, List[str]] = {}
        streams_langs: Dict[str, List[str]] = {}
        row_presence = {}
        for stream_key in streams.keys():
            if stream_key in info["streams"]:
                comments, langs = extract_event_comments(streams[stream_key], info["streams"][stream_key], args.peak_pad)
                streams_comments[stream_key] = comments
                streams_langs[stream_key] = langs
                row_presence[os.path.basename(stream_key)] = 1
            else:
                streams_comments[stream_key] = []
                streams_langs[stream_key] = []
                row_presence[os.path.basename(stream_key)] = 0

        # è·é›¢è¡Œåˆ—ï¼ˆèªå½™ãƒ»è¨€èªãƒ»çµµæ–‡å­—ã®ã‚¹ã‚¿ã‚¤ãƒ«å·®ï¼‰
        keys = list(streams_comments.keys())
        n = len(keys)
        dmat = np.zeros((n, n), dtype=float)
        lang_mat = np.zeros((n, n), dtype=float)
        emoji_mat = np.zeros((n, n), dtype=float)
        for i in range(n):
            for j in range(n):
                if i == j:
                    dmat[i, j] = 0.0
                    lang_mat[i, j] = 0.0
                    emoji_mat[i, j] = 0.0
                elif i < j:
                    # èªå½™å·®
                    d = compute_lexical_distance(streams_comments[keys[i]], streams_comments[keys[j]])
                    dmat[i, j] = d; dmat[j, i] = d
                    # è¨€èªåˆ†å¸ƒå·®
                    lang_dist_a = compute_language_distribution(streams_langs[keys[i]])
                    lang_dist_b = compute_language_distribution(streams_langs[keys[j]])
                    ld = js_distance_distribution(lang_dist_a, lang_dist_b)
                    lang_mat[i, j] = ld; lang_mat[j, i] = ld
                    # çµµæ–‡å­—æ¯”ç‡å·®
                    er_a = compute_emoji_ratio(streams_comments[keys[i]])
                    er_b = compute_emoji_ratio(streams_comments[keys[j]])
                    emoji_diff = abs(er_a - er_b)
                    emoji_mat[i, j] = emoji_diff; emoji_mat[j, i] = emoji_diff

        # å¹³å‡è·é›¢ãªã©ã‚’çµæœã«
        tril = dmat[np.tril_indices(n, k=-1)]
        avg_dist = float(np.mean(tril)) if tril.size else 0.0
        tril_lang = lang_mat[np.tril_indices(n, k=-1)]
        avg_lang_dist = float(np.mean(tril_lang)) if tril_lang.size else 0.0
        tril_emoji = emoji_mat[np.tril_indices(n, k=-1)]
        avg_emoji_diff = float(np.mean(tril_emoji)) if tril_emoji.size else 0.0
        result = {
            "event_id": int(eid),
            "label": label,
            "peak_bin": int(bin_id),
            "avg_js_distance": avg_dist,
            "avg_language_distance": avg_lang_dist,
            "avg_emoji_difference": avg_emoji_diff,
        }
        # å„ãƒšã‚¢ã”ã¨ã®å€¤ã‚’è¿½åŠ 
        # äº‹å‰ã«å„é…ä¿¡ã®æ„Ÿæƒ…ãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ç‰¹å¾´ã‚’è¨ˆç®—
        sentiments = {k: compute_sentiment_metrics(streams_comments[k]) for k in keys}
        styles = {k: compute_style_profile(streams_comments[k]) for k in keys}
        for i in range(n):
            for j in range(i+1, n):
                name = f"{os.path.basename(keys[i])} vs {os.path.basename(keys[j])}"
                result[f"{name} (lex)"] = float(dmat[i, j])
                result[f"{name} (lang)"] = float(lang_mat[i, j])
                result[f"{name} (emoji)"] = float(emoji_mat[i, j])
        results.append(result)

        # JSONç”¨
        if args.save_json:
            raw_data[eid] = {os.path.basename(k): v for k, v in streams_comments.items()}

        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰å‡ºåŠ›ï¼ˆæ©Ÿèƒ½ãƒ•ãƒ©ã‚°ã§åˆ¶å¾¡ï¼‰
        if ENABLE_WORDCLOUDS:
            evt_dir = os.path.join(OUT_DIR, "wordclouds", f"event_{eid}")
            for stream_key, texts in streams_comments.items():
                out_wc = os.path.join(evt_dir, f"WC_{os.path.basename(stream_key).replace('.csv','')}.png")
                try:
                    make_wordcloud(texts, out_wc)
                except Exception as e:
                    print(f"[WARN] wordcloud failed for {stream_key}: {e}")

        # ã‚¤ãƒ™ãƒ³ãƒˆÃ—é…ä¿¡è€… presence è¡Œ
        event_presence.append({"event_id": int(eid), **row_presence})

    # === å‡ºåŠ›: ã‚¤ãƒ™ãƒ³ãƒˆæ¯”è¼ƒçµæœ ===
    # ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã«ç©ºã®çµæœã‚’é˜²ã
    if results:
        results_df = pd.DataFrame(results).sort_values(["avg_js_distance", "event_id"])
    else:
        results_df = pd.DataFrame(results)
    out_csv = os.path.join(OUT_DIR, "event_comparison_results.csv")
    results_df.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"Saved: {out_csv}")

    # å…±é€šã‚¤ãƒ™ãƒ³ãƒˆ presence ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    if event_presence:
        # [DISABLED] event_eventmap.png - ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æœ›ã«ã‚ˆã‚Šç„¡åŠ¹åŒ–
        # eventmap_df = pd.DataFrame(event_presence)
        # # event_idåˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®š
        # if "event_id" in eventmap_df.columns:
        #     eventmap_df = eventmap_df.set_index("event_id").reindex(sorted(eventmap_df["event_id"].unique()))
        # hm_csv = os.path.join(OUT_DIR, "event_eventmap.csv")
        # hm_png = os.path.join(OUT_DIR, "event_eventmap.png")
        # save_csv_and_png_heatmap(eventmap_df, hm_csv, hm_png, title="Shared Events Presence (1=matched)")
        print("[INFO] Skipping event_eventmap.png generation (disabled per user request)")
    else:
        print("[INFO] No matched events to create event presence heatmap.")

    # ãƒšã‚¢è·é›¢ã®å¹³å‡ â†’ å¹³å‡è·é›¢è¡Œåˆ—ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆã”ã¨å¹³å‡ã—ã¦ã‚‚ã‚ˆã„ãŒã€ã“ã“ã¯å…¨ã‚¤ãƒ™ãƒ³ãƒˆå¹³å‡ï¼‰
    # å…¨ã‚¤ãƒ™ãƒ³ãƒˆã®è·é›¢ã‚’åˆç®—ã—ã¦å¹³å‡
    # æ§‹ç¯‰: ã‚¹ãƒˆãƒªãƒ¼ãƒ é † keys ã«åˆã‚ã›ã‚‹
    stream_keys = list(streams.keys())
    n = len(stream_keys)
    acc = np.zeros((n, n), dtype=float); cnt = np.zeros((n, n), dtype=int)
    for eid, info in events_by_id.items():
        # åŒã˜ã‚¤ãƒ™ãƒ³ãƒˆã«å«ã¾ã‚Œã‚‹é…ä¿¡è€…ãŒ2ã¤æœªæº€ãªã‚‰ã‚°ãƒ­ãƒ¼ãƒãƒ«è·é›¢ã«ã¯åŠ ç®—ã—ãªã„
        if len(info["streams"]) < 2:
            continue
        # å„ã‚¤ãƒ™ãƒ³ãƒˆã§è·é›¢ã‚’è¨ˆç®—
        comments: Dict[str, List[str]] = {}
        for k in stream_keys:
            if k in info["streams"]:
                cmt, _langs = extract_event_comments(streams[k], info["streams"][k], args.peak_pad)
                comments[k] = cmt
            else:
                comments[k] = []
        for i in range(n):
            for j in range(i+1, n):
                d = compute_lexical_distance(comments[stream_keys[i]], comments[stream_keys[j]])
                acc[i, j] += d; acc[j, i] += d
                cnt[i, j] += 1; cnt[j, i] += 1
    # [DISABLED] event_comparison_results.png - ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æœ›ã«ã‚ˆã‚Šç„¡åŠ¹åŒ–
    # with np.errstate(divide='ignore', invalid='ignore'):
    #     avg = np.where(cnt>0, acc/np.maximum(cnt,1), 0.0)
    # names = [os.path.basename(k) for k in stream_keys]
    # dist_df = pd.DataFrame(avg, index=names, columns=names)
    # dist_csv = os.path.join(OUT_DIR, "event_comparison_distance_matrix.csv")
    # dist_png = os.path.join(OUT_DIR, "event_comparison_results.png")
    # save_png_distance_matrix(dist_df, dist_csv, dist_png, title="Average JS Distance across Shared Events")
    print("[INFO] Skipping event_comparison_results.png generation (disabled per user request)")

    # JSONä¿å­˜
    if args.save_json:
        with open(os.path.join(OUT_DIR, "event_comments.json"), "w", encoding="utf-8") as f:
            json.dump(raw_data, f, ensure_ascii=False, indent=2)
    print(f"Saved: {os.path.join(OUT_DIR, 'event_comments.json')}")

    print("All done")

    # === ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ä¸€æ™‚çš„ã«ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé¡ä¼¼ãƒˆãƒ”ãƒƒã‚¯æ©Ÿèƒ½ã‚’ä½¿ç”¨ï¼‰ ===
    print("[INFO] Skipping exact match section - using similar topics for matched_event_presence.png")
    matched_pair_rows_all: List[Dict[str, object]] = []
    matched_results = []
    matched_presence = []
    matched_details_all: List[Dict[str, object]] = []
    matched_comments: Dict[str, Dict[str, List[str]]] = {}
    
    # å®Œå…¨ä¸€è‡´ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ç¾åœ¨ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™
    # ï¼ˆé¡ä¼¼ãƒˆãƒ”ãƒƒã‚¯æ©Ÿèƒ½ã§ matched_event_presence.png ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ï¼‰
    
    if False:  # ä»¥ä¸‹ã®å®Œå…¨ä¸€è‡´ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
        pass  # ãƒ€ãƒŸãƒ¼æ–‡ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰
    
    # å®Œå…¨ä¸€è‡´ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ®‹ã‚Šã®å¤‰æ•°å®šç¾©
    matched_meta = []
    pair_rows = []
    gid = -1
    bin_id = -1
    
    # ã“ã“ã‹ã‚‰å…ƒã®å‡¦ç†ï¼ˆå®Ÿéš›ã¯å®Ÿè¡Œã•ã‚Œãªã„ï¼‰
    if False:
        events_by_group_bin: Dict[Tuple[int, int], Dict[str, Dict[str, object]]] = defaultdict(dict)
        for stream_key, evts in events_by_stream.items():
            for evt in evts:
                key = (int(evt.get("group_id", -1)), int(evt.get("bin_id", -1)))
                events_by_group_bin[key][stream_key] = evt
        for (gid, bin_id), evts_dict in events_by_group_bin.items():
            # 2ã¤ä»¥ä¸Šã®é…ä¿¡è€…ãŒè©²å½“ã™ã‚‹å ´åˆã®ã¿æ¯”è¼ƒ
            if len(evts_dict) < 2:
                continue
        # ã‚°ãƒ«ãƒ¼ãƒ—ãƒ©ãƒ™ãƒ«ã¯ä»£è¡¨ã®ã‚‚ã®ã‚’æ¡ç”¨
        label = list(evts_dict.values())[0].get("label", f"group_{gid}")
        # ã‚³ãƒ¡ãƒ³ãƒˆãƒ»è¨€èªã‚’åé›†
        comments_by_stream: Dict[str, List[str]] = {}
        langs_by_stream: Dict[str, List[str]] = {}
        presence_row = {"group_id": gid, "bin_id": bin_id, "label": label}
        for stream_key in streams.keys():
            if stream_key in evts_dict:
                comments, langs = extract_event_comments(streams[stream_key], evts_dict[stream_key], args.peak_pad)
                comments_by_stream[stream_key] = comments
                langs_by_stream[stream_key] = langs
                presence_row[os.path.basename(stream_key)] = 1
            else:
                comments_by_stream[stream_key] = []
                langs_by_stream[stream_key] = []
                presence_row[os.path.basename(stream_key)] = 0
        # è·é›¢è¡Œåˆ—ã‚’è¨ˆç®—
        keys = list(streams.keys())
        n = len(keys)
        dmat = np.zeros((n, n), dtype=float)
        lang_mat = np.zeros((n, n), dtype=float)
        emoji_mat = np.zeros((n, n), dtype=float)
        for i in range(n):
            for j in range(i+1, n):
                d = compute_lexical_distance(comments_by_stream[keys[i]], comments_by_stream[keys[j]])
                dmat[i, j] = d; dmat[j, i] = d
                ld = js_distance_distribution(
                    compute_language_distribution(langs_by_stream[keys[i]]),
                    compute_language_distribution(langs_by_stream[keys[j]])
                )
                lang_mat[i, j] = ld; lang_mat[j, i] = ld
                ediff = abs(
                    compute_emoji_ratio(comments_by_stream[keys[i]]) - compute_emoji_ratio(comments_by_stream[keys[j]])
                )
                emoji_mat[i, j] = ediff; emoji_mat[j, i] = ediff
        # æ„Ÿæƒ…ãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ç‰¹å¾´ï¼ˆãƒšã‚¢è·é›¢ç®—å‡ºç”¨ï¼‰
        sentiments = {k: compute_sentiment_metrics(comments_by_stream.get(k, [])) for k in keys}
        styles = {k: compute_style_profile(comments_by_stream.get(k, [])) for k in keys}
        # === è¿½åŠ : ã“ã®ä¸€è‡´ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆåŒä¸€groupÃ—binï¼‰ã®"æ™‚é–“å¸¯"ã¨"å„é…ä¿¡ã®ãƒ©ãƒ™ãƒ«(ä¸Šä½èª)"ã€"é¡ä¼¼åº¦(1-JSè·é›¢)"ã‚’æ³¨é‡ˆã¨ã—ã¦ä½œæˆ ===
        # å‚åŠ é…ä¿¡ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆãŒå­˜åœ¨ã™ã‚‹ï¼presence=1ï¼‰ã®æŠ½å‡º
        present_streams_basename: List[str] = []
        present_streams_keys: List[str] = []
        for sk in streams.keys():
            if sk in evts_dict:
                present_streams_keys.append(sk)
                present_streams_basename.append(os.path.basename(sk))
        # æ™‚é–“å¸¯ï¼ˆçµ¶å¯¾æ™‚åˆ»ã®ä¸­å¿ƒ: ns since epochï¼‰ã®ä»£è¡¨å€¤ï¼ˆä¸­å¤®å€¤ï¼‰ã‚’ç®—å‡º
        center_ns: List[int] = []
        stream_label_map: Dict[str, str] = {}
        stream_words_map: Dict[str, str] = {}
        for sk in present_streams_keys:
            evt_info = evts_dict[sk]
            stream_obj = streams[sk]
            bins = build_relative_time_bins(stream_obj.df_valid["timestamp"], stream_obj.nr_bins)
            b_local = int(evt_info.get("bin_id", -1))
            if 0 <= b_local < len(bins):
                interval = bins[b_local]
                center_ts = interval.left + (interval.right - interval.left)/2
                try:
                    center_ns.append(int(pd.Timestamp(center_ts).value))
                except Exception:
                    pass
            # å„é…ä¿¡ã®ãƒ©ãƒ™ãƒ«ï¼ˆçŸ­ç¸®ç‰ˆ: ä¸Šä½èª 2ã€œ3 èª, ãªã‘ã‚Œã°å…ƒãƒ©ãƒ™ãƒ«ã‚’2èªã«ï¼‰
            gid_local = int(evt_info.get("group_id", -1))
            top_words_local = stream_obj.group_top_words.get(gid_local, [])[:3]
            if top_words_local:
                short = ",".join(top_words_local)
            else:
                raw_label = str(evt_info.get("label", ""))
                toks = [normalize_term(w) for w in re.split(r"[\sãƒ»,ï¼Œã€‚!ï¼?ï¼Ÿ]+", raw_label) if w]
                toks = [t for t in toks if len(t) > 1][:2]
                short = ",".join(toks) if toks else "topic"
            stream_label_map[os.path.basename(sk)] = short
            stream_words_map[os.path.basename(sk)] = short
        time_label = ""
        if center_ns:
            cen = pd.to_datetime(int(np.median(center_ns)))
            try:
                time_label = pd.Timestamp(cen).strftime("%H:%M")
            except Exception:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ†ä¸¸ã‚
                minutes = int(round((pd.Timestamp(cen).value/1e9) / 60.0))
                time_label = f"{minutes:02d}:00"
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºã®èªå½™é¡ä¼¼åº¦(1-JSè·é›¢)ã‚’è¨ˆç®—ã—ã€æœ€è‰¯ãƒšã‚¢ã‚’æŠ½å‡º
        pair_sims: List[Tuple[str, str, float]] = []
        key_index = {k: i for i, k in enumerate(keys)}
        for i in range(len(present_streams_keys)):
            for j in range(i+1, len(present_streams_keys)):
                ki = key_index[present_streams_keys[i]]
                kj = key_index[present_streams_keys[j]]
                sim = float(max(0.0, min(1.0, 1.0 - dmat[ki, kj])))
                pair_sims.append((os.path.basename(present_streams_keys[i]), os.path.basename(present_streams_keys[j]), sim))
        best_pair_str = ""
        if pair_sims:
            # é¡ä¼¼åº¦ã®é«˜ã„é †ã«
            pair_sims.sort(key=lambda x: x[2], reverse=True)
            a, b, s = pair_sims[0]
            # ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
            la = stream_label_map.get(a, "")
            lb = stream_label_map.get(b, "")
            best_pair_str = f"{a}:{la} vs {b}:{lb}, sim={s:.2f}"
            # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºé¡ä¼¼åº¦ï¼ˆ1âˆ’JSï¼‰ã‚’åˆ—ã«å±•é–‹ï¼ˆç°¡æ½”ãªåˆ—å: "A vs B")
            sim_row: Dict[str, object] = {"group_id": gid, "bin_id": bin_id}
            for i in range(n):
                for j in range(i+1, n):
                    base_i = os.path.basename(keys[i]).replace('.csv','')
                    base_j = os.path.basename(keys[j]).replace('.csv','')
                    name = f"{base_i} vs {base_j}"
                    sim_val = float(max(0.0, min(1.0, 1.0 - dmat[i, j])))
                    sim_row[name] = sim_val
            # ã“ã®ã‚¤ãƒ™ãƒ³ãƒˆã®å‚åŠ è€…ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆç·æ•°ï¼ˆãƒšã‚¢è¡Œã®å„ªå…ˆåº¦ç”¨ï¼‰
            participants_here = int(sum(1 for sk in streams.keys() if sk in evts_dict))
            total_comments_here = int(sum(len(comments_by_stream.get(sk, [])) for sk in streams.keys()))
            # å¹³å‡ã‚’è¨ˆç®—
            tril = dmat[np.tril_indices(n, k=-1)]
            avg_lex = float(np.mean(tril)) if tril.size else 0.0
            tril_lang = lang_mat[np.tril_indices(n, k=-1)]
            avg_lang = float(np.mean(tril_lang)) if tril_lang.size else 0.0
            tril_emoji = emoji_mat[np.tril_indices(n, k=-1)]
            avg_emoji = float(np.mean(tril_emoji)) if tril_emoji.size else 0.0
        result: Dict[str, object] = {
            "group_id": gid,
            "bin_id": bin_id,
            "label": label,
            "avg_js_distance": avg_lex,
            "avg_language_distance": avg_lang,
            "avg_emoji_difference": avg_emoji,
        }
        # å„ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§ã®è©²å½“ã‚¤ãƒ™ãƒ³ãƒˆã® bin_id ã¨ label ã‚’è¨˜éŒ²ã™ã‚‹
        # å­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºç™½ã«ã™ã‚‹
        for sk in streams.keys():
            base = os.path.basename(sk)
            if sk in evts_dict:
                evt_info = evts_dict[sk]
                # ãã®ã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã§ã®binã‚„ãƒ©ãƒ™ãƒ«ã‚’ä¿å­˜
                result[f"{base}_bin"] = int(evt_info.get("bin_id", -1))
                result[f"{base}_label"] = str(evt_info.get("label", ""))
            else:
                result[f"{base}_bin"] = ""
                result[f"{base}_label"] = ""
        # å„ãƒšã‚¢ã®è©³ç´°ã‚’è¿½åŠ 
        # ãƒšã‚¢ã”ã¨ã®è©³ç´°ã‚’è¿½åŠ ï¼ˆlex/lang/emojiã«åŠ ãˆ sentiment/styleï¼‰
        pair_rows = []
        present_set = set(present_streams_keys)
        for i in range(n):
            for j in range(i+1, n):
                name = f"{os.path.basename(keys[i])} vs {os.path.basename(keys[j])}"
                lex = float(dmat[i, j])
                langd = float(lang_mat[i, j])
                emj = float(emoji_mat[i, j])
                result[f"{name} (lex)"] = lex
                result[f"{name} (lang)"] = langd
                result[f"{name} (emoji)"] = emj
                # sentiment/style è·é›¢ï¼ˆå¹³å‡çµ¶å¯¾å·®ï¼‰
                sdist = style_distance(sentiments[keys[i]], sentiments[keys[j]])
                tdist = style_distance(styles[keys[i]], styles[keys[j]])
                result[f"{name} (sentiment)"] = float(sdist)
                result[f"{name} (style)"] = float(tdist)
                pair_rows.append({
                    "group_id": gid,
                    "bin_id": bin_id,
                    "pair": name,
                    "lex_js": lex,
                    "lang_js": langd,
                    "emoji_diff": emj,
                    "sentiment_dist": float(sdist),
                    "style_dist": float(tdist),
                })
                # å¯è¦–åŒ–ç”¨ã®ã€Œã‚¤ãƒ™ãƒ³ãƒˆãƒšã‚¢è¡Œã€ã‚‚åŒæ™‚ã«è“„ç©
                # ã¾ãšã€ã“ã®ãƒšã‚¢ãŒä¸¡æ–¹present_setã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                if (keys[i] not in present_set) or (keys[j] not in present_set):
                    # ã©ã¡ã‚‰ã‹ãŒã“ã®æ™‚é–“å¸¯ã«ã‚¤ãƒ™ãƒ³ãƒˆã‚’æŒã£ã¦ã„ãªã„â†’ã‚¹ã‚­ãƒƒãƒ—
                    continue
                base_i_full = os.path.basename(keys[i])
                base_j_full = os.path.basename(keys[j])
                base_i_short = base_i_full.replace('.csv','')
                base_j_short = base_j_full.replace('.csv','')
                # ãƒˆãƒ”ãƒƒã‚¯å†…å®¹ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
                la = stream_label_map.get(base_i_full, "")
                lb = stream_label_map.get(base_j_full, "")
                if not la:
                    la = "æœªåˆ†é¡"
                if not lb:
                    lb = "æœªåˆ†é¡"
                sim_pair = float(max(0.0, min(1.0, 1.0 - lex)))
                # è¡Œãƒ©ãƒ™ãƒ«: æ™‚é–“, ãƒˆãƒ”ãƒƒã‚¯å†…å®¹X vs ãƒˆãƒ”ãƒƒã‚¯å†…å®¹Y, é¡ä¼¼åº¦
                pair_label_only = f"{la} vs {lb}"
                row_lbl = f"{time_label}, {pair_label_only}, sim={sim_pair:.2f}" if time_label else f"{pair_label_only}, sim={sim_pair:.2f}"
                matched_pair_rows_all.append({
                    "group_id": gid,
                    "bin_id": bin_id,
                    "time_label": time_label,
                    "A": base_i_short,
                    "A_label": la,
                    "B": base_j_short,
                    "B_label": lb,
                    "pair": pair_label_only,
                    "Similarity": sim_pair,
                    "participants": participants_here,
                    "total_comments": total_comments_here,
                    "row_label": row_lbl,
                })
        matched_results.append(result)
        # æ³¨é‡ˆæƒ…å ±ã‚’presence_rowã«è¿½åŠ ï¼ˆå¯è¦–åŒ–æ™‚ã®è¡Œãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨ï¼‰
        if time_label:
            presence_row["time_label"] = time_label
        # å„é…ä¿¡ã®ãƒ©ãƒ™ãƒ«ã‚‚è¡Œã«å«ã‚ã¦ãŠãï¼ˆCSVã«æ®‹ã™ãŸã‚ï¼‰
        for base, lab in stream_label_map.items():
            presence_row[f"{base}_label"] = lab
        if time_label and best_pair_str:
            presence_row["row_label"] = f"{time_label}, {best_pair_str}"
        else:
            # çŸ­ã„ãƒ©ãƒ™ãƒ«ï¼ˆå‚åŠ é…ä¿¡ã®ä¸Šä½èªã‹ã‚‰ä»£è¡¨ã‚’æ§‹æˆï¼‰
            short_all = []
            for v in stream_words_map.values():
                for w in str(v).split(","):
                    if w and w not in short_all:
                        short_all.append(w)
            short_join = ",".join(short_all[:3]) if short_all else str(label)
            presence_row["row_label"] = f"{time_label}, {short_join}" if time_label else short_join
        matched_presence.append(presence_row)
        # similarity è¡Œã«ã‚‚æ³¨é‡ˆã‚’ä»˜ä¸
        if time_label:
            sim_row["time_label"] = time_label
        sim_row["row_label"] = presence_row["row_label"]
        matched_similarity.append(sim_row)
        # è¡Œãƒ¡ã‚¿: å‚åŠ è€…æ•°ã¨ã‚³ãƒ¡ãƒ³ãƒˆç·æ•°
        participants = int(sum(1 for sk in streams.keys() if sk in evts_dict))
        total_comments = int(sum(len(comments_by_stream.get(sk, [])) for sk in streams.keys()))
        matched_meta.append({
            "group_id": gid,
            "bin_id": bin_id,
            "participants": participants,
            "total_comments": total_comments,
            "time_label": time_label,
            "row_label": presence_row.get("row_label", ""),
        })

        # ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹ã‚’ä¿å­˜ï¼ˆç©ºã®é…åˆ—ã‚‚å«ã‚€ï¼‰
        evt_key = f"gid{gid}_bin{bin_id}"
        matched_comments[evt_key] = {}
        for sk in streams.keys():
            # ä¿å­˜ã™ã‚‹éš›ã¯ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã‚’ã‚­ãƒ¼ã¨ã™ã‚‹
            matched_comments[evt_key][os.path.basename(sk)] = comments_by_stream.get(sk, [])

        # ã‚¤ãƒ™ãƒ³ãƒˆå…¨ä½“ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä½¿ã£ã¦ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆï¼ˆæ©Ÿèƒ½ãƒ•ãƒ©ã‚°ã§åˆ¶å¾¡ï¼‰
        if ENABLE_WORDCLOUDS:
            # å„ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’çµåˆã—ã¦1ã¤ã®ãƒªã‚¹ãƒˆã«
            aggregated_comments: List[str] = []
            for _sk, comms in comments_by_stream.items():
                aggregated_comments.extend(comms)
            # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            all_wc_dir = os.path.join(OUT_DIR, "wordclouds_matched", f"gid{gid}_bin{bin_id}")
            os.makedirs(all_wc_dir, exist_ok=True)
            wc_path = os.path.join(all_wc_dir, f"WC_ALL.png")
            try:
                make_wordcloud(aggregated_comments, wc_path)
            except Exception as e:
                print(f"[WARN] wordcloud failed for aggregated event gid{gid}_bin{bin_id}: {e}")

        # --- è©³ç´°æƒ…å ±ã‚’åé›†: å„é…ä¿¡è€…ã”ã¨ã®binç¯„å›²ã‚„ãƒ©ãƒ™ãƒ«ãªã© ---
        # event_id ã‚’gidã¨bin_idã«åŸºã¥ã„ã¦ç”Ÿæˆ
        event_identifier = f"gid{gid}_bin{bin_id}"
        for sk in streams.keys():
            base_name = os.path.basename(sk)
            if sk in evts_dict:
                evt_info = evts_dict[sk]
                stream_obj = streams[sk]
                # binå¢ƒç•Œã‚’å–å¾—
                bins = build_relative_time_bins(stream_obj.df_valid["timestamp"], stream_obj.nr_bins)
                b = int(evt_info.get("bin_id", -1))
                if 0 <= b < len(bins):
                    interval = bins[b]
                    t0 = stream_obj.df_valid["timestamp"].min()
                    # ç›¸å¯¾ç§’
                    bin_start_sec = int((interval.left - t0).total_seconds())
                    bin_end_sec = int((interval.right - t0).total_seconds())
                else:
                    bin_start_sec = None
                    bin_end_sec = None
                gid_local = int(evt_info.get("group_id", -1))
                label_local = str(evt_info.get("label", ""))
                top_words_local = stream_obj.group_top_words.get(gid_local, [])[:5]
                matched_details_all.append({
                    "event_id": event_identifier,
                    "stream": base_name,
                    "bin_id": b,
                    "bin_start_sec": bin_start_sec if bin_start_sec is not None else "",
                    "bin_end_sec": bin_end_sec if bin_end_sec is not None else "",
                    "label": label_local,
                    "top_words": " ".join(top_words_local),
                })
            else:
                # ã“ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«ã¯è©²å½“ã‚¤ãƒ™ãƒ³ãƒˆãŒå­˜åœ¨ã—ãªã„
                matched_details_all.append({
                    "event_id": event_identifier,
                    "stream": base_name,
                    "bin_id": "",
                    "bin_start_sec": "",
                    "bin_end_sec": "",
                    "label": "",
                    "top_words": "",
                })
    # ä¿å­˜
    if matched_results:
        matched_df = pd.DataFrame(matched_results)
        out_csv2 = os.path.join(OUT_DIR, "matched_event_comparison_results.csv")
        matched_df.to_csv(out_csv2, index=False, encoding="utf-8-sig")
        print(f"Saved matched events results: {out_csv2}")
        # presence CSV ã¯ä¿å­˜ï¼ˆå¾“æ¥ã® 0/1 æƒ…å ±ï¼‰
        presence_df = pd.DataFrame(matched_presence)
        # presence å´ã‚‚åŒã˜ä¸Šä½ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã—ã¦ã€è¦‹ãŸç›®ã¨CSVã®æ•´åˆã‚’å–ã‚‹
        try:
            top_n = int(getattr(args, "top_matched", 5) or 0)
        except Exception:
            top_n = 5
        if top_n and not presence_df.empty and 'group_id' in presence_df.columns and 'bin_id' in presence_df.columns:
            meta_df2 = pd.DataFrame(matched_meta)
            order_keys = (meta_df2.sort_values(["total_comments","participants"], ascending=[False, False])
                                   [["group_id","bin_id"]].apply(tuple, axis=1).tolist()[:top_n])
            keep_idx = set(order_keys)
            presence_df = presence_df[presence_df.apply(lambda r: (r.get("group_id"), r.get("bin_id")) in keep_idx, axis=1)]
        # ä¿å­˜ç”¨CSVã¯å…ƒã®å½¢ã§å‡ºåŠ›
        pres_csv = os.path.join(OUT_DIR, "matched_event_presence.csv")
        presence_df.to_csv(pres_csv, index=False, encoding="utf-8-sig")
        # å¯è¦–åŒ–ã¯ã€Œã‚¤ãƒ™ãƒ³ãƒˆãƒšã‚¢è¡Œ Ã— 1åˆ—(Similarity)ã€ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆ1âˆ’JSï¼‰ã‚’ä½¿ç”¨
        pair_all_df = pd.DataFrame(matched_pair_rows_all)
        if pair_all_df is None or pair_all_df.empty:
            print("[WARN] No pair rows to plot for matched events")
        else:
            try:
                top_n = int(getattr(args, "top_matched", 5) or 0)
            except Exception:
                top_n = 5
            pair_plot_df = pair_all_df.copy()
            # ä¸Šä½é¸åˆ¥: ã‚³ãƒ¡ãƒ³ãƒˆç·æ•°â†’Similarity ã®é †ã§ä¸¦ã¹æ›¿ãˆ
            pair_plot_df["total_comments"] = pd.to_numeric(pair_plot_df.get("total_comments"), errors="coerce").fillna(0).astype(int)
            pair_plot_df["Similarity"] = pd.to_numeric(pair_plot_df.get("Similarity"), errors="coerce").fillna(0.0).astype(float)
            pair_plot_df = pair_plot_df.sort_values(["total_comments","Similarity"], ascending=[False, False])
            if top_n:
                pair_plot_df = pair_plot_df.head(top_n)
            # [DISABLED] matched_event_presence.png (location 1) - ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æœ›ã«ã‚ˆã‚Šç„¡åŠ¹åŒ–
            # # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¡Œãƒ©ãƒ™ãƒ«ã«
            # if "row_label" in pair_plot_df.columns:
            #     pair_plot_df = pair_plot_df.set_index("row_label")
            # else:
            #     pair_plot_df = pair_plot_df.set_index(pair_plot_df.apply(lambda r: f"{r.get('time_label','')}, {r.get('pair','')}".strip(', '), axis=1))
            # # ãƒ—ãƒ­ãƒƒãƒˆç”¨CSVã¨PNGã®å‡ºåŠ›å…ˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã¯äº’æ›ã®ãŸã‚æ®ãˆç½®ãï¼‰
            # pres_csv_plot = os.path.join(OUT_DIR, "matched_event_presence_plot.csv")
            # pres_png = os.path.join(OUT_DIR, "matched_event_presence.png")
            # save_csv_and_png_heatmap(
            #     pair_plot_df[["Similarity"]],
            #     pres_csv_plot,
            #     pres_png,
            #     title="Topic Pair Similarity for Matched Events (1âˆ’JS)"
            # )
            print("[INFO] Skipping matched_event_presence.png generation (disabled per user request)")
        # ä¸€è‡´ã‚¤ãƒ™ãƒ³ãƒˆã®ã‚µãƒãƒªãƒ¼ï¼ˆè¦‹ã‚„ã™ã„ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰ã‚‚ä¿å­˜
        try:
            summary_png = os.path.join(OUT_DIR, "matched_event_summary.png")
            _save_matched_summary_table(presence_df, matched_df, summary_png, top_k=20)
            print(f"Saved matched events summary: {summary_png}")
        except Exception as e:
            print(f"[WARN] failed to save matched events summary: {e}")
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæŒ‡æ¨™ã‚’CSV/PNGã§ä¿å­˜
        try:
            if pair_rows:
                pair_df = pd.DataFrame(pair_rows)
                pair_dir = os.path.join(OUT_DIR, "matched_event_pairs")
                os.makedirs(pair_dir, exist_ok=True)
                pair_csv = os.path.join(pair_dir, f"gid{gid}_bin{bin_id}_pairs.csv")
                pair_df.to_csv(pair_csv, index=False, encoding="utf-8-sig")
                # ç°¡æ˜“ãƒãƒ¼å›³: å„è·é›¢ã®å¹³å‡
                agg = pair_df[["lex_js","lang_js","emoji_diff","sentiment_dist","style_dist"]].mean()
                plt.figure(figsize=(6,4))
                plt.bar(agg.index, agg.values)
                plt.xticks(rotation=45, ha="right")
                plt.title(f"gid{gid}_bin{bin_id} pairwise metrics (avg)")
                plt.tight_layout()
                plt.savefig(os.path.join(pair_dir, f"gid{gid}_bin{bin_id}_pairs.png"), dpi=200)
                plt.close()
        except Exception as e:
            print(f"[WARN] failed to save pairwise metrics: {e}")

        # JSONã«ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹ã‚’ä¿å­˜
        comments_json_path = os.path.join(OUT_DIR, "matched_event_comments.json")
        try:
            with open(comments_json_path, "w", encoding="utf-8") as f:
                json.dump(matched_comments, f, ensure_ascii=False, indent=2)
            print(f"Saved matched event comments: {comments_json_path}")
        except Exception as e:
            print(f"[WARN] failed to save matched event comments: {e}")
        # è©³ç´°CSVã‚‚ä¿å­˜
        if matched_details_all:
            details_df = pd.DataFrame(matched_details_all)
            details_csv = os.path.join(OUT_DIR, "matched_event_details.csv")
            details_df.to_csv(details_csv, index=False, encoding="utf-8-sig")
            print(f"Saved matched event details: {details_csv}")
            # PNGã¨ã—ã¦ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¿å­˜
            details_png = os.path.join(OUT_DIR, "matched_event_details.png")
            try:
                save_df_as_table_png(details_df, details_png, title="Matched Event Details")
                print(f"Saved matched event details PNG: {details_png}")
            except Exception as e:
                print(f"[WARN] Failed to save matched event details PNG: {e}")
    else:
        print("[INFO] No matched events on exact group & bin across streams.")

    # === æ–°æ©Ÿèƒ½: é¡ä¼¼ãƒˆãƒ”ãƒƒã‚¯ Ã— æ™‚é–“ãŒè¿‘ã„ã‚¤ãƒ™ãƒ³ãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ é–“ã§ç…§åˆã—æ¯”è¼ƒ ===
    # `match_events_across_streams` ã‚’åˆ©ç”¨ã—ã¦ã€åŸ‹ã‚è¾¼ã¿é¡ä¼¼åº¦ã¨æ™‚é–“å·®ã«åŸºã¥ãã‚¤ãƒ™ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    print("Matching events across streams by topic similarity and time ...")
    print(f"[DEBUG] Matching parameters: word_match_th={args.word_match_th}, time_match_th={args.time_match_th}, embedding_match_th={args.embedding_match_th}")
    print(f"[DEBUG] Total events: {sum(len(evts) for evts in events_by_stream.values())}")
    similar_event_map = match_events_across_streams(events_by_stream, args.word_match_th, args.time_match_th, args.embedding_match_th)
    print(f"[DEBUG] Similar event map created with {len(set(similar_event_map.values()))} unique groups")
    similar_results = []
    similar_presence = []
    similar_comments: Dict[int, Dict[str, List[str]]] = {}
    # é¡ä¼¼ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°æƒ…å ±ã®ä¿å­˜ç”¨ãƒªã‚¹ãƒˆ
    similar_details_all: List[Dict[str, object]] = []
    # Build mapping from event_id to list of (stream_key, event)
    events_by_sim_id: Dict[int, Dict[str, Dict[str, object]]] = defaultdict(dict)
    for stream_key, evts in events_by_stream.items():
        for i, evt in enumerate(evts):
            eid = similar_event_map.get((stream_key, i))
            if eid is None:
                continue
            events_by_sim_id[eid][stream_key] = evt
    for sim_id, evts_dict in events_by_sim_id.items():
        # skip if less than two streams participate
        if len(evts_dict) < 2:
            continue
        # Determine representative label (concatenate top words across streams)
        # Use first stream's label as base
        labels = []
        for evt in evts_dict.values():
            if evt.get("label"):
                labels.append(evt["label"])
        label = labels[0] if labels else f"event_{sim_id}"
        # Collect comments and languages per stream
        comments_by_stream: Dict[str, List[str]] = {}
        langs_by_stream: Dict[str, List[str]] = {}
        presence_row: Dict[str, int] = {}
        for sk in streams.keys():
            if sk in evts_dict:
                comments, langs = extract_event_comments(streams[sk], evts_dict[sk], args.peak_pad)
                comments_by_stream[sk] = comments
                langs_by_stream[sk] = langs
                presence_row[os.path.basename(sk)] = 1
            else:
                comments_by_stream[sk] = []
                langs_by_stream[sk] = []
                presence_row[os.path.basename(sk)] = 0
        # Compute distance matrices
        keys = list(streams.keys())
        n = len(keys)
        dmat = np.zeros((n, n), dtype=float)
        lang_mat = np.zeros((n, n), dtype=float)
        emoji_mat = np.zeros((n, n), dtype=float)
        for i in range(n):
            for j in range(i+1, n):
                d = compute_lexical_distance(comments_by_stream[keys[i]], comments_by_stream[keys[j]])
                dmat[i, j] = d; dmat[j, i] = d
                ld = js_distance_distribution(
                    compute_language_distribution(langs_by_stream[keys[i]]),
                    compute_language_distribution(langs_by_stream[keys[j]])
                )
                lang_mat[i, j] = ld; lang_mat[j, i] = ld
                ediff = abs(
                    compute_emoji_ratio(comments_by_stream[keys[i]]) - compute_emoji_ratio(comments_by_stream[keys[j]])
                )
                emoji_mat[i, j] = ediff; emoji_mat[j, i] = ediff
        # Compute averages
        tril = dmat[np.tril_indices(n, k=-1)]
        avg_lex = float(np.mean(tril)) if tril.size else 0.0
        tril_lang = lang_mat[np.tril_indices(n, k=-1)]
        avg_lang = float(np.mean(tril_lang)) if tril_lang.size else 0.0
        tril_emoji = emoji_mat[np.tril_indices(n, k=-1)]
        avg_emoji = float(np.mean(tril_emoji)) if tril_emoji.size else 0.0
        result = {
            "sim_event_id": sim_id,
            "label": label,
            "avg_js_distance": avg_lex,
            "avg_language_distance": avg_lang,
            "avg_emoji_difference": avg_emoji,
        }
        for i in range(n):
            for j in range(i+1, n):
                name = f"{os.path.basename(keys[i])} vs {os.path.basename(keys[j])}"
                result[f"{name} (lex)"] = float(dmat[i, j])
                result[f"{name} (lang)"] = float(lang_mat[i, j])
                result[f"{name} (emoji)"] = float(emoji_mat[i, j])
        similar_results.append(result)
        similar_presence.append({"sim_event_id": sim_id, "label": label, **presence_row})
        # Save comments
        similar_comments[sim_id] = {os.path.basename(sk): comments_by_stream[sk] for sk in streams.keys()}
        # Generate aggregated wordcloud for this similar eventï¼ˆæ©Ÿèƒ½ãƒ•ãƒ©ã‚°ã§åˆ¶å¾¡ï¼‰
        if ENABLE_WORDCLOUDS:
            aggregated_comments: List[str] = []
            for comms in comments_by_stream.values():
                aggregated_comments.extend(comms)
            all_wc_dir = os.path.join(OUT_DIR, "wordclouds_similar", f"event_{sim_id}")
            os.makedirs(all_wc_dir, exist_ok=True)
            wc_path = os.path.join(all_wc_dir, f"WC_ALL.png")
            try:
                make_wordcloud(aggregated_comments, wc_path)
            except Exception as e:
                print(f"[WARN] wordcloud failed for similar event {sim_id}: {e}")
            # Also per-stream wordclouds
            for sk, comms in comments_by_stream.items():
                wc_stream_path = os.path.join(all_wc_dir, f"WC_{os.path.basename(sk).replace('.csv','')}.png")
                try:
                    make_wordcloud(comms, wc_stream_path)
                except Exception as e:
                    print(f"[WARN] wordcloud failed for similar event {sim_id}, stream {sk}: {e}")

        # --- è©³ç´°æƒ…å ±ã‚’åé›†: å„é…ä¿¡è€…ã”ã¨ã®binç¯„å›²ã‚„ãƒ©ãƒ™ãƒ«ãªã© ---
        for sk in streams.keys():
            base_name = os.path.basename(sk)
            if sk in evts_dict:
                evt_info = evts_dict[sk]
                stream_obj = streams[sk]
                bins = build_relative_time_bins(stream_obj.df_valid["timestamp"], stream_obj.nr_bins)
                b = int(evt_info.get("bin_id", -1))
                if 0 <= b < len(bins):
                    interval = bins[b]
                    t0 = stream_obj.df_valid["timestamp"].min()
                    bin_start_sec = int((interval.left - t0).total_seconds())
                    bin_end_sec = int((interval.right - t0).total_seconds())
                else:
                    bin_start_sec = None
                    bin_end_sec = None
                gid_local = int(evt_info.get("group_id", -1))
                label_local = str(evt_info.get("label", ""))
                top_words_local = stream_obj.group_top_words.get(gid_local, [])[:5]
                similar_details_all.append({
                    "sim_event_id": sim_id,
                    "stream": base_name,
                    "bin_id": b,
                    "bin_start_sec": bin_start_sec if bin_start_sec is not None else "",
                    "bin_end_sec": bin_end_sec if bin_end_sec is not None else "",
                    "label": label_local,
                    "top_words": " ".join(top_words_local),
                })
            else:
                similar_details_all.append({
                    "sim_event_id": sim_id,
                    "stream": base_name,
                    "bin_id": "",
                    "bin_start_sec": "",
                    "bin_end_sec": "",
                    "label": "",
                    "top_words": "",
                })
    # Save similar event results
    if similar_results:
        similar_df = pd.DataFrame(similar_results)
        csv_path = os.path.join(OUT_DIR, "similar_event_comparison_results.csv")
        similar_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        print(f"Saved similar events results: {csv_path}")
        # [DISABLED] similar_event_presence.png - ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æœ›ã«ã‚ˆã‚Šç„¡åŠ¹åŒ–
        # Save presence CSV only (no heatmap/plot)
        # pres_df = pd.DataFrame(similar_presence)
        # pres_csv = os.path.join(OUT_DIR, "similar_event_presence.csv")
        # pres_df.to_csv(pres_csv, index=False, encoding="utf-8-sig")
        # # For heatmap, set index to label
        # pres_df_plot = pres_df.set_index("label")
        # pres_df_plot = pres_df_plot.drop(columns=["sim_event_id", "label"], errors="ignore")
        # pres_csv_plot = os.path.join(OUT_DIR, "similar_event_presence_plot.csv")
        # pres_png = os.path.join(OUT_DIR, "similar_event_presence.png")
        # save_csv_and_png_heatmap(pres_df_plot, pres_csv_plot, pres_png, title="Similar Events Presence (1=present)")
        print("[INFO] Skipping similar_event_presence.png generation (disabled per user request)")
        # Save comments JSON
        comments_json = os.path.join(OUT_DIR, "similar_event_comments.json")
        with open(comments_json, "w", encoding="utf-8") as f:
            json.dump(similar_comments, f, ensure_ascii=False, indent=2)
        print(f"Saved similar event comments: {comments_json}")
        # é¡ä¼¼ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°CSVã‚’ä¿å­˜
    if similar_details_all:
            sim_details_df = pd.DataFrame(similar_details_all)
            sim_details_csv = os.path.join(OUT_DIR, "similar_event_details.csv")
            sim_details_df.to_csv(sim_details_csv, index=False, encoding="utf-8-sig")
            print(f"Saved similar event details: {sim_details_csv}")
            # PNGã¨ã—ã¦ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¿å­˜
            sim_details_png = os.path.join(OUT_DIR, "similar_event_details.png")
            try:
                save_df_as_table_png(sim_details_df, sim_details_png, title="Similar Event Details")
                print(f"Saved similar event details PNG: {sim_details_png}")
            except Exception as e:
                print(f"[WARN] Failed to save similar event details PNG: {e}")
    
    # === å€‹åˆ¥ã‚¤ãƒ™ãƒ³ãƒˆã®é…ä¿¡è€…é–“æ¯”è¼ƒã‚°ãƒ©ãƒ•ç”Ÿæˆï¼ˆå­¦ä¼šç™ºè¡¨ç”¨ï¼‰ ===
    print("\n[INFO] Generating individual event broadcaster comparison graphs...")
    bc_comparison_dir = os.path.join(OUT_DIR, "broadcaster_comparisons")
    os.makedirs(bc_comparison_dir, exist_ok=True)
    
    # ä¸Šä½ã‚¤ãƒ™ãƒ³ãƒˆã‚’é¸æŠï¼ˆå‚åŠ é…ä¿¡è€…æ•°ãŒå¤šãã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚‚å¤šã„ã‚‚ã®ï¼‰
    event_priority = []
    for sim_id, evts_dict in events_by_sim_id.items():
        if len(evts_dict) < 2:  # 2é…ä¿¡è€…ä»¥ä¸Š
            continue
        total_comments = sum(len(extract_event_comments(streams[sk], evt, args.peak_pad)[0]) 
                           for sk, evt in evts_dict.items())
        event_priority.append((sim_id, len(evts_dict), total_comments))
    
    # é…ä¿¡è€…æ•°ã§ã‚½ãƒ¼ãƒˆã€æ¬¡ã«ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã§ã‚½ãƒ¼ãƒˆ
    event_priority.sort(key=lambda x: (x[1], x[2]), reverse=True)
    
    # ä¸Šä½10ã‚¤ãƒ™ãƒ³ãƒˆã¾ãŸã¯å…¨ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆå°‘ãªã„æ–¹ï¼‰
    top_events_to_visualize = min(10, len(event_priority))
    
    for rank, (sim_id, num_broadcasters, total_comments) in enumerate(event_priority[:top_events_to_visualize], 1):
        try:
            evts_dict = events_by_sim_id[sim_id]
            
            # è·é›¢ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆCSVã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿ï¼‰
            distance_results = {}
            try:
                csv_path = os.path.join(OUT_DIR, "similar_event_comparison_results.csv")
                sim_results_df = pd.read_csv(csv_path, encoding="utf-8-sig")
                
                for row_idx, row in sim_results_df.iterrows():
                    if row["sim_event_id"] == sim_id:
                        for col in sim_results_df.columns:
                            if " vs " in col and ("(lex)" in col or "(lang)" in col or "(emoji)" in col):
                                distance_results[col] = row[col]
                        break
            except Exception as e:
                print(f"[WARN] Could not load distance data for Event {sim_id}: {e}")
                distance_results = {}
            
            out_png = os.path.join(bc_comparison_dir, f"event_{sim_id}_comparison.png")
            generate_event_broadcaster_comparison(
                sim_id, evts_dict, streams, args.peak_pad, distance_results, out_png
            )
            
            if rank <= 3:  # ãƒˆãƒƒãƒ—3ã ã‘ãƒ­ã‚°å‡ºåŠ›
                print(f"  Rank {rank}: Event {sim_id} ({num_broadcasters} broadcasters, {total_comments} comments)")
        
        except Exception as e:
            print(f"[WARN] Failed to generate comparison for Event {sim_id}: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"[INFO] Generated {top_events_to_visualize} broadcaster comparison graphs")
    
    # === è¿½åŠ : matched_event_presence.png ã‚’é¡ä¼¼ãƒˆãƒ”ãƒƒã‚¯ãƒ™ãƒ¼ã‚¹ã§ç”Ÿæˆ ===
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æœ›: ä¼¼ãŸå†…å®¹ã®ãƒˆãƒ”ãƒƒã‚¯åŒå£«ã‚’æ™‚é–“å¸¯ã”ã¨ã«æ¯”è¼ƒ
    if similar_results:
        print("[INFO] Generating matched_event_presence.png from similar topics...")
        matched_pair_rows_similar: List[Dict[str, object]] = []
        
        # events_by_sim_idã‚’å†åº¦ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒˆï¼ˆã™ã§ã«æ§‹ç¯‰æ¸ˆã¿ï¼‰
        for sim_id, evts_dict in events_by_sim_id.items():
            if len(evts_dict) < 2:
                continue
            
            # æ™‚é–“å¸¯ã‚’è¨ˆç®—ï¼ˆå„é…ä¿¡è€…ã®ã‚¤ãƒ™ãƒ³ãƒˆã®ä¸­å¤®æ™‚åˆ»ã®ä¸­å¤®å€¤ï¼‰
            center_ns_list: List[int] = []
            stream_label_map_sim: Dict[str, str] = {}
            
            for sk, evt_info in evts_dict.items():
                stream_obj = streams[sk]
                bins = build_relative_time_bins(stream_obj.df_valid["timestamp"], stream_obj.nr_bins)
                b_local = int(evt_info.get("bin_id", -1))
                if 0 <= b_local < len(bins):
                    interval = bins[b_local]
                    center_ts = interval.left + (interval.right - interval.left)/2
                    try:
                        center_ns_list.append(int(pd.Timestamp(center_ts).value))
                    except Exception:
                        pass
                
                # ãƒˆãƒ”ãƒƒã‚¯ãƒ©ãƒ™ãƒ«ï¼ˆçŸ­ç¸®ç‰ˆï¼‰
                gid_local = int(evt_info.get("group_id", -1))
                top_words_local = stream_obj.group_top_words.get(gid_local, [])[:3]
                if top_words_local:
                    short = ",".join(top_words_local)
                else:
                    raw_label = str(evt_info.get("label", ""))
                    toks = [normalize_term(w) for w in re.split(r"[\sãƒ»,ï¼Œã€‚!ï¼?ï¼Ÿ]+", raw_label) if w]
                    toks = [t for t in toks if len(t) > 1][:2]
                    short = ",".join(toks) if toks else "topic"
                stream_label_map_sim[os.path.basename(sk)] = short
            
            # æ™‚é–“ãƒ©ãƒ™ãƒ«
            time_label_sim = ""
            if center_ns_list:
                cen = pd.to_datetime(int(np.median(center_ns_list)))
                try:
                    time_label_sim = pd.Timestamp(cen).strftime("%H:%M")
                except Exception:
                    minutes = int(round((pd.Timestamp(cen).value/1e9) / 60.0))
                    time_label_sim = f"{minutes:02d}:00"
            
            # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºé¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰
            present_keys = list(evts_dict.keys())
            n_present = len(present_keys)
            
            # å„ãƒšã‚¢ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã¦é¡ä¼¼åº¦ã‚’è¨ˆç®—
            for i in range(n_present):
                for j in range(i+1, n_present):
                    sk_i = present_keys[i]
                    sk_j = present_keys[j]
                    
                    # ã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡º
                    comments_i, _ = extract_event_comments(streams[sk_i], evts_dict[sk_i], args.peak_pad)
                    comments_j, _ = extract_event_comments(streams[sk_j], evts_dict[sk_j], args.peak_pad)
                    
                    # JSè·é›¢ã‚’è¨ˆç®—
                    lex_dist = compute_lexical_distance(comments_i, comments_j)
                    sim_val = float(max(0.0, min(1.0, 1.0 - lex_dist)))
                    
                    # ãƒ©ãƒ™ãƒ«
                    base_i = os.path.basename(sk_i)
                    base_j = os.path.basename(sk_j)
                    la = stream_label_map_sim.get(base_i, "æœªåˆ†é¡")
                    lb = stream_label_map_sim.get(base_j, "æœªåˆ†é¡")
                    
                    pair_label = f"{la} vs {lb}"
                    row_lbl = f"{time_label_sim}, {pair_label}, sim={sim_val:.2f}" if time_label_sim else f"{pair_label}, sim={sim_val:.2f}"
                    
                    matched_pair_rows_similar.append({
                        "sim_event_id": sim_id,
                        "time_label": time_label_sim,
                        "A": base_i.replace('.csv',''),
                        "A_label": la,
                        "B": base_j.replace('.csv',''),
                        "B_label": lb,
                        "pair": pair_label,
                        "Similarity": sim_val,
                        "total_comments": len(comments_i) + len(comments_j),
                        "row_label": row_lbl,
                    })
        
        # matched_event_presence.png ã‚’ç”Ÿæˆ
        if matched_pair_rows_similar:
            pair_df_sim = pd.DataFrame(matched_pair_rows_similar)
            try:
                top_n = int(getattr(args, "top_matched", 5) or 5)
            except Exception:
                top_n = 5
            
            # ã‚½ãƒ¼ãƒˆã—ã¦ãƒˆãƒƒãƒ—N
            pair_df_sim["total_comments"] = pd.to_numeric(pair_df_sim.get("total_comments"), errors="coerce").fillna(0).astype(int)
            pair_df_sim["Similarity"] = pd.to_numeric(pair_df_sim.get("Similarity"), errors="coerce").fillna(0.0).astype(float)
            pair_df_sim = pair_df_sim.sort_values(["total_comments","Similarity"], ascending=[False, False])
            if top_n:
                pair_df_sim = pair_df_sim.head(top_n)
            
            # [DISABLED] matched_event_presence.png (location 2) - ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æœ›ã«ã‚ˆã‚Šç„¡åŠ¹åŒ–
            # # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¡Œãƒ©ãƒ™ãƒ«ã«
            # if "row_label" in pair_df_sim.columns:
            #     pair_df_sim = pair_df_sim.set_index("row_label")
            # 
            # # matched_event_presence ã¨ã—ã¦ä¿å­˜ï¼ˆä¸Šæ›¸ãï¼‰
            # matched_pres_csv = os.path.join(OUT_DIR, "matched_event_presence_plot.csv")
            # matched_pres_png = os.path.join(OUT_DIR, "matched_event_presence.png")
            # save_csv_and_png_heatmap(
            #     pair_df_sim[["Similarity"]],
            #     matched_pres_csv,
            #     matched_pres_png,
            #     title="Similar Topic Pair Similarity by Time (1âˆ’JS)"
            # )
            print(f"[INFO] Skipping matched_event_presence.png generation (disabled per user request)")
        else:
            print("[WARN] No similar topic pairs found to generate matched_event_presence.png")
    else:
        print("[INFO] No similar events matched across streams under current thresholds.")

    # === Emojiãƒ©ãƒ³ã‚­ãƒ³ã‚°: å„CSVã”ã¨ã«çµµæ–‡å­—ã®å‡ºç¾é »åº¦ã‚’é›†è¨ˆã—ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¨æ£’ã‚°ãƒ©ãƒ•ã‚’å‡ºåŠ› ===
    # message_clean ã§ã¯çµµæ–‡å­—ãŒé™¤å»ã•ã‚Œã¦ã—ã¾ã†ãŸã‚ã€å…ƒã® message åˆ—ã‹ã‚‰çµµæ–‡å­—ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    try:
        emoji_dir = os.path.join(OUT_DIR, "emoji_rankings")
        os.makedirs(emoji_dir, exist_ok=True)
        rankings_rows: List[Dict[str, object]] = []
        for stream_key, sd in streams.items():
            # stream_key ã¯CSVã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚å…ƒã®ã‚³ãƒ¡ãƒ³ãƒˆå…¨ä½“ã‚’èª­ã¿å‡ºã—ã¦çµµæ–‡å­—ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹
            msgs: List[str] = []
            # ã¾ãšå…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿å‡ºã™
            try:
                full_df = read_csv_any(stream_key)
                if "message" in full_df.columns:
                    msgs = full_df["message"].astype(str).tolist()
            except Exception:
                # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãŒèª­ã‚ãªã„å ´åˆã¯ã€df_valid ã® message åˆ—ã§ä»£ç”¨
                try:
                    msgs = sd.df_valid.get("message", pd.Series([], dtype=str)).astype(str).tolist()
                except Exception:
                    msgs = []
            # çµµæ–‡å­—ã®å‡ºç¾æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            emoji_counter: Dict[str, int] = defaultdict(int)
            for txt in msgs:
                if not isinstance(txt, str):
                    continue
                for ch in txt:
                    if is_emoji(ch):
                        emoji_counter[ch] += 1
            # ä¸Šä½10ä»¶ã®çµµæ–‡å­—ã¨é »åº¦ã‚’å–å¾—
            top_emojis = sorted(emoji_counter.items(), key=lambda x: x[1], reverse=True)[:10]
            # æ£’ã‚°ãƒ©ãƒ•ã‚ã‚‹ã„ã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä»˜ãå›³ã‚’ä½œæˆ
            plt.figure(figsize=(8, 4))
            if top_emojis:
                emojis, freq = zip(*top_emojis)
                plt.bar(range(len(emojis)), freq)
                # Use actual emojis as labels and supply emoji-supporting font if available
                emoji_font_prop = None
                try:
                    if _emoji_font_path:
                        emoji_font_prop = font_manager.FontProperties(fname=_emoji_font_path)
                except Exception:
                    emoji_font_prop = None
                plt.xticks(range(len(emojis)), list(emojis), fontsize=14, rotation=0, fontproperties=emoji_font_prop)
                plt.xlabel("Emoji")
                plt.ylabel("Frequency")
                plt.title(f"Top Emojis in {os.path.basename(stream_key)}")
            else:
                # 1ã¤ã‚‚çµµæ–‡å­—ãŒãªã‹ã£ãŸå ´åˆã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                plt.text(0.5, 0.5, "No emojis found", ha='center', va='center', fontsize=14)
                plt.axis('off')
                plt.title(f"Top Emojis in {os.path.basename(stream_key)}")
            plt.tight_layout()
            chart_fname = f"emoji_ranking_{os.path.basename(stream_key).replace('.csv','')}.png"
            chart_path = os.path.join(emoji_dir, chart_fname)
            plt.savefig(chart_path, dpi=200)
            plt.close()
            # Print saved chart and build ranking row
            print(f"Saved emoji ranking chart: {chart_path}")
            rank_data: Dict[str, object] = {"stream": os.path.basename(stream_key)}
            for idx, (emo, cnt) in enumerate(top_emojis, start=1):
                rank_data[f"emoji_{idx}"] = emo
                rank_data[f"freq_{idx}"] = cnt
            rankings_rows.append(rank_data)
        # After iterating all streams, save rankings CSV
        if rankings_rows:
            emoji_csv = os.path.join(emoji_dir, "emoji_rankings.csv")
            pd.DataFrame(rankings_rows).to_csv(emoji_csv, index=False, encoding="utf-8-sig")
            print(f"Saved emoji rankings CSV: {emoji_csv}")
    except Exception as e:
        print(f"[WARN] Failed to compute emoji rankings: {e}")

    # === Emoji ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³: å„é…ä¿¡ã”ã¨ã«æ™‚é–“å¸¯Ã—çµµæ–‡å­—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’å‡ºåŠ› ===
    try:
        topk_emoji = int(getattr(args, "emoji_topk", 10) or 10)
        et_dir = os.path.join(OUT_DIR, "emoji_timelines")
        os.makedirs(et_dir, exist_ok=True)
        for stream_key, sd in streams.items():
            # å…ƒã® message ã¨ timestamp ã‚’èª­ã¿è¾¼ã¿
            try:
                df_full = read_csv_any(stream_key)
            except Exception:
                df_full = sd.df_valid.copy()
            if "timestamp" not in df_full.columns or "message" not in df_full.columns:
                continue
            ts = pd.to_datetime(df_full["timestamp"], errors="coerce", utc=True).dt.tz_localize(None)
            msgs = df_full["message"].astype(str).tolist()
            # æ™‚é–“binï¼ˆçµµæ–‡å­—ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ç”¨ï¼šè¦‹ã‚„ã™ã•ã®ãŸã‚10åˆ†ç¨‹åº¦ã®ç²’åº¦ã«å›ºå®šï¼‰
            # è©¦åˆæ™‚é–“ï¼ˆ90-120åˆ†ï¼‰ã‚’æƒ³å®šã—ã¦ã€10-12ãƒ“ãƒ³ç¨‹åº¦ã«è¨­å®š
            emoji_timeline_bins = 12  # ç´„10åˆ†ã”ã¨ï¼ˆ120åˆ†Ã·12=10åˆ†ï¼‰
            bins = build_relative_time_bins(ts.dropna(), emoji_timeline_bins)
            # å„binÃ—emojiã®ã‚«ã‚¦ãƒ³ãƒˆ
            # ã¾ãšå…¨emojiã®ç·æ•°ã§ä¸Šä½ topk ã‚’é¸ã¶
            total_emoji_counter: Dict[str, int] = defaultdict(int)
            for txt in msgs:
                for ch in str(txt):
                    if is_emoji(ch):
                        total_emoji_counter[ch] += 1
            top_emojis = [e for e, _ in sorted(total_emoji_counter.items(), key=lambda x: x[1], reverse=True)[:topk_emoji]]
            if not top_emojis:
                continue
            # binå‰²å½“ã¨ã‚«ã‚¦ãƒ³ãƒˆ
            # DataFrameã«ã¾ã¨ã‚ã¦é«˜é€ŸåŒ–
            df_tmp = pd.DataFrame({"timestamp": ts, "message": msgs}).dropna(subset=["timestamp"]).reset_index(drop=True)
            # å„è¡Œã‹ã‚‰å€™è£œã®çµµæ–‡å­—ã®ã¿æŠ½å‡º
            def _extract_emojis(s: str) -> List[str]:
                return [ch for ch in str(s) if ch in top_emojis and is_emoji(ch)]
            df_tmp["emojis"] = df_tmp["message"].apply(_extract_emojis)
            if df_tmp["emojis"].map(len).sum() == 0:
                continue
            # å„è¡Œã®bin id ã‚’æ±ºå®š
            def _find_bin(t: pd.Timestamp) -> int:
                for i, iv in enumerate(bins):
                    if t >= iv.left and t < iv.right:
                        return i
                centers = np.array([iv.left.value for iv in bins], dtype=np.int64)
                return int(np.argmin(np.abs(centers - int(t.value))))
            df_tmp["bin_id"] = df_tmp["timestamp"].apply(_find_bin)
            # æ™‚é–“ãƒ©ãƒ™ãƒ«ï¼ˆHH:MM ä¸­å¤®ï¼‰ã‚’ç”¨æ„
            time_labels: Dict[int, str] = {}
            for i, iv in enumerate(bins):
                center_ts = iv.left + (iv.right - iv.left)/2
                try:
                    time_labels[i] = pd.Timestamp(center_ts).strftime("%H:%M")
                except Exception:
                    time_labels[i] = str(i)
            # é›†è¨ˆ: binÃ—emoji
            rows = []
            for _, r in df_tmp.iterrows():
                if not r["emojis"]:
                    continue
                b = int(r["bin_id"])
                for ch in r["emojis"]:
                    rows.append({"bin_id": b, "emoji": ch, "cnt": 1})
            if not rows:
                continue
            edf = pd.DataFrame(rows)
            pivot = edf.pivot_table(index="bin_id", columns="emoji", values="cnt", aggfunc="sum", fill_value=0)
            pivot = pivot.reindex(range(len(bins)), fill_value=0)
            pivot.index = [time_labels.get(i, str(i)) for i in pivot.index]
            # å‡ºåŠ›
            base = os.path.basename(stream_key).replace('.csv','')
            out_csv_t = os.path.join(et_dir, f"emoji_timeline_{base}.csv")
            out_png_t = os.path.join(et_dir, f"emoji_timeline_{base}.png")
            save_emoji_timeline_heatmap(pivot, out_csv_t, out_png_t, title=f"Emoji Timeline: {base}")
    except Exception as e:
        print(f"[WARN] Failed to build emoji timelines: {e}")
    
    # ========================================
    # Event-to-Eventé¡ä¼¼åº¦åˆ†æï¼ˆã‚¤ãƒ™ãƒ³ãƒˆé–“æ¯”è¼ƒï¼‰
    # ========================================
    print("\n" + "="*60)
    print("Starting Event-to-Event Similarity Analysis")
    print("="*60)
    
    try:
        # NÃ—Né¡ä¼¼åº¦è¡Œåˆ—ã¨ãƒšã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        sim_matrix_df, event_pairs_df = generate_event_similarity_matrix(
            events_by_sim_id, streams, args.peak_pad
        )
        
        if not sim_matrix_df.empty:
            # é¡ä¼¼åº¦è¡Œåˆ—CSVãƒ»ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä¿å­˜
            event_sim_csv = os.path.join(OUT_DIR, "event_to_event_similarity_matrix.csv")
            event_sim_png = os.path.join(OUT_DIR, "event_to_event_similarity_heatmap.png")
            save_event_similarity_heatmap(sim_matrix_df, event_sim_csv, event_sim_png)
            
            # ãƒšã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ä¿å­˜
            event_pairs_csv = os.path.join(OUT_DIR, "event_to_event_pairs.csv")
            event_pairs_df.to_csv(event_pairs_csv, index=False, encoding="utf-8-sig")
            print(f"Saved event pairs: {event_pairs_csv}")
            
            # æ–°æ©Ÿèƒ½: æ™‚é–“çš„ç›¸é–¢ã¨ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®è¦–è¦šåŒ–
            try:
                visualize_temporal_correlation_and_confidence(event_pairs_df, OUT_DIR)
            except Exception as e:
                print(f"[WARN] Failed to visualize temporal correlation and confidence: {e}")

            # ãƒˆãƒƒãƒ—10é¡ä¼¼ãƒšã‚¢ã‚’è¡¨ç¤º
            print("\n[Top 10 Most Similar Event Pairs]")
            print("-" * 80)
            for idx, row in event_pairs_df.head(10).iterrows():
                print(f"Event {row['event_A_id']} <-> Event {row['event_B_id']}: "
                      f"similarity={row['main_similarity']:.3f}")
                print(f"  A: {row['event_A_label'][:50]}")
                print(f"  B: {row['event_B_label'][:50]}")
                emb_val = f"{row['embedding_similarity']:.3f}" if row['embedding_similarity'] is not None else 'N/A'
                ctx_val = f"{row.get('context_penalty', 1.0):.3f}"
                temp_corr = f"{row.get('temporal_correlation', 0.0):.3f}"
                conf_score = f"{row.get('confidence_score', 0.0):.3f}"
                print(f"  Metrics: emb={emb_val}, "
                      f"topic={row['topic_jaccard']:.3f}, lex={row['lexical_similarity']:.3f}, "
                      f"context={ctx_val}")
                print(f"  æ–°æ©Ÿèƒ½: temporal_corr={temp_corr}, confidence={conf_score}")
                print()
            
            print(f"\nEvent-to-Event analysis complete: {len(sim_matrix_df)} events analyzed")
            print(f"Total event pairs: {len(event_pairs_df)}")
        else:
            print("[WARN] No multi-stream events found for Event-to-Event analysis")
    except Exception as e:
        print(f"[ERROR] Failed to perform Event-to-Event analysis: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "="*60)
    print("All processing complete!")
    print("="*60)
    
    # ========================================
    # ã€æ–°æ©Ÿèƒ½ã€‘æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
    # ========================================
    if not event_pairs_df.empty:
        print("\n" + "="*60)
        print("FINAL RESULTS SUMMARY")
        print("="*60)
        
        # åŸºæœ¬çµ±è¨ˆ
        print("\n[Basic Statistics]")
        print(f"  Total Events: {len(sim_matrix_df)}")
        print(f"  Total Pairs: {len(event_pairs_df)}")
        print(f"  Average Similarity: {event_pairs_df['main_similarity'].mean():.3f}")
        print(f"  Max Similarity: {event_pairs_df['main_similarity'].max():.3f}")
        print(f"  Min Similarity: {event_pairs_df['main_similarity'].min():.3f}")
        
        # ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ç‡ã®åˆ†æ
        print("\n[Topic Matching Analysis]")
        topic_zero = len(event_pairs_df[event_pairs_df['topic_jaccard'] == 0])
        topic_nonzero = len(event_pairs_df[event_pairs_df['topic_jaccard'] > 0])
        topic_high = len(event_pairs_df[event_pairs_df['topic_jaccard'] > 0.3])
        print(f"  topic_jaccard = 0: {topic_zero}/{len(event_pairs_df)} ({topic_zero/len(event_pairs_df)*100:.1f}%)")
        print(f"  topic_jaccard > 0: {topic_nonzero}/{len(event_pairs_df)} ({topic_nonzero/len(event_pairs_df)*100:.1f}%)")
        print(f"  topic_jaccard > 0.3: {topic_high}/{len(event_pairs_df)} ({topic_high/len(event_pairs_df)*100:.1f}%)")
        print(f"  Average topic_jaccard (all): {event_pairs_df['topic_jaccard'].mean():.3f}")
        if topic_nonzero > 0:
            print(f"  Average topic_jaccard (>0): {event_pairs_df[event_pairs_df['topic_jaccard'] > 0]['topic_jaccard'].mean():.3f}")
        
        # é¡ä¼¼åº¦åˆ†å¸ƒ
        print("\n[Similarity Distribution]")
        low_sim = len(event_pairs_df[event_pairs_df['main_similarity'] < 0.5])
        mid_sim = len(event_pairs_df[(event_pairs_df['main_similarity'] >= 0.5) & (event_pairs_df['main_similarity'] < 0.7)])
        high_sim = len(event_pairs_df[event_pairs_df['main_similarity'] >= 0.7])
        print(f"  Low (<0.5): {low_sim}/{len(event_pairs_df)} ({low_sim/len(event_pairs_df)*100:.1f}%)")
        print(f"  Mid (0.5-0.7): {mid_sim}/{len(event_pairs_df)} ({mid_sim/len(event_pairs_df)*100:.1f}%)")
        print(f"  High (>=0.7): {high_sim}/{len(event_pairs_df)} ({high_sim/len(event_pairs_df)*100:.1f}%)")
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒšãƒŠãƒ«ãƒ†ã‚£ã®çµ±è¨ˆ
        print("\n[Context Penalty Analysis]")
        if 'context_penalty' in event_pairs_df.columns:
            penalty_1_0 = len(event_pairs_df[event_pairs_df['context_penalty'] == 1.0])
            penalty_0_3 = len(event_pairs_df[event_pairs_df['context_penalty'] == 0.3])
            print(f"  context_penalty = 1.0: {penalty_1_0}/{len(event_pairs_df)} ({penalty_1_0/len(event_pairs_df)*100:.1f}%)")
            print(f"  context_penalty = 0.3: {penalty_0_3}/{len(event_pairs_df)} ({penalty_0_3/len(event_pairs_df)*100:.1f}%)")
        
        # æ™‚é–“çš„ç›¸é–¢ã®çµ±è¨ˆ
        print("\n[Temporal Correlation]")
        if 'temporal_correlation' in event_pairs_df.columns:
            print(f"  Average: {event_pairs_df['temporal_correlation'].mean():.3f}")
            print(f"  Median: {event_pairs_df['temporal_correlation'].median():.3f}")
            strong_corr = len(event_pairs_df[event_pairs_df['temporal_correlation'] > 0.5])
            print(f"  Strong correlation (>0.5): {strong_corr}/{len(event_pairs_df)} ({strong_corr/len(event_pairs_df)*100:.1f}%)")
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®çµ±è¨ˆ
        print("\n[Confidence Score]")
        if 'confidence_score' in event_pairs_df.columns:
            print(f"  Average: {event_pairs_df['confidence_score'].mean():.3f}")
            print(f"  Median: {event_pairs_df['confidence_score'].median():.3f}")
            high_conf = len(event_pairs_df[event_pairs_df['confidence_score'] > 0.7])
            print(f"  High confidence (>0.7): {high_conf}/{len(event_pairs_df)} ({high_conf/len(event_pairs_df)*100:.1f}%)")
        
        # N-gramæŠ½å‡ºã®åŠ¹æœ
        print("\n[N-gram Topic Extraction Impact]")
        print(f"  [OK] N-gram phrases extracted directly via TfidfVectorizer")
        print(f"  [OK] Phrases like 'Real Madrid', 'penalty kick' preserved")
        print(f"  [OK] Weight adjusted: embedding 0.7 : lexical 0.1 : topic 0.2 (Phase 3 optimal, p<0.001)")
        
        # è«–æ–‡ãƒ¬ãƒ™ãƒ«è©•ä¾¡
        print("\n[Paper Quality Assessment]")
        avg_sim = event_pairs_df['main_similarity'].mean()
        topic_nonzero_pct = topic_nonzero / len(event_pairs_df) * 100
        
        score = 0
        if avg_sim >= 0.60:
            score += 4
        elif avg_sim >= 0.50:
            score += 3
        elif avg_sim >= 0.40:
            score += 2
        else:
            score += 1
        
        if topic_nonzero_pct >= 50:
            score += 4
        elif topic_nonzero_pct >= 30:
            score += 3
        elif topic_nonzero_pct >= 20:
            score += 2
        else:
            score += 1
        
        if penalty_0_3 == 0:
            score += 2
        elif penalty_0_3 <= 3:
            score += 1
        
        print(f"  Estimated Level: {score}/10")
        if score >= 9:
            print(f"  [EXCELLENT!] Paper-ready quality achieved!")
        elif score >= 7:
            print(f"  [GOOD!] Near paper quality, minor improvements recommended")
        elif score >= 5:
            print(f"  [ACCEPTABLE] Requires improvements for publication")
        else:
            print(f"  [NEEDS WORK] Major improvements required")
        
        print("\n" + "="*60)

def generate_event_broadcaster_comparison(
    sim_id: int,
    evts_dict: Dict[str, Dict[str, object]],
    streams: Dict[str, 'StreamData'],
    peak_pad: int,
    distance_results: Dict[str, float],
    out_png: str
) -> None:
    """
    å€‹åˆ¥ã‚¤ãƒ™ãƒ³ãƒˆã®é…ä¿¡è€…é–“æ¯”è¼ƒã‚’é«˜å“è³ªã§å¯è¦–åŒ–ï¼ˆå­¦ä¼šç™ºè¡¨ç”¨ï¼‰
    
    Parameters:
    - sim_id: ã‚¤ãƒ™ãƒ³ãƒˆID
    - evts_dict: {stream_key: event_dict}
    - streams: {stream_key: StreamData}
    - peak_pad: padding
    - distance_results: {pair_name: distance_value}
    - out_png: å‡ºåŠ›PNG
    """
    # é…ä¿¡è€…æƒ…å ±ã‚’åé›†
    broadcaster_data = {}
    all_comments_timeline = []
    
    for stream_key, evt in evts_dict.items():
        base_name = os.path.basename(stream_key).replace('.csv', '')
        comments, bins = extract_event_comments(streams[stream_key], evt, peak_pad)
        
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆå‰å¾Œã®paddingå«ã‚€ï¼‰
        bin_id = int(evt.get("bin_id", -1))
        time_range = range(bin_id - peak_pad, bin_id + peak_pad + 1)
        comment_counts = []
        
        # bin_idã‚«ãƒ©ãƒ ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if "bin_id" not in streams[stream_key].df_valid.columns:
            # bin_idãŒãªã„å ´åˆã¯ã€timestampã‹ã‚‰è¨ˆç®—
            bins = build_relative_time_bins(
                streams[stream_key].df_valid["timestamp"], 
                streams[stream_key].nr_bins
            )
            for b in time_range:
                count = (bins == b).sum()
                comment_counts.append(count)
        else:
            for b in time_range:
                mask = (streams[stream_key].df_valid["bin_id"] == b)
                count = mask.sum()
                comment_counts.append(count)
        
        broadcaster_data[base_name] = {
            "label": evt.get("label", ""),
            "comments": comments,
            "comment_counts": comment_counts,
            "time_range": list(time_range),
            "bin_id": bin_id,
            "top_words": evt.get("top_words", [])[:5],
            "num_comments": len(comments)
        }
    
    # Figureä½œæˆï¼ˆ3è¡Œæ§‹æˆï¼‰
    fig = plt.figure(figsize=(16, 12), dpi=300)
    gs = fig.add_gridspec(3, 1, height_ratios=[2, 1, 1.5], hspace=0.4)
    
    # ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆ
    colors = plt.cm.Set2(np.linspace(0, 1, len(broadcaster_data)))
    
    # ============================================================
    # ä¸Šæ®µï¼šã‚³ãƒ¡ãƒ³ãƒˆæ•°æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•
    # ============================================================
    ax1 = fig.add_subplot(gs[0])
    
    for idx, (broadcaster, data) in enumerate(broadcaster_data.items()):
        ax1.plot(
            data["time_range"], 
            data["comment_counts"],
            marker='o',
            linewidth=2.5,
            markersize=6,
            label=broadcaster,
            color=colors[idx],
            alpha=0.8
        )
        
        # ãƒ”ãƒ¼ã‚¯ä½ç½®ã«ç¸¦ç·š
        ax1.axvline(
            x=data["bin_id"], 
            color=colors[idx], 
            linestyle='--', 
            alpha=0.3,
            linewidth=1.5
        )
    
    ax1.set_xlabel("Time Bin ID", fontsize=12, fontweight='bold')
    ax1.set_ylabel("Number of Comments", fontsize=12, fontweight='bold')
    ax1.set_title(
        f"Event {sim_id}: Multi-Broadcaster Comment Timeline Comparison",
        fontsize=14,
        fontweight='bold',
        pad=20
    )
    ax1.legend(loc='upper right', fontsize=10, framealpha=0.9)
    ax1.grid(True, alpha=0.3, linestyle=':', linewidth=0.8)
    ax1.tick_params(labelsize=10)
    
    # ============================================================
    # ä¸­æ®µï¼šãƒˆãƒ”ãƒƒã‚¯ãƒ©ãƒ™ãƒ«ã¨çµ±è¨ˆæƒ…å ±
    # ============================================================
    ax2 = fig.add_subplot(gs[1])
    ax2.axis('off')
    
    # è¡¨ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    table_data = []
    headers = ["Broadcaster", "Topic Label", "Top Words", "# Comments", "Peak Bin"]
    
    for broadcaster, data in broadcaster_data.items():
        label_short = data["label"][:40] + "..." if len(data["label"]) > 40 else data["label"]
        top_words_str = "ãƒ»".join([str(w) for w in data["top_words"]])
        if len(top_words_str) > 40:
            top_words_str = top_words_str[:37] + "..."
        
        table_data.append([
            broadcaster,
            label_short,
            top_words_str,
            str(data["num_comments"]),
            str(data["bin_id"])
        ])
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æç”»
    table = ax2.table(
        cellText=table_data,
        colLabels=headers,
        cellLoc='left',
        loc='center',
        colWidths=[0.15, 0.35, 0.30, 0.10, 0.10]
    )
    
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1, 2.5)
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ«
    for i in range(len(headers)):
        cell = table[(0, i)]
        cell.set_facecolor('#4472C4')
        cell.set_text_props(weight='bold', color='white')
    
    # ãƒ‡ãƒ¼ã‚¿è¡Œã®è‰²ä»˜ã‘
    for i in range(len(table_data)):
        for j in range(len(headers)):
            cell = table[(i+1, j)]
            cell.set_facecolor(colors[i] if j == 0 else '#F2F2F2')
            cell.set_alpha(0.3 if j == 0 else 0.5)
    
    ax2.set_title("Topic Information", fontsize=12, fontweight='bold', pad=10)
    
    # ============================================================
    # ä¸‹æ®µï¼šé…ä¿¡è€…é–“è·é›¢æ¯”è¼ƒ
    # ============================================================
    ax3 = fig.add_subplot(gs[2])
    ax3.axis('off')
    
    # è·é›¢ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†
    distance_table_data = []
    distance_headers = ["Broadcaster Pair", "Lexical Distance", "Language Distance", "Emoji Difference"]
    
    broadcasters = list(broadcaster_data.keys())
    for i in range(len(broadcasters)):
        for j in range(i+1, len(broadcasters)):
            pair_name = f"{broadcasters[i]} vs {broadcasters[j]}"
            
            lex_key = f"{broadcasters[i]}.csv vs {broadcasters[j]}.csv (lex)"
            lang_key = f"{broadcasters[i]}.csv vs {broadcasters[j]}.csv (lang)"
            emoji_key = f"{broadcasters[i]}.csv vs {broadcasters[j]}.csv (emoji)"
            
            lex_val = distance_results.get(lex_key, "N/A")
            lang_val = distance_results.get(lang_key, "N/A")
            emoji_val = distance_results.get(emoji_key, "N/A")
            
            lex_str = f"{lex_val:.3f}" if isinstance(lex_val, (int, float)) else str(lex_val)
            lang_str = f"{lang_val:.3f}" if isinstance(lang_val, (int, float)) else str(lang_val)
            emoji_str = f"{emoji_val:.3f}" if isinstance(emoji_val, (int, float)) else str(emoji_val)
            
            distance_table_data.append([pair_name, lex_str, lang_str, emoji_str])
    
    if distance_table_data:
        # è·é›¢ãƒ†ãƒ¼ãƒ–ãƒ«æç”»
        dist_table = ax3.table(
            cellText=distance_table_data,
            colLabels=distance_headers,
            cellLoc='center',
            loc='center',
            colWidths=[0.40, 0.20, 0.20, 0.20]
        )
        
        dist_table.auto_set_font_size(False)
        dist_table.set_fontsize(9)
        dist_table.scale(1, 2.2)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ«
        for i in range(len(distance_headers)):
            cell = dist_table[(0, i)]
            cell.set_facecolor('#70AD47')
            cell.set_text_props(weight='bold', color='white')
        
        # ãƒ‡ãƒ¼ã‚¿è¡Œã®è‰²ä»˜ã‘ï¼ˆè·é›¢ã®å¤§å°ã§è‰²åˆ†ã‘ï¼‰
        for i in range(len(distance_table_data)):
            for j in range(1, 4):  # æ•°å€¤åˆ—ã®ã¿
                cell = dist_table[(i+1, j)]
                try:
                    val = float(distance_table_data[i][j])
                    # è·é›¢ãŒå¤§ãã„ã»ã©èµ¤ã€å°ã•ã„ã»ã©ç·‘
                    if val < 0.3:
                        cell.set_facecolor('#C6EFCE')  # ç·‘ç³»
                    elif val < 0.6:
                        cell.set_facecolor('#FFEB9C')  # é»„è‰²ç³»
                    else:
                        cell.set_facecolor('#FFC7CE')  # èµ¤ç³»
                    cell.set_alpha(0.6)
                except:
                    pass
        
        ax3.set_title("Broadcaster Pair Distance Metrics", fontsize=12, fontweight='bold', pad=10)
    
    # å…¨ä½“ã®ã‚¿ã‚¤ãƒˆãƒ«
    fig.suptitle(
        f"Event {sim_id}: Cross-Broadcaster Topic Analysis",
        fontsize=16,
        fontweight='bold',
        y=0.98
    )
    
    plt.tight_layout(rect=[0, 0, 1, 0.97])
    plt.savefig(out_png, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"Saved broadcaster comparison: {out_png}")

def save_df_as_table_png(df: pd.DataFrame, out_png: str, title: str = "") -> None:
    """Save a DataFrame as a table PNG. Adjust figure size based on number of rows and columns."""
    if df is None or df.empty:
        # nothing to save
        return
    
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’çŸ­ç¸®ï¼ˆlabelã‚„top_wordsãªã©ã®é•·ã„æ–‡å­—åˆ—ï¼‰
    df_display = df.copy()
    for col in df_display.columns:
        if df_display[col].dtype == 'object':  # æ–‡å­—åˆ—åˆ—ã®ã¿
            df_display[col] = df_display[col].apply(
                lambda x: (str(x).split('(')[0][:30] + '...' 
                          if isinstance(x, str) and len(str(x)) > 30 
                          else str(x))
            )
    
    # Determine figure size heuristically
    n_rows, n_cols = df_display.shape
    # ã‚ˆã‚Šå¤§ããªã‚µã‚¤ã‚ºã§ã€é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾å¿œ
    width = min(30, 3 + 2.5 * n_cols)  # åˆ—ã‚ãŸã‚Šã‚ˆã‚Šåºƒã
    height = min(30, 2 + 0.4 * n_rows)  # è¡Œã‚ãŸã‚Šã‚ˆã‚Šé«˜ã
    
    fig, ax = plt.subplots(figsize=(width, height))
    ax.axis('off')
    
    # build table
    # Convert values to strings to avoid potential formatting issues
    cell_text = [[str(x) for x in row] for row in df_display.values]
    table = ax.table(cellText=cell_text, colLabels=df_display.columns, loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(7)  # å°ã•ã‚ã«
    table.scale(1, 2.0)  # ç¸¦æ–¹å‘ã«å°‘ã—ä¼¸ã°ã™
    
    # ã‚»ãƒ«ã®å¹…ã‚’è‡ªå‹•èª¿æ•´
    for key, cell in table.get_celld().items():
        cell.set_text_props(wrap=True)
        cell.PAD = 0.05
    
    # Optionally set title
    if title:
        plt.title(title, fontsize=14, pad=20)
    
    plt.tight_layout()
    # Save with higher DPI for clarity
    plt.savefig(out_png, dpi=150, bbox_inches='tight')
    plt.close(fig)


def visualize_temporal_correlation_and_confidence(pairs_df, output_dir):
    """
    æ–°æ©Ÿèƒ½ã®è¦–è¦šåŒ–ï¼šæ™‚é–“çš„ç›¸é–¢ã¨ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®åˆ†æ
    
    Parameters:
    - pairs_df: event_to_event_pairs.csv ã®DataFrame
    - output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
    plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    
    # 1. æ™‚é–“çš„ç›¸é–¢ vs ãƒ¡ã‚¤ãƒ³é¡ä¼¼åº¦ã®æ•£å¸ƒå›³
    ax1 = axes[0, 0]
    scatter1 = ax1.scatter(pairs_df['temporal_correlation'], 
                           pairs_df['main_similarity'],
                           c=pairs_df['confidence_score'],
                           cmap='viridis',
                           s=100,
                           alpha=0.6,
                           edgecolors='black')
    ax1.set_xlabel('æ™‚é–“çš„ç›¸é–¢ (Temporal Correlation)', fontsize=11)
    ax1.set_ylabel('ãƒ¡ã‚¤ãƒ³é¡ä¼¼åº¦ (Main Similarity)', fontsize=11)
    ax1.set_title('æ™‚é–“çš„ç›¸é–¢ vs ãƒ¡ã‚¤ãƒ³é¡ä¼¼åº¦\nï¼ˆè‰²=ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼‰', fontsize=12)
    ax1.grid(True, alpha=0.3)
    cbar1 = plt.colorbar(scatter1, ax=ax1)
    cbar1.set_label('ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢', fontsize=10)
    
    # 2. ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    ax2 = axes[0, 1]
    ax2.hist(pairs_df['confidence_score'], bins=15, color='steelblue', alpha=0.7, edgecolor='black')
    ax2.axvline(pairs_df['confidence_score'].mean(), color='red', linestyle='--', 
                label=f'å¹³å‡: {pairs_df["confidence_score"].mean():.3f}')
    ax2.set_xlabel('ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ (Confidence Score)', fontsize=11)
    ax2.set_ylabel('é »åº¦', fontsize=11)
    ax2.set_title('ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ', fontsize=12)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. æ™‚é–“çš„ç›¸é–¢ã®åˆ†å¸ƒï¼ˆç›¸é–¢ãŒæœ‰æ„ãªãƒšã‚¢ã®ã¿ï¼‰
    ax3 = axes[1, 0]
    significant_corr = pairs_df[pairs_df['temporal_correlation'] > 0.3]
    if len(significant_corr) > 0:
        ax3.hist(significant_corr['temporal_correlation'], bins=15, 
                color='coral', alpha=0.7, edgecolor='black')
        ax3.set_xlabel('æ™‚é–“çš„ç›¸é–¢ (r > 0.3)', fontsize=11)
        ax3.set_ylabel('é »åº¦', fontsize=11)
        ax3.set_title(f'æœ‰æ„ãªæ™‚é–“çš„ç›¸é–¢ã®åˆ†å¸ƒ\nï¼ˆ{len(significant_corr)}/{len(pairs_df)} ãƒšã‚¢ï¼‰', fontsize=12)
        ax3.grid(True, alpha=0.3)
    else:
        ax3.text(0.5, 0.5, 'æœ‰æ„ãªæ™‚é–“çš„ç›¸é–¢ãªã—\n(r > 0.3)', 
                ha='center', va='center', fontsize=14, transform=ax3.transAxes)
    
    # 4. è¤‡æ•°æŒ‡æ¨™ã®ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼‰
    ax4 = axes[1, 1]
    metrics = ['main_similarity', 'topic_jaccard', 'lexical_similarity', 
               'temporal_correlation', 'confidence_score']
    available_metrics = [m for m in metrics if m in pairs_df.columns]
    
    if len(available_metrics) >= 2:
        corr_matrix = pairs_df[available_metrics].corr()
        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
                   center=0, ax=ax4, cbar_kws={'label': 'ç›¸é–¢ä¿‚æ•°'})
        ax4.set_title('æŒ‡æ¨™é–“ã®ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹', fontsize=12)
        
        # ãƒ©ãƒ™ãƒ«ã‚’æ—¥æœ¬èªã«å¤‰æ›
        label_map = {
            'main_similarity': 'ãƒ¡ã‚¤ãƒ³é¡ä¼¼åº¦',
            'topic_jaccard': 'ãƒˆãƒ”ãƒƒã‚¯Jaccard',
            'lexical_similarity': 'èªå½™é¡ä¼¼åº¦',
            'temporal_correlation': 'æ™‚é–“çš„ç›¸é–¢',
            'confidence_score': 'ä¿¡é ¼åº¦'
        }
        ax4.set_xticklabels([label_map.get(m, m) for m in available_metrics], 
                           rotation=45, ha='right', fontsize=9)
        ax4.set_yticklabels([label_map.get(m, m) for m in available_metrics], 
                           rotation=0, fontsize=9)
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, 'temporal_correlation_and_confidence_analysis.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close(fig)
    print(f"[INFO] æ™‚é–“çš„ç›¸é–¢ãƒ»ä¿¡é ¼åº¦åˆ†æå›³ã‚’ä¿å­˜: {output_path}")
    
    # çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    print("\n=== æ–°æ©Ÿèƒ½ã®çµ±è¨ˆã‚µãƒãƒªãƒ¼ ===")
    print(f"æ™‚é–“çš„ç›¸é–¢ (Temporal Correlation):")
    print(f"  - å¹³å‡: {pairs_df['temporal_correlation'].mean():.3f}")
    print(f"  - ä¸­å¤®å€¤: {pairs_df['temporal_correlation'].median():.3f}")
    print(f"  - æœ‰æ„ãªç›¸é–¢ (r>0.3): {len(significant_corr)}/{len(pairs_df)} ãƒšã‚¢ ({len(significant_corr)/len(pairs_df)*100:.1f}%)")
    
    print(f"\nä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ (Confidence Score):")
    print(f"  - å¹³å‡: {pairs_df['confidence_score'].mean():.3f}")
    print(f"  - ä¸­å¤®å€¤: {pairs_df['confidence_score'].median():.3f}")
    print(f"  - é«˜ä¿¡é ¼åº¦ (>0.7): {len(pairs_df[pairs_df['confidence_score'] > 0.7])}/{len(pairs_df)} ãƒšã‚¢")
    print(f"  - ä¸­ä¿¡é ¼åº¦ (0.5-0.7): {len(pairs_df[(pairs_df['confidence_score'] >= 0.5) & (pairs_df['confidence_score'] <= 0.7)])}/{len(pairs_df)} ãƒšã‚¢")
    print(f"  - ä½ä¿¡é ¼åº¦ (<0.5): {len(pairs_df[pairs_df['confidence_score'] < 0.5])}/{len(pairs_df)} ãƒšã‚¢")


if __name__ == "__main__":
    main()
