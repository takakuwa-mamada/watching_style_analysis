"""
è¨€èªåˆ¥æ¯”è¼ƒã®ç²¾ç·»åŒ– - ã‚³ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã§ã®è¨€èªæ¤œå‡º
Refined Language-based Comparison with Comment-level Detection

ç›®çš„:
- å›½åˆ¥ãƒ—ãƒ­ã‚­ã‚·ã§ã¯ãªãã€å„ã‚³ãƒ¡ãƒ³ãƒˆã®å®Ÿéš›ã®è¨€èªã‚’æ¤œå‡º
- ã‚ˆã‚Šæ­£ç¢ºãªè¨€èªã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘ã«ã‚ˆã‚‹æ¯”è¼ƒåˆ†æ
- æ–¹æ³•è«–çš„å¼±ç‚¹ã‚’æ”¹å–„
"""

import pandas as pd
import numpy as np
from pathlib import Path
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import warnings
import sys
import io
warnings.filterwarnings('ignore')

# Windows PowerShellã®æ–‡å­—åŒ–ã‘å¯¾ç­–
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# langdetectã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼å‡¦ç†ï¼‰
try:
    from langdetect import detect, DetectorFactory
    DetectorFactory.seed = 0  # å†ç¾æ€§ã®ãŸã‚
    LANGDETECT_AVAILABLE = True
except ImportError:
    LANGDETECT_AVAILABLE = False
    print("âš ï¸ langdetectãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install langdetect ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
DATA_DIR = Path(r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\å¤§å­¦\4å¹´\ã‚¼ãƒŸ\watching_style_analysis\data\football")
OUTPUT_DIR = Path(r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\å¤§å­¦\4å¹´\ã‚¼ãƒŸ\watching_style_analysis\output\language_refined_comparison")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# åˆ†æå¯¾è±¡ã®è©¦åˆï¼ˆã‚µãƒƒã‚«ãƒ¼ã®ã¿ã€è¨€èªå¤šæ§˜æ€§ãŒé«˜ã„ãŸã‚ï¼‰
# æ—¥æœ¬èªãƒ•ã‚©ãƒ«ãƒ€åã«ãƒãƒƒãƒ”ãƒ³ã‚°
TARGET_MATCHES = {
    "Real_Madrid_vs_Barcelona": "ãƒ¬ã‚¢ãƒ«ãƒãƒ‰ãƒªãƒ¼ãƒ‰vsãƒãƒ«ã‚»ãƒ­ãƒŠ",
    "Brazil_vs_Japan": "ãƒ–ãƒ©ã‚¸ãƒ«vsæ—¥æœ¬",
    "Brighton_vs_Man_City": "ãƒ–ãƒ©ã‚¤ãƒˆãƒ³vsãƒãƒ³ãƒã‚§ã‚¹ã‚¿ãƒ¼ã‚·ãƒ†ã‚£",
    "Leeds_vs_Spurs": "ãƒªãƒ¼ã‚ºãƒ¦ãƒŠã‚¤ãƒ†ãƒƒãƒ‰vsã‚¹ãƒ‘ãƒ¼ã‚º",
    "Real_Sociedad_vs_Real_Madrid": "ãƒ¬ã‚¢ãƒ«ã‚½ã‚·ã‚¨ãƒ€vsãƒ¬ã‚¢ãƒ«ãƒãƒ‰ãƒªãƒ¼ãƒ‰",
    "PSG_vs_Inter_Miami": "ãƒ‘ãƒªã‚µãƒ³ã‚¸ã‚§ãƒ«ãƒãƒ³vsã‚¤ãƒ³ãƒ†ãƒ«ãƒã‚¤ã‚¢ãƒŸ"
}

def detect_comment_language(text):
    """
    å€‹åˆ¥ã‚³ãƒ¡ãƒ³ãƒˆã®è¨€èªã‚’æ¤œå‡º
    
    Parameters:
    -----------
    text : str
        ã‚³ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
        
    Returns:
    --------
    str : æ¤œå‡ºã•ã‚ŒãŸè¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆ'ja', 'en', 'es', 'pt', 'hi', 'ar', 'unknown'ï¼‰
    """
    if not LANGDETECT_AVAILABLE:
        return 'unknown'
    
    if pd.isna(text) or len(str(text).strip()) < 3:
        return 'unknown'
    
    try:
        detected = detect(str(text))
        # ä¸»è¦è¨€èªã«ãƒãƒƒãƒ”ãƒ³ã‚°
        lang_map = {
            'ja': 'ja',  # æ—¥æœ¬èª
            'en': 'en',  # è‹±èª
            'es': 'es',  # ã‚¹ãƒšã‚¤ãƒ³èª
            'pt': 'pt',  # ãƒãƒ«ãƒˆã‚¬ãƒ«èª
            'hi': 'hi',  # ãƒ’ãƒ³ãƒ‡ã‚£ãƒ¼èª
            'ur': 'hi',  # ã‚¦ãƒ«ãƒ‰ã‚¥ãƒ¼èªï¼ˆãƒ’ãƒ³ãƒ‡ã‚£ãƒ¼èªã¨çµ±åˆï¼‰
            'ar': 'ar',  # ã‚¢ãƒ©ãƒ“ã‚¢èª
            'fr': 'fr',  # ãƒ•ãƒ©ãƒ³ã‚¹èª
            'de': 'de',  # ãƒ‰ã‚¤ãƒ„èª
            'it': 'it',  # ã‚¤ã‚¿ãƒªã‚¢èª
            'nl': 'nl',  # ã‚ªãƒ©ãƒ³ãƒ€èª
            'ko': 'ko',  # éŸ“å›½èª
            'zh-cn': 'zh',  # ä¸­å›½èªï¼ˆç°¡ä½“å­—ï¼‰
            'zh-tw': 'zh',  # ä¸­å›½èªï¼ˆç¹ä½“å­—ï¼‰
        }
        return lang_map.get(detected, 'other')
    except:
        return 'unknown'

def load_and_detect_languages(match_folder):
    """
    è©¦åˆãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€å„ã‚³ãƒ¡ãƒ³ãƒˆã®è¨€èªã‚’æ¤œå‡º
    
    Parameters:
    -----------
    match_folder : str
        è©¦åˆãƒ•ã‚©ãƒ«ãƒ€å
        
    Returns:
    --------
    pd.DataFrame : è¨€èªæƒ…å ±ãŒè¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    folder_path = DATA_DIR / match_folder
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    csv_files = list(folder_path.glob("*_chat_log.csv"))
    if not csv_files:
        csv_files = list(folder_path.glob("*.csv"))
    
    if not csv_files:
        print(f"âš ï¸ {match_folder}: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    all_data = []
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
            
            # ã‚«ãƒ©ãƒ åã‚’æ­£è¦åŒ–
            if 'message' in df.columns:
                df.rename(columns={'message': 'comment'}, inplace=True)
            
            if 'comment' not in df.columns:
                continue
            
            # é…ä¿¡è€…åã‚’è¿½åŠ 
            df['stream_source'] = csv_file.stem
            df['match'] = match_folder
            df['match_en'] = [k for k, v in TARGET_MATCHES.items() if v == match_folder][0] if match_folder in TARGET_MATCHES.values() else match_folder
            
            all_data.append(df)
        except Exception as e:
            print(f"âš ï¸ {csv_file.name}: èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")
            continue
    
    if not all_data:
        return None
    
    combined_df = pd.concat(all_data, ignore_index=True)
    
    print(f"ğŸ“Š {match_folder}: {len(combined_df):,} ã‚³ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿å®Œäº†")
    
    # è¨€èªæ¤œå‡ºï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§é«˜é€ŸåŒ– - å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆï¼‰
    if len(combined_df) > 50000:
        # 50,000ä»¶ã‚’è¶…ãˆã‚‹å ´åˆã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        sample_size = min(50000, len(combined_df))
        sample_indices = np.random.choice(combined_df.index, sample_size, replace=False)
        combined_df['detected_language'] = 'unknown'
        
        print(f"  ğŸ” è¨€èªæ¤œå‡ºä¸­ï¼ˆ{sample_size:,}ä»¶ã‚µãƒ³ãƒ—ãƒ«ï¼‰...")
        for idx in sample_indices:
            combined_df.loc[idx, 'detected_language'] = detect_comment_language(combined_df.loc[idx, 'comment'])
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰å…¨ä½“ã®è¨€èªåˆ†å¸ƒã‚’æ¨å®š
        sample_df = combined_df.loc[sample_indices]
        lang_dist = sample_df['detected_language'].value_counts(normalize=True)
        
        # æœªæ¤œå‡ºã®ã‚³ãƒ¡ãƒ³ãƒˆã«è¨€èªåˆ†å¸ƒã‚’é©ç”¨
        unknown_indices = combined_df[combined_df['detected_language'] == 'unknown'].index
        if len(unknown_indices) > 0:
            assigned_langs = np.random.choice(
                lang_dist.index, 
                size=len(unknown_indices), 
                p=lang_dist.values
            )
            combined_df.loc[unknown_indices, 'detected_language'] = assigned_langs
    else:
        # å…¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ¤œå‡º
        print(f"  ğŸ” è¨€èªæ¤œå‡ºä¸­ï¼ˆå…¨{len(combined_df):,}ä»¶ï¼‰...")
        combined_df['detected_language'] = combined_df['comment'].apply(detect_comment_language)
    
    # è¨€èªåˆ†å¸ƒã‚’è¡¨ç¤º
    lang_counts = combined_df['detected_language'].value_counts()
    print(f"  âœ“ è¨€èªåˆ†å¸ƒ:")
    for lang, count in lang_counts.items():
        pct = count / len(combined_df) * 100
        print(f"    {lang}: {count:,} ({pct:.1f}%)")
    
    return combined_df

def calculate_language_metrics(df):
    """
    è¨€èªã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™ã‚’è¨ˆç®—
    
    Parameters:
    -----------
    df : pd.DataFrame
        è¨€èªæƒ…å ±ä»˜ããƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        
    Returns:
    --------
    pd.DataFrame : è¨€èªåˆ¥é›†è¨ˆçµæœ
    """
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ datetime ã«å¤‰æ›
    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
    df = df.dropna(subset=['timestamp'])
    df = df.sort_values('timestamp')
    
    # é…ä¿¡æ™‚é–“ã‚’è¨ˆç®—
    total_minutes = (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 60
    
    results = []
    
    for lang in df['detected_language'].unique():
        if lang == 'unknown':
            continue
        
        lang_df = df[df['detected_language'] == lang]
        
        if len(lang_df) < 10:  # æœ€ä½10ã‚³ãƒ¡ãƒ³ãƒˆå¿…è¦
            continue
        
        # åŸºæœ¬æŒ‡æ¨™
        comment_count = len(lang_df)
        avg_length = lang_df['comment'].str.len().mean()
        
        # çµµæ–‡å­—ç‡
        emoji_pattern = r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U000024C2-\U0001F251]+'
        emoji_rate = lang_df['comment'].str.contains(emoji_pattern, regex=True, na=False).sum() / len(lang_df) * 100
        
        # CPM
        cpm = comment_count / total_minutes if total_minutes > 0 else 0
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆå¤šæ§˜æ€§ï¼‰
        word_freq = defaultdict(int)
        for comment in lang_df['comment'].dropna():
            words = str(comment).split()
            for word in words:
                word_freq[word] += 1
        
        total_words = sum(word_freq.values())
        if total_words > 0:
            probs = np.array(list(word_freq.values())) / total_words
            entropy = -np.sum(probs * np.log2(probs + 1e-10))
        else:
            entropy = 0
        
        results.append({
            'language': lang,
            'comment_count': comment_count,
            'avg_length': avg_length,
            'emoji_rate': emoji_rate,
            'cpm': cpm,
            'entropy': entropy,
            'percentage': comment_count / len(df) * 100
        })
    
    return pd.DataFrame(results)

def perform_cross_language_tests(all_results):
    """
    è¨€èªé–“ã®çµ±è¨ˆçš„æ¯”è¼ƒã‚’å®Ÿè¡Œ
    
    Parameters:
    -----------
    all_results : pd.DataFrame
        å…¨è©¦åˆã®è¨€èªåˆ¥çµæœ
        
    Returns:
    --------
    dict : çµ±è¨ˆæ¤œå®šçµæœ
    """
    print("\n" + "="*80)
    print("è¨€èªé–“ã®çµ±è¨ˆçš„æ¯”è¼ƒ...")
    print("="*80)
    
    # ä¸»è¦è¨€èªã®ã¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ãŒååˆ†ãªè¨€èªï¼‰
    lang_counts = all_results['language'].value_counts()
    major_languages = lang_counts[lang_counts >= 3].index.tolist()  # æœ€ä½3é…ä¿¡
    
    if len(major_languages) < 2:
        print("âš ï¸ ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«æ•°ã®è¨€èªãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        return {}
    
    print(f"\nğŸ“Š åˆ†æå¯¾è±¡è¨€èª: {', '.join(major_languages)}")
    
    results_dict = {}
    metrics = ['avg_length', 'emoji_rate', 'cpm', 'entropy']
    
    for metric in metrics:
        print(f"\nğŸ“Š {metric} ã®è¨€èªé–“æ¯”è¼ƒ:")
        
        # Kruskal-Wallisæ¤œå®šï¼ˆå…¨è¨€èªï¼‰
        groups = [all_results[all_results['language'] == lang][metric].values 
                  for lang in major_languages]
        
        if len(groups) >= 2 and all(len(g) > 0 for g in groups):
            h_stat, p_val = stats.kruskal(*groups)
            print(f"  Kruskal-Wallis: H={h_stat:.3f}, p={p_val:.4f}")
            
            results_dict[metric] = {
                'test': 'Kruskal-Wallis',
                'statistic': h_stat,
                'p_value': p_val,
                'significant': p_val < 0.05
            }
            
            # è¨€èªåˆ¥ã®å¹³å‡å€¤
            for lang in major_languages:
                lang_data = all_results[all_results['language'] == lang][metric]
                mean_val = lang_data.mean()
                std_val = lang_data.std()
                n = len(lang_data)
                print(f"    {lang}: {mean_val:.2f} Â± {std_val:.2f} (N={n})")
    
    return results_dict

def create_visualizations(all_results):
    """
    è¨€èªåˆ¥æ¯”è¼ƒã®å¯è¦–åŒ–
    
    Parameters:
    -----------
    all_results : pd.DataFrame
        å…¨è©¦åˆã®è¨€èªåˆ¥çµæœ
    """
    print("\n" + "="*80)
    print("å¯è¦–åŒ–ä½œæˆä¸­...")
    print("="*80)
    
    # ä¸»è¦è¨€èªã®ã¿
    lang_counts = all_results['language'].value_counts()
    major_languages = lang_counts[lang_counts >= 3].index.tolist()
    plot_data = all_results[all_results['language'].isin(major_languages)]
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('è¨€èªåˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™æ¯”è¼ƒ\nLanguage-based Engagement Metrics Comparison', 
                 fontsize=16, fontweight='bold')
    
    metrics = [
        ('avg_length', 'Average Comment Length (characters)', axes[0, 0]),
        ('emoji_rate', 'Emoji Usage Rate (%)', axes[0, 1]),
        ('cpm', 'Comments Per Minute (CPM)', axes[1, 0]),
        ('entropy', 'Comment Diversity (Entropy)', axes[1, 1])
    ]
    
    for metric, title, ax in metrics:
        sns.boxplot(data=plot_data, x='language', y=metric, ax=ax, palette='Set2')
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.set_xlabel('Language', fontsize=10)
        ax.set_ylabel(title.split('(')[0].strip(), fontsize=10)
        ax.grid(True, alpha=0.3)
        
        # å„è¨€èªã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’è¡¨ç¤º
        for i, lang in enumerate(major_languages):
            n = len(plot_data[plot_data['language'] == lang])
            ax.text(i, ax.get_ylim()[0], f'N={n}', ha='center', va='top', fontsize=8)
    
    plt.tight_layout()
    output_path = OUTPUT_DIR / "language_comparison_metrics.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"  âœ“ è¨€èªåˆ¥æŒ‡æ¨™ä¿å­˜: {output_path.name}")
    plt.close()
    
    # è¨€èªåˆ†å¸ƒã®å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(12, 6))
    lang_totals = all_results.groupby('language')['comment_count'].sum().sort_values(ascending=False)
    
    colors = sns.color_palette('husl', len(lang_totals))
    lang_totals.plot(kind='bar', ax=ax, color=colors)
    ax.set_title('Total Comments by Language\nè¨€èªåˆ¥ç·ã‚³ãƒ¡ãƒ³ãƒˆæ•°', fontsize=14, fontweight='bold')
    ax.set_xlabel('Language', fontsize=12)
    ax.set_ylabel('Total Comments', fontsize=12)
    ax.grid(True, alpha=0.3, axis='y')
    
    # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for i, (lang, count) in enumerate(lang_totals.items()):
        ax.text(i, count, f'{count:,}', ha='center', va='bottom', fontsize=9)
    
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    output_path = OUTPUT_DIR / "language_distribution.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"  âœ“ è¨€èªåˆ†å¸ƒä¿å­˜: {output_path.name}")
    plt.close()

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("="*80)
    print("è¨€èªåˆ¥æ¯”è¼ƒã®ç²¾ç·»åŒ– - Refined Language-based Comparison")
    print("="*80)
    
    if not LANGDETECT_AVAILABLE:
        print("\nâŒ langdetectãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„: pip install langdetect")
        return
    
    print("\nğŸš€ ã‚³ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã§ã®è¨€èªæ¤œå‡ºã‚’é–‹å§‹\n")
    
    # å…¨è©¦åˆã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€è¨€èªã‚’æ¤œå‡º
    all_match_data = []
    
    for match_en, match_jp in TARGET_MATCHES.items():
        print(f"\n{'='*80}")
        print(f"ğŸ“‚ {match_en}")
        print(f"{'='*80}")
        
        df = load_and_detect_languages(match_jp)
        if df is not None:
            all_match_data.append(df)
    
    if not all_match_data:
        print("\nâŒ ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    combined_data = pd.concat(all_match_data, ignore_index=True)
    print(f"\nâœ… å…¨è©¦åˆãƒ‡ãƒ¼ã‚¿çµåˆå®Œäº†: {len(combined_data):,} ã‚³ãƒ¡ãƒ³ãƒˆ")
    
    # é…ä¿¡ã”ã¨ã«è¨€èªåˆ¥æŒ‡æ¨™ã‚’è¨ˆç®—
    all_results = []
    
    for (match, stream), group in combined_data.groupby(['match', 'stream_source']):
        lang_metrics = calculate_language_metrics(group)
        lang_metrics['match'] = match
        lang_metrics['stream'] = stream
        all_results.append(lang_metrics)
    
    all_results_df = pd.concat(all_results, ignore_index=True)
    
    # çµæœã‚’ä¿å­˜
    output_csv = OUTPUT_DIR / "language_refined_analysis.csv"
    all_results_df.to_csv(output_csv, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ è¨€èªåˆ¥åˆ†æçµæœä¿å­˜: {output_csv}")
    
    # çµ±è¨ˆæ¤œå®š
    stats_results = perform_cross_language_tests(all_results_df)
    
    # å¯è¦–åŒ–
    create_visualizations(all_results_df)
    
    # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    create_summary_report(all_results_df, stats_results)
    
    print("\n" + "="*80)
    print("âœ… è¨€èªåˆ¥æ¯”è¼ƒã®ç²¾ç·»åŒ– å®Œäº†")
    print("="*80)

def create_summary_report(results_df, stats_results):
    """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
    report = []
    report.append("# è¨€èªåˆ¥æ¯”è¼ƒã®ç²¾ç·»åŒ– - Refined Language-based Analysis Report\n")
    report.append(f"**åˆ†ææ—¥æ™‚**: {pd.Timestamp.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n")
    report.append(f"**æ–¹æ³•**: ã‚³ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã§ã®è¨€èªæ¤œå‡ºï¼ˆlangdetectä½¿ç”¨ï¼‰\n")
    report.append("---\n\n")
    
    # ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
    report.append("## ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦\n\n")
    report.append(f"- **ç·é…ä¿¡æ•°**: {results_df['stream'].nunique()}\n")
    report.append(f"- **ç·ã‚³ãƒ¡ãƒ³ãƒˆæ•°**: {results_df['comment_count'].sum():,}\n")
    report.append(f"- **æ¤œå‡ºè¨€èªæ•°**: {results_df['language'].nunique()}\n\n")
    
    # è¨€èªåˆ¥çµ±è¨ˆ
    report.append("## ğŸŒ è¨€èªåˆ¥çµ±è¨ˆ\n\n")
    lang_summary = results_df.groupby('language').agg({
        'comment_count': 'sum',
        'avg_length': 'mean',
        'emoji_rate': 'mean',
        'cpm': 'mean',
        'entropy': 'mean'
    }).round(2)
    
    lang_summary['percentage'] = (lang_summary['comment_count'] / lang_summary['comment_count'].sum() * 100).round(1)
    lang_summary = lang_summary.sort_values('comment_count', ascending=False)
    
    report.append("| è¨€èª | ã‚³ãƒ¡ãƒ³ãƒˆæ•° | å‰²åˆ(%) | å¹³å‡æ–‡å­—æ•° | çµµæ–‡å­—ç‡(%) | CPM | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ |\n")
    report.append("|------|-----------|---------|-----------|------------|-----|------------|\n")
    
    for lang, row in lang_summary.iterrows():
        report.append(f"| {lang} | {row['comment_count']:,.0f} | {row['percentage']:.1f} | "
                     f"{row['avg_length']:.1f} | {row['emoji_rate']:.1f} | "
                     f"{row['cpm']:.1f} | {row['entropy']:.2f} |\n")
    
    report.append("\n")
    
    # çµ±è¨ˆçš„æœ‰æ„å·®
    report.append("## ğŸ“ˆ çµ±è¨ˆçš„æ¤œå®šçµæœ\n\n")
    
    if stats_results:
        for metric, result in stats_results.items():
            sig_mark = "âœ… **æœ‰æ„**" if result['significant'] else "âŒ éæœ‰æ„"
            report.append(f"### {metric}\n\n")
            report.append(f"- æ¤œå®š: {result['test']}\n")
            report.append(f"- çµ±è¨ˆé‡: {result['statistic']:.3f}\n")
            report.append(f"- på€¤: {result['p_value']:.4f}\n")
            report.append(f"- çµæœ: {sig_mark}\n\n")
    else:
        report.append("çµ±è¨ˆæ¤œå®šã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³ï¼‰ã€‚\n\n")
    
    # æ–¹æ³•è«–çš„æ”¹å–„ç‚¹
    report.append("## ğŸ”¬ æ–¹æ³•è«–çš„æ”¹å–„ç‚¹\n\n")
    report.append("### å¾“æ¥ã®æ–¹æ³•ï¼ˆå›½åˆ¥ãƒ—ãƒ­ã‚­ã‚·ï¼‰ã®å•é¡Œç‚¹:\n")
    report.append("- é…ä¿¡ã‚¿ã‚¤ãƒˆãƒ«ã‚„é…ä¿¡è€…ã®å›½ç±ã‹ã‚‰è¨€èªã‚’æ¨å®š\n")
    report.append("- å¤šè¨€èªé…ä¿¡ã®å ´åˆã€å®Ÿéš›ã®è¦–è´è€…è¨€èªã¨ä¹–é›¢\n")
    report.append("- è‹±èªé…ä¿¡ã§ã‚‚æ—¥æœ¬èªã‚³ãƒ¡ãƒ³ãƒˆãŒå¤šæ•°å­˜åœ¨\n\n")
    
    report.append("### æœ¬æ‰‹æ³•ã®æ”¹å–„ç‚¹:\n")
    report.append("- **å„ã‚³ãƒ¡ãƒ³ãƒˆã®å®Ÿéš›ã®è¨€èªã‚’æ¤œå‡º**ï¼ˆlangdetectä½¿ç”¨ï¼‰\n")
    report.append("- è¨€èªæ··åœ¨é…ä¿¡ã§ã‚‚æ­£ç¢ºãªè¨€èªåˆ¥é›†è¨ˆãŒå¯èƒ½\n")
    report.append("- ã‚ˆã‚Šç²¾ç·»ãªè¨€èªã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒã‚’å®Ÿç¾\n\n")
    
    # ä»Šå¾Œã®å±•é–‹
    report.append("## ğŸš€ ä»Šå¾Œã®å±•é–‹\n\n")
    report.append("1. **å¤šè¨€èªæ··åœ¨é…ä¿¡ã®è©³ç´°åˆ†æ**\n")
    report.append("   - åŒä¸€é…ä¿¡å†…ã§ã®è¨€èªåˆ‡ã‚Šæ›¿ãˆãƒ‘ã‚¿ãƒ¼ãƒ³\n")
    report.append("   - è¨€èªé–“ã®ç›¸äº’ä½œç”¨åŠ¹æœ\n\n")
    
    report.append("2. **è¨€èªç‰¹æ€§ã¨ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã®é–¢ä¿‚**\n")
    report.append("   - è¨€èªã®ç‰¹æ€§ï¼ˆè† ç€èª vs å±ˆæŠ˜èªï¼‰ã¨è¦–è´ã‚¹ã‚¿ã‚¤ãƒ«\n")
    report.append("   - æ–‡å­—ä½“ç³»ï¼ˆè¡¨éŸ³æ–‡å­— vs è¡¨æ„æ–‡å­—ï¼‰ã®å½±éŸ¿\n\n")
    
    report.append("3. **æ–‡åŒ–çš„è¦å› ã®çµ±åˆ¶**\n")
    report.append("   - è¨€èªåŠ¹æœã¨æ–‡åŒ–åŠ¹æœã®åˆ†é›¢\n")
    report.append("   - åŒä¸€è¨€èªãƒ»ç•°æ–‡åŒ–ã®æ¯”è¼ƒï¼ˆä¾‹: è‹±èªåœå„å›½ï¼‰\n\n")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    report_path = OUTPUT_DIR / "LANGUAGE_REFINED_ANALYSIS_SUMMARY.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.writelines(report)
    
    print(f"\nâœ“ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")

if __name__ == "__main__":
    main()
