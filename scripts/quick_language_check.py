"""
è¨€èªåˆ¥æ¯”è¼ƒã®ç°¡æ˜“ç‰ˆ - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§é«˜é€ŸåŒ–
Quick Language-based Comparison with Sampling

ç›®çš„:
- å…¨ã‚³ãƒ¡ãƒ³ãƒˆã§ã¯ãªãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§é«˜é€Ÿã«è¨€èªåˆ†å¸ƒã‚’æŠŠæ¡
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys
import io

# Windows PowerShellã®æ–‡å­—åŒ–ã‘å¯¾ç­–
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# langdetectã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from langdetect import detect, DetectorFactory
    DetectorFactory.seed = 0
    LANGDETECT_AVAILABLE = True
except ImportError:
    LANGDETECT_AVAILABLE = False
    print("âš ï¸ langdetectãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")

# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
DATA_DIR = Path(r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\å¤§å­¦\4å¹´\ã‚¼ãƒŸ\watching_style_analysis\data\football")
OUTPUT_DIR = Path(r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\å¤§å­¦\4å¹´\ã‚¼ãƒŸ\watching_style_analysis\output\language_quick_check")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# è©¦åˆãƒ•ã‚©ãƒ«ãƒ€ï¼ˆæ—¥æœ¬èªåï¼‰
MATCHES = {
    "Real_Madrid_vs_Barcelona": "ãƒ¬ã‚¢ãƒ«ãƒãƒ‰ãƒªãƒ¼ãƒ‰vsãƒãƒ«ã‚»ãƒ­ãƒŠ",
    "Brazil_vs_Japan": "ãƒ–ãƒ©ã‚¸ãƒ«vsæ—¥æœ¬",
    "Brighton_vs_Man_City": "ãƒ–ãƒ©ã‚¤ãƒˆãƒ³vsãƒãƒ³ãƒã‚§ã‚¹ã‚¿ãƒ¼ã‚·ãƒ†ã‚£",
    "Leeds_vs_Spurs": "ãƒªãƒ¼ã‚ºãƒ¦ãƒŠã‚¤ãƒ†ãƒƒãƒ‰vsã‚¹ãƒ‘ãƒ¼ã‚º",
    "Real_Sociedad_vs_Real_Madrid": "ãƒ¬ã‚¢ãƒ«ã‚½ã‚·ã‚¨ãƒ€vsãƒ¬ã‚¢ãƒ«ãƒãƒ‰ãƒªãƒ¼ãƒ‰",
    "PSG_vs_Inter_Miami": "ãƒ‘ãƒªã‚µãƒ³ã‚¸ã‚§ãƒ«ãƒãƒ³vsã‚¤ãƒ³ãƒ†ãƒ«ãƒã‚¤ã‚¢ãƒŸ"
}

def detect_language(text):
    """è¨€èªæ¤œå‡º"""
    if not LANGDETECT_AVAILABLE or pd.isna(text) or len(str(text).strip()) < 3:
        return 'unknown'
    try:
        detected = detect(str(text))
        lang_map = {
            'ja': 'æ—¥æœ¬èª', 'en': 'è‹±èª', 'es': 'ã‚¹ãƒšã‚¤ãƒ³èª',
            'pt': 'ãƒãƒ«ãƒˆã‚¬ãƒ«èª', 'hi': 'ãƒ’ãƒ³ãƒ‡ã‚£ãƒ¼èª', 
            'ar': 'ã‚¢ãƒ©ãƒ“ã‚¢èª', 'fr': 'ãƒ•ãƒ©ãƒ³ã‚¹èª'
        }
        return lang_map.get(detected, detected)
    except:
        return 'unknown'

def quick_language_check(match_jp, sample_size=1000):
    """é«˜é€Ÿè¨€èªãƒã‚§ãƒƒã‚¯ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰"""
    folder_path = DATA_DIR / match_jp
    
    csv_files = list(folder_path.glob("*.csv"))
    if not csv_files:
        return None
    
    results = []
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
            
            if 'message' in df.columns:
                df.rename(columns={'message': 'comment'}, inplace=True)
            
            if 'comment' not in df.columns or len(df) == 0:
                continue
            
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            n_sample = min(sample_size, len(df))
            sampled = df.sample(n=n_sample, random_state=42)
            
            # è¨€èªæ¤œå‡º
            print(f"  ğŸ” {csv_file.stem}: {len(df):,}ä»¶ä¸­{n_sample}ä»¶ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°")
            sampled['language'] = sampled['comment'].apply(detect_language)
            
            # è¨€èªåˆ†å¸ƒ
            lang_dist = sampled['language'].value_counts()
            
            for lang, count in lang_dist.items():
                pct = count / n_sample * 100
                total_est = int(count / n_sample * len(df))
                results.append({
                    'match': match_jp,
                    'stream': csv_file.stem,
                    'language': lang,
                    'sample_count': count,
                    'sample_percentage': pct,
                    'estimated_total': total_est,
                    'total_comments': len(df)
                })
        except Exception as e:
            print(f"  âš ï¸ {csv_file.name}: {e}")
            continue
    
    return pd.DataFrame(results) if results else None

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("="*80)
    print("è¨€èªåˆ¥æ¯”è¼ƒ ç°¡æ˜“ç‰ˆ - Quick Language Check")
    print("="*80)
    
    if not LANGDETECT_AVAILABLE:
        print("\nâŒ langdetectãŒå¿…è¦ã§ã™: pip install langdetect")
        return
    
    print("\nğŸš€ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹é«˜é€Ÿè¨€èªæ¤œå‡ºé–‹å§‹\n")
    
    all_results = []
    
    for match_en, match_jp in MATCHES.items():
        print(f"\n{'='*80}")
        print(f"ğŸ“‚ {match_en} ({match_jp})")
        print(f"{'='*80}")
        
        result_df = quick_language_check(match_jp, sample_size=1000)
        
        if result_df is not None:
            all_results.append(result_df)
            
            # è©¦åˆã”ã¨ã®ã‚µãƒãƒªãƒ¼
            print(f"\n  ğŸ“Š è¨€èªåˆ†å¸ƒ:")
            for lang in result_df['language'].unique():
                lang_data = result_df[result_df['language'] == lang]
                total_est = lang_data['estimated_total'].sum()
                total_comments = result_df['total_comments'].sum()
                pct = total_est / total_comments * 100
                print(f"    {lang}: æ¨å®š{total_est:,}ä»¶ ({pct:.1f}%)")
    
    if not all_results:
        print("\nâŒ ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # å…¨çµæœã‚’çµåˆ
    combined_df = pd.concat(all_results, ignore_index=True)
    
    # ä¿å­˜
    output_csv = OUTPUT_DIR / "quick_language_check_results.csv"
    combined_df.to_csv(output_csv, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ çµæœä¿å­˜: {output_csv}")
    
    # å…¨ä½“ã‚µãƒãƒªãƒ¼
    print("\n" + "="*80)
    print("å…¨ä½“ã‚µãƒãƒªãƒ¼")
    print("="*80)
    
    total_comments = combined_df['total_comments'].sum()
    print(f"\nç·ã‚³ãƒ¡ãƒ³ãƒˆæ•°: {total_comments:,}ä»¶\n")
    
    for lang in combined_df['language'].unique():
        lang_data = combined_df[combined_df['language'] == lang]
        total_est = lang_data['estimated_total'].sum()
        pct = total_est / total_comments * 100
        print(f"{lang}: æ¨å®š{total_est:,}ä»¶ ({pct:.1f}%)")
    
    print("\n" + "="*80)
    print("âœ… ç°¡æ˜“è¨€èªãƒã‚§ãƒƒã‚¯å®Œäº†")
    print("="*80)
    print("\nğŸ’¡ è©³ç´°åˆ†æã¯ analyze_language_refined.py ã§å®Ÿè¡Œã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    main()
