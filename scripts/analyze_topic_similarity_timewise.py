"""
æ™‚é–“å¸¯åˆ¥ãƒˆãƒ”ãƒƒã‚¯é¡ä¼¼åº¦åˆ†æ - El Clasico 10é…ä¿¡
Time-wise Topic Similarity Analysis

ç›®çš„:
1. åŒã˜è©¦åˆã®åŒã˜æ™‚é–“å¸¯ã§ã€å„é…ä¿¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡ºã•ã‚Œã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’æ¯”è¼ƒ
2. é…ä¿¡é–“ã®ãƒˆãƒ”ãƒƒã‚¯é¡ä¼¼åº¦ã‚’ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§è¨ˆç®—
3. ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–ã—ã¦æ–‡åŒ–çš„å·®ç•°ã‚’å®šé‡åŒ–

æ‰‹æ³•:
- BERTopicã§å„é…ä¿¡ãƒ»å„æ™‚é–“å¸¯ã®ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
- ãƒˆãƒ”ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
- éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°
"""

import pandas as pd
import numpy as np
import os
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import squareform
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from bertopic.representation import MaximalMarginalRelevance
from sklearn.feature_extraction.text import CountVectorizer
from umap import UMAP
from hdbscan import HDBSCAN
import warnings
warnings.filterwarnings('ignore')

# UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šï¼ˆæ–‡å­—åŒ–ã‘é˜²æ­¢ï¼‰
import sys
import io
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
import matplotlib
matplotlib.rcParams['font.family'] = 'sans-serif'
matplotlib.rcParams['font.sans-serif'] = ['Yu Gothic', 'Meiryo', 'MS Gothic', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Yu Gothic', 'Meiryo', 'MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ‡ãƒ¼ã‚¿è¨­å®š
FOOTBALL_STREAMS = {
    # El Clasico streams (10é…ä¿¡)
    'â±ï¸ MINUTO A MINUTO _ Real Madrid vs Barcelona _ El ClÃ¡sico_chat_log.csv': {
        'country': 'Spain', 'name': 'Spain_1'
    },
    'âš½ï¸ REAL MADRID vs FC BARCELONA _ #LaLiga 25_26 - Jornada 10 _ \'EL CLÃSICO\' EN DIRECTO_chat_log.csv': {
        'country': 'Spain', 'name': 'Spain_2'
    },
    'REAL MADRID VS FC BARCELONA EN DIRECTO _ EL CLÃSICO _ LALIGA _ Tiempo de Juego COPE _ EN VIVO_chat_log.csv': {
        'country': 'Spain', 'name': 'Spain_3'
    },
    'ã€ã‚¨ãƒ«ã‚¯ãƒ©ã‚·ã‚³ã€‘ãƒ¬ã‚¢ãƒ«ãƒãƒ‰ãƒªãƒ¼ãƒ‰Ã—ãƒãƒ«ã‚»ãƒ­ãƒŠ 0_15ã‚­ãƒƒã‚¯ã‚ªãƒ• ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æˆ¦è¡“åˆ†æ_chat_log.csv': {
        'country': 'Japan', 'name': 'Japan_1'
    },
    'ã€LIVEåˆ†æã€‘ãƒ¬ã‚¢ãƒ«ãƒãƒ‰ãƒªãƒ¼ãƒ‰vsãƒãƒ«ã‚»ãƒ­ãƒŠã€€â–·ãƒ©ãƒ»ãƒªãƒ¼ã‚¬ï½œç¬¬10ç¯€ã€€ã‚¨ãƒ«ã‚¯ãƒ©ã‚·ã‚³_chat_log.csv': {
        'country': 'Japan', 'name': 'Japan_2'
    },
    'Real Madrid vs Barcelona _EL CLASICO_ Laliga 2025 Live Reaction_chat_log.csv': {
        'country': 'UK', 'name': 'UK_1'
    },
    'Real Madrid vs Barcelona _ La Liga LIVE WATCHALONG_chat_log.csv': {
        'country': 'UK', 'name': 'UK_2'
    },
    'REAL MADRID VS BARCELONA _ EL CLASICO LIVE REACTION!_chat_log.csv': {
        'country': 'UK', 'name': 'UK_3'
    },
    'Real Madrid vs Barcelona El Clasico Watchalong LaLiga LIVE _ TFHD_chat_log.csv': {
        'country': 'UK', 'name': 'UK_4'
    },
    'ğŸ”´ REAL MADRID - BARCELONE LIVE _ ğŸš¨LE CLASICO POUR LA 1ERE PLACE ! _ ğŸ”¥PLACE AU SPECTACLE ! _ LIGA_chat_log.csv': {
        'country': 'France', 'name': 'France'
    }
}

DATA_DIR = 'data/football/ãƒ¬ã‚¢ãƒ«ãƒãƒ‰ãƒªãƒ¼ãƒ‰vsãƒãƒ«ã‚»ãƒ­ãƒŠ'
OUTPUT_DIR = 'output/topic_similarity_timewise'
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
TIME_BINS = 10  # è©¦åˆã‚’10åˆ†å‰²
MIN_COMMENTS_PER_BIN = 50  # å„æ™‚é–“å¸¯ã®æœ€å°ã‚³ãƒ¡ãƒ³ãƒˆæ•°

def load_stream_data():
    """å…¨é…ä¿¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€æ™‚é–“å¸¯ã§åˆ†å‰²"""
    print("\n" + "="*80)
    print("ğŸ“‚ Loading El Clasico streams with timestamps...")
    print("="*80)
    
    stream_data = {}
    
    for stream_file, meta in FOOTBALL_STREAMS.items():
        filepath = os.path.join(DATA_DIR, stream_file)
        if not os.path.exists(filepath):
            print(f"âš ï¸  Warning: {filepath} not found, skipping...")
            continue
        
        try:
            df = pd.read_csv(filepath, encoding='utf-8')
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ ã‚’æ¢ã™
            text_col = None
            for col in ['message', 'text', 'comment', 'body']:
                if col in df.columns:
                    text_col = col
                    break
            
            if text_col is None:
                print(f"âš ï¸  Warning: No text column in {stream_file}")
                continue
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚«ãƒ©ãƒ ã‚’æ¢ã™
            time_col = None
            for col in ['timestamp', 'time', 'time_seconds', 'elapsed_time']:
                if col in df.columns:
                    time_col = col
                    break
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
            df_stream = pd.DataFrame()
            df_stream['comment'] = df[text_col].astype(str)
            df_stream['stream'] = meta['name']
            df_stream['country'] = meta['country']
            
            if time_col is not None:
                try:
                    df_stream['timestamp'] = pd.to_datetime(df[time_col], errors='coerce')
                    first_time = df_stream['timestamp'].min()
                    df_stream['time_seconds'] = (df_stream['timestamp'] - first_time).dt.total_seconds()
                except:
                    df_stream['time_seconds'] = np.arange(len(df))
            else:
                df_stream['time_seconds'] = np.arange(len(df))
            
            # NaNã‚’é™¤å¤–
            df_stream = df_stream[df_stream['comment'].notna()]
            df_stream = df_stream[df_stream['comment'].str.strip() != '']
            
            # æ™‚é–“å¸¯ã§åˆ†å‰²ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
            df_stream['time_bin'] = pd.qcut(df_stream['time_seconds'], 
                                             q=TIME_BINS, 
                                             labels=False, 
                                             duplicates='drop')
            
            stream_data[meta['name']] = df_stream
            
            print(f"âœ… {meta['name']} ({meta['country']}): {len(df_stream):,} comments, "
                  f"{df_stream['time_bin'].nunique()} time bins")
            
        except Exception as e:
            print(f"âŒ Error loading {stream_file}: {e}")
    
    return stream_data

def extract_topics_per_stream_time(stream_data):
    """å„é…ä¿¡ãƒ»å„æ™‚é–“å¸¯ã§ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º"""
    print("\n" + "="*80)
    print("ğŸ” Extracting topics for each stream and time bin...")
    print("="*80)
    
    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆå…±é€šï¼‰
    embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    
    topic_data = []
    
    for stream_name, df_stream in stream_data.items():
        print(f"\nğŸ“Š Processing {stream_name}...")
        
        for time_bin in sorted(df_stream['time_bin'].dropna().unique()):
            df_bin = df_stream[df_stream['time_bin'] == time_bin]
            
            if len(df_bin) < MIN_COMMENTS_PER_BIN:
                print(f"  âš ï¸  Time bin {int(time_bin)} skipped (only {len(df_bin)} comments)")
                continue
            
            comments = df_bin['comment'].tolist()
            
            # ãƒˆãƒ”ãƒƒã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼šé »å‡ºå˜èªTop 10ï¼‰
            from collections import Counter
            import re
            
            # å˜èªæŠ½å‡ºï¼ˆè‹±æ•°å­—ã€æ—¥æœ¬èªã€ã‚¹ãƒšã‚¤ãƒ³èªãªã©ï¼‰
            words = []
            for comment in comments:
                # å˜èªåˆ†å‰²ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                tokens = re.findall(r'\w+', comment.lower())
                words.extend([w for w in tokens if len(w) > 2])
            
            # é »å‡ºå˜èªTop 10
            word_freq = Counter(words)
            top_words = [word for word, count in word_freq.most_common(10)]
            
            # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ï¼ˆãƒˆãƒƒãƒ—å˜èªã®å¹³å‡ï¼‰
            if top_words:
                embeddings = embedding_model.encode(top_words)
                topic_embedding = np.mean(embeddings, axis=0)
            else:
                topic_embedding = np.zeros(384)  # ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°
            
            topic_data.append({
                'stream': stream_name,
                'country': df_stream['country'].iloc[0],
                'time_bin': int(time_bin),
                'num_comments': len(df_bin),
                'top_words': ', '.join(top_words[:5]),
                'embedding': topic_embedding
            })
            
            print(f"  âœ… Time bin {int(time_bin)}: {len(df_bin)} comments, "
                  f"top words: {', '.join(top_words[:3])}")
    
    return pd.DataFrame(topic_data)

def calculate_similarity_matrix(topic_df):
    """æ™‚é–“å¸¯ã”ã¨ã«é…ä¿¡é–“ã®ãƒˆãƒ”ãƒƒã‚¯é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
    print("\n" + "="*80)
    print("ğŸ“ Calculating topic similarity between streams...")
    print("="*80)
    
    similarity_matrices = {}
    
    for time_bin in sorted(topic_df['time_bin'].unique()):
        df_bin = topic_df[topic_df['time_bin'] == time_bin]
        
        if len(df_bin) < 2:
            continue
        
        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¡Œåˆ—ã«å¤‰æ›
        streams = df_bin['stream'].tolist()
        embeddings = np.vstack(df_bin['embedding'].values)
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        similarity = cosine_similarity(embeddings)
        
        similarity_matrices[time_bin] = {
            'streams': streams,
            'similarity': similarity,
            'countries': df_bin['country'].tolist(),
            'top_words': df_bin['top_words'].tolist()
        }
        
        print(f"âœ… Time bin {time_bin}: {len(streams)} streams, "
              f"avg similarity: {similarity[np.triu_indices_from(similarity, k=1)].mean():.3f}")
    
    return similarity_matrices

def visualize_similarity_heatmaps(similarity_matrices):
    """æ™‚é–“å¸¯ã”ã¨ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆ"""
    print("\n" + "="*80)
    print("ğŸ“Š Creating similarity heatmaps...")
    print("="*80)
    
    n_bins = len(similarity_matrices)
    n_cols = 3
    n_rows = (n_bins + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 6*n_rows))
    axes = axes.flatten() if n_bins > 1 else [axes]
    
    for idx, (time_bin, data) in enumerate(sorted(similarity_matrices.items())):
        streams = data['streams']
        similarity = data['similarity']
        countries = data['countries']
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ åã‚’çŸ­ç¸®ï¼ˆå›½å_ç•ªå·ï¼‰
        stream_labels = [f"{country}_{stream.split('_')[-1]}" 
                        for stream, country in zip(streams, countries)]
        
        ax = axes[idx]
        
        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        im = ax.imshow(similarity, cmap='YlOrRd', vmin=0, vmax=1)
        
        # è»¸è¨­å®š
        ax.set_xticks(np.arange(len(streams)))
        ax.set_yticks(np.arange(len(streams)))
        ax.set_xticklabels(stream_labels, rotation=45, ha='right')
        ax.set_yticklabels(stream_labels)
        
        # å€¤ã‚’è¡¨ç¤º
        for i in range(len(streams)):
            for j in range(len(streams)):
                text = ax.text(j, i, f'{similarity[i, j]:.2f}',
                             ha="center", va="center", color="black", fontsize=8)
        
        ax.set_title(f'Time Bin {time_bin} ({int(time_bin*10)}%-{int((time_bin+1)*10)}%)',
                    fontweight='bold', fontsize=12)
        
        # ã‚«ãƒ©ãƒ¼ãƒãƒ¼
        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    
    # ä½™ã£ãŸã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
    for idx in range(len(similarity_matrices), len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    output_file = os.path.join(OUTPUT_DIR, 'topic_similarity_heatmaps_timewise.png')
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: {output_file}")
    plt.close()

def visualize_average_similarity_heatmap(similarity_matrices):
    """å…¨æ™‚é–“å¸¯ã®å¹³å‡é¡ä¼¼åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—"""
    print("\n" + "="*80)
    print("ğŸ“Š Creating average similarity heatmap...")
    print("="*80)
    
    # å…¨é…ä¿¡åã‚’å–å¾—
    all_streams = set()
    for data in similarity_matrices.values():
        all_streams.update(data['streams'])
    all_streams = sorted(list(all_streams))
    
    # é…ä¿¡åã‹ã‚‰å›½åã‚’å–å¾—
    stream_to_country = {}
    for data in similarity_matrices.values():
        for stream, country in zip(data['streams'], data['countries']):
            stream_to_country[stream] = country
    
    # å¹³å‡é¡ä¼¼åº¦è¡Œåˆ—ã‚’åˆæœŸåŒ–
    n_streams = len(all_streams)
    avg_similarity = np.zeros((n_streams, n_streams))
    count_matrix = np.zeros((n_streams, n_streams))
    
    # å„æ™‚é–“å¸¯ã®é¡ä¼¼åº¦ã‚’ç´¯ç©
    for data in similarity_matrices.values():
        streams = data['streams']
        similarity = data['similarity']
        
        for i, stream_i in enumerate(streams):
            for j, stream_j in enumerate(streams):
                idx_i = all_streams.index(stream_i)
                idx_j = all_streams.index(stream_j)
                avg_similarity[idx_i, idx_j] += similarity[i, j]
                count_matrix[idx_i, idx_j] += 1
    
    # å¹³å‡ã‚’è¨ˆç®—
    avg_similarity = np.divide(avg_similarity, count_matrix, 
                               where=count_matrix!=0, 
                               out=np.zeros_like(avg_similarity))
    
    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(14, 12))
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒ åã‚’çŸ­ç¸®
    stream_labels = [f"{stream_to_country.get(s, 'Unknown')}_{s.split('_')[-1]}" 
                    for s in all_streams]
    
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    im = ax.imshow(avg_similarity, cmap='YlOrRd', vmin=0, vmax=1)
    
    # è»¸è¨­å®š
    ax.set_xticks(np.arange(n_streams))
    ax.set_yticks(np.arange(n_streams))
    ax.set_xticklabels(stream_labels, rotation=45, ha='right', fontsize=10)
    ax.set_yticklabels(stream_labels, fontsize=10)
    
    # å€¤ã‚’è¡¨ç¤º
    for i in range(n_streams):
        for j in range(n_streams):
            if count_matrix[i, j] > 0:
                text = ax.text(j, i, f'{avg_similarity[i, j]:.2f}',
                             ha="center", va="center", 
                             color="white" if avg_similarity[i, j] > 0.5 else "black",
                             fontsize=9, fontweight='bold')
    
    ax.set_title('Average Topic Similarity Across All Time Bins\n'
                 '(El Clasico 10 Streams)',
                 fontweight='bold', fontsize=14, pad=20)
    
    # ã‚«ãƒ©ãƒ¼ãƒãƒ¼
    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    cbar.set_label('Cosine Similarity', rotation=270, labelpad=20, fontsize=12)
    
    plt.tight_layout()
    output_file = os.path.join(OUTPUT_DIR, 'topic_similarity_average_heatmap.png')
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: {output_file}")
    plt.close()
    
    return avg_similarity, all_streams

def visualize_country_similarity(similarity_matrices):
    """å›½åˆ¥ã®å¹³å‡é¡ä¼¼åº¦"""
    print("\n" + "="*80)
    print("ğŸ“Š Creating country-level similarity heatmap...")
    print("="*80)
    
    countries = ['Spain', 'Japan', 'UK', 'France']
    country_similarity = np.zeros((len(countries), len(countries)))
    count_matrix = np.zeros((len(countries), len(countries)))
    
    for data in similarity_matrices.values():
        streams = data['streams']
        similarity = data['similarity']
        stream_countries = data['countries']
        
        for i, (stream_i, country_i) in enumerate(zip(streams, stream_countries)):
            for j, (stream_j, country_j) in enumerate(zip(streams, stream_countries)):
                if country_i in countries and country_j in countries:
                    idx_i = countries.index(country_i)
                    idx_j = countries.index(country_j)
                    country_similarity[idx_i, idx_j] += similarity[i, j]
                    count_matrix[idx_i, idx_j] += 1
    
    # å¹³å‡ã‚’è¨ˆç®—
    country_similarity = np.divide(country_similarity, count_matrix,
                                   where=count_matrix!=0,
                                   out=np.zeros_like(country_similarity))
    
    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(8, 7))
    
    im = ax.imshow(country_similarity, cmap='YlOrRd', vmin=0, vmax=1)
    
    ax.set_xticks(np.arange(len(countries)))
    ax.set_yticks(np.arange(len(countries)))
    ax.set_xticklabels(countries, fontsize=12)
    ax.set_yticklabels(countries, fontsize=12)
    
    # å€¤ã‚’è¡¨ç¤º
    for i in range(len(countries)):
        for j in range(len(countries)):
            if count_matrix[i, j] > 0:
                text = ax.text(j, i, f'{country_similarity[i, j]:.3f}',
                             ha="center", va="center",
                             color="white" if country_similarity[i, j] > 0.5 else "black",
                             fontsize=14, fontweight='bold')
    
    ax.set_title('Country-Level Topic Similarity\n(Average Across All Time Bins)',
                 fontweight='bold', fontsize=14, pad=20)
    
    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    cbar.set_label('Average Cosine Similarity', rotation=270, labelpad=20, fontsize=12)
    
    plt.tight_layout()
    output_file = os.path.join(OUTPUT_DIR, 'topic_similarity_by_country.png')
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: {output_file}")
    plt.close()
    
    return country_similarity, countries

def save_similarity_stats(similarity_matrices, avg_similarity, all_streams):
    """é¡ä¼¼åº¦çµ±è¨ˆã‚’CSVã«ä¿å­˜"""
    print("\n" + "="*80)
    print("ğŸ’¾ Saving similarity statistics...")
    print("="*80)
    
    # æ™‚é–“å¸¯åˆ¥çµ±è¨ˆ
    time_stats = []
    for time_bin, data in sorted(similarity_matrices.items()):
        similarity = data['similarity']
        upper_tri = similarity[np.triu_indices_from(similarity, k=1)]
        
        time_stats.append({
            'time_bin': time_bin,
            'time_range': f'{int(time_bin*10)}-{int((time_bin+1)*10)}%',
            'num_streams': len(data['streams']),
            'avg_similarity': upper_tri.mean(),
            'std_similarity': upper_tri.std(),
            'min_similarity': upper_tri.min(),
            'max_similarity': upper_tri.max()
        })
    
    df_time_stats = pd.DataFrame(time_stats)
    output_file = os.path.join(OUTPUT_DIR, 'similarity_stats_by_time.csv')
    df_time_stats.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"âœ… Saved: {output_file}")
    
    # é…ä¿¡ãƒšã‚¢åˆ¥å¹³å‡é¡ä¼¼åº¦
    pair_stats = []
    for i, stream_i in enumerate(all_streams):
        for j, stream_j in enumerate(all_streams):
            if i < j:  # ä¸Šä¸‰è§’ã®ã¿
                pair_stats.append({
                    'stream_1': stream_i,
                    'stream_2': stream_j,
                    'avg_similarity': avg_similarity[i, j]
                })
    
    df_pair_stats = pd.DataFrame(pair_stats).sort_values('avg_similarity', ascending=False)
    output_file = os.path.join(OUTPUT_DIR, 'stream_pair_similarities.csv')
    df_pair_stats.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"âœ… Saved: {output_file}")

def main():
    print("="*80)
    print("ğŸ¯ Time-wise Topic Similarity Analysis - El Clasico")
    print("="*80)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æ™‚é–“å¸¯åˆ†å‰²
    stream_data = load_stream_data()
    
    # 2. å„é…ä¿¡ãƒ»å„æ™‚é–“å¸¯ã§ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
    topic_df = extract_topics_per_stream_time(stream_data)
    
    # 3. æ™‚é–“å¸¯ã”ã¨ã®é¡ä¼¼åº¦è¡Œåˆ—è¨ˆç®—
    similarity_matrices = calculate_similarity_matrix(topic_df)
    
    # 4. å¯è¦–åŒ–
    visualize_similarity_heatmaps(similarity_matrices)
    avg_similarity, all_streams = visualize_average_similarity_heatmap(similarity_matrices)
    country_similarity, countries = visualize_country_similarity(similarity_matrices)
    
    # 5. çµ±è¨ˆä¿å­˜
    save_similarity_stats(similarity_matrices, avg_similarity, all_streams)
    
    # ã‚µãƒãƒªãƒ¼
    print("\n" + "="*80)
    print("ğŸ“Š ANALYSIS SUMMARY")
    print("="*80)
    print(f"Total streams analyzed: {len(stream_data)}")
    print(f"Time bins: {len(similarity_matrices)}")
    print(f"Total topic comparisons: {sum(len(d['streams']) for d in similarity_matrices.values())}")
    
    print("\nğŸŒ Country-level similarity:")
    for i, country_i in enumerate(countries):
        for j, country_j in enumerate(countries):
            if i < j:
                print(f"  {country_i} - {country_j}: {country_similarity[i, j]:.3f}")
    
    print("\n" + "="*80)
    print("âœ… Time-wise Topic Similarity Analysis Complete!")
    print(f"ğŸ“ Output directory: {OUTPUT_DIR}/")
    print("="*80)

if __name__ == '__main__':
    main()
