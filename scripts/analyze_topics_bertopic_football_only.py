"""
BERTopic ã«ã‚ˆã‚‹å¤šè¨€èªãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º (Football-Onlyç‰ˆ)
ç ”ç©¶è¨ˆç”»æ›¸ 5ç¯€ã€ŒBERTopicã‚’ç”¨ã„ã¦æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—ã€ã«å¯¾å¿œ

9é…ä¿¡ (Spain 2, Japan 2, UK 4, France 1) ã®ã‚³ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡ºã—ã€
å›½åˆ¥ã®ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒã¨æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¯è¦–åŒ–ã™ã‚‹ã€‚
"""

import pandas as pd
import numpy as np
import os
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from bertopic.representation import MaximalMarginalRelevance
from sklearn.feature_extraction.text import CountVectorizer
from umap import UMAP
from hdbscan import HDBSCAN
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
plt.rcParams['axes.unicode_minus'] = False

# ==================== ãƒ‡ãƒ¼ã‚¿è¨­å®š ====================
FOOTBALL_STREAMS = {
    # El Clasico streams
    'â±ï¸ MINUTO A MINUTO _ Real Madrid vs Barcelona _ El ClÃ¡sico_chat_log.csv': {
        'country': 'Spain', 'name': 'Spain_1'
    },
    'âš½ï¸ REAL MADRID vs FC BARCELONA _ #LaLiga 25_26 - Jornada 10 _ \'EL CLÃSICO\' EN DIRECTO_chat_log.csv': {
        'country': 'Spain', 'name': 'Spain_2'
    },
    'ã€ã‚¨ãƒ«ã‚¯ãƒ©ã‚·ã‚³ã€‘ãƒ¬ã‚¢ãƒ«ãƒãƒ‰ãƒªãƒ¼ãƒ‰Ã—ãƒãƒ«ã‚»ãƒ­ãƒŠ 0_15ã‚­ãƒƒã‚¯ã‚ªãƒ• ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æˆ¦è¡“åˆ†æ_chat_log.csv': {
        'country': 'Japan', 'name': 'Japan_1'
    },
    'ã€LIVEåˆ†æã€‘ãƒ¬ã‚¢ãƒ«ãƒãƒ‰ãƒªãƒ¼ãƒ‰vsãƒãƒ«ã‚»ãƒ­ãƒŠã€€â–·ãƒ©ãƒ»ãƒªãƒ¼ã‚¬ï½œç¬¬10ç¯€ã€€ã‚¨ãƒ«ã‚¯ãƒ©ã‚·ã‚³_chat_log.csv': {
        'country': 'Japan', 'name': 'Japan_2'
    },
    'Real Madrid vs Barcelona _EL CLASICO_ Laliga 2025 Live Reaction_chat_log.csv': {
        'country': 'UK', 'name': 'UK_1'
    },
    'Real Madrid vs Barcelona _ La Liga LIVE WATCHALONG_chat_log.csv': {
        'country': 'UK', 'name': 'UK_2'
    },
    'REAL MADRID VS BARCELONA _ EL CLASICO LIVE REACTION!_chat_log.csv': {
        'country': 'UK', 'name': 'UK_3'
    },
    'Real Madrid vs Barcelona El Clasico Watchalong LaLiga LIVE _ TFHD_chat_log.csv': {
        'country': 'UK', 'name': 'UK_4'
    },
    'ğŸ”´ REAL MADRID - BARCELONE LIVE _ ğŸš¨LE CLASICO POUR LA 1ERE PLACE ! _ ğŸ”¥PLACE AU SPECTACLE ! _ LIGA_chat_log.csv': {
        'country': 'France', 'name': 'France'
    }
}

DATA_DIR = 'data/chat'
OUTPUT_DIR = 'output/bertopic_analysis'
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ==================== ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ====================
def load_football_comments():
    """Football-Only 9é…ä¿¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    all_data = []
    
    for stream_file, meta in FOOTBALL_STREAMS.items():
        filepath = os.path.join(DATA_DIR, stream_file)
        if not os.path.exists(filepath):
            print(f"âš ï¸  Warning: {filepath} not found, skipping...")
            continue
        
        try:
            df = pd.read_csv(filepath, encoding='utf-8')
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ ã‚’æ¢ã™
            text_col = None
            for col in ['message', 'text', 'comment', 'body']:
                if col in df.columns:
                    text_col = col
                    break
            
            if text_col is None:
                print(f"âš ï¸  Warning: No text column found in {stream_file}")
                continue
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚«ãƒ©ãƒ ã‚’æ¢ã™
            time_col = None
            for col in ['timestamp', 'time', 'time_seconds', 'elapsed_time']:
                if col in df.columns:
                    time_col = col
                    break
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            df_filtered = df[[text_col]].copy()
            df_filtered['comment'] = df_filtered[text_col].astype(str)
            df_filtered['country'] = meta['country']
            df_filtered['stream'] = meta['name']
            
            if time_col is not None:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’datetimeã«å¤‰æ›ã—ã¦ã‹ã‚‰æ•°å€¤åŒ–
                try:
                    df_filtered['timestamp'] = pd.to_datetime(df[time_col], errors='coerce')
                    # æœ€åˆã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰ã®çµŒéç§’æ•°ã«å¤‰æ›
                    first_time = df_filtered['timestamp'].min()
                    df_filtered['timestamp'] = (df_filtered['timestamp'] - first_time).dt.total_seconds()
                except:
                    # å¤‰æ›å¤±æ•—æ™‚ã¯è¡Œç•ªå·ã‚’ä½¿ç”¨
                    df_filtered['timestamp'] = np.arange(len(df))
            else:
                df_filtered['timestamp'] = np.arange(len(df))  # ç–‘ä¼¼ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            
            # NaNã‚’é™¤å¤–
            df_filtered = df_filtered[df_filtered['comment'].notna()]
            df_filtered = df_filtered[df_filtered['comment'].astype(str).str.strip() != '']
            
            all_data.append(df_filtered)
            print(f"âœ… Loaded {len(df_filtered)} comments from {meta['name']} ({meta['country']})")
            
        except Exception as e:
            print(f"âŒ Error loading {stream_file}: {e}")
    
    if not all_data:
        raise ValueError("No data loaded! Check DATA_DIR and file paths.")
    
    combined = pd.concat(all_data, ignore_index=True)
    print(f"\nğŸ“Š Total: {len(combined)} comments from {len(combined['stream'].unique())} streams")
    print(f"Countries: {combined['country'].value_counts().to_dict()}")
    
    return combined

# ==================== BERTopic ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ====================
def build_bertopic_model():
    """å¤šè¨€èªå¯¾å¿œ BERTopic ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰"""
    print("\nğŸ”§ Building BERTopic model...")
    
    # å¤šè¨€èªåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
    embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    
    # CountVectorizer (å¤šè¨€èªå¯¾å¿œ)
    vectorizer_model = CountVectorizer(
        token_pattern=r"(?u)\b\w+\b",
        max_features=3000,
        min_df=3,
        ngram_range=(1, 2)
    )
    
    # UMAP (æ¬¡å…ƒå‰Šæ¸›)
    umap_model = UMAP(
        n_components=5,
        n_neighbors=15,
        min_dist=0.0,
        metric='cosine',
        random_state=42
    )
    
    # HDBSCAN (ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°)
    hdbscan_model = HDBSCAN(
        min_cluster_size=30,  # å°ã•ã„ã‚¯ãƒ©ã‚¹ã‚¿ã‚‚æ¤œå‡º
        min_samples=10,
        metric='euclidean',
        cluster_selection_method='eom',
        prediction_data=True
    )
    
    # MMR (ãƒˆãƒ”ãƒƒã‚¯è¡¨ç¾ã®å¤šæ§˜æ€§å‘ä¸Š)
    representation_model = MaximalMarginalRelevance(diversity=0.5)
    
    # BERTopic
    topic_model = BERTopic(
        embedding_model=embedding_model,
        vectorizer_model=vectorizer_model,
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        representation_model=representation_model,
        top_n_words=10,
        min_topic_size=20,
        nr_topics='auto',  # è‡ªå‹•æœ€é©åŒ–
        calculate_probabilities=True,
        verbose=True
    )
    
    return topic_model

# ==================== ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º ====================
def extract_topics(df, topic_model):
    """ã‚³ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º"""
    print("\nğŸ” Extracting topics...")
    
    documents = df['comment'].tolist()
    
    # ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    topics, probs = topic_model.fit_transform(documents)
    
    # çµæœã‚’è¿½åŠ 
    df['topic'] = topics
    df['topic_prob'] = [p.max() if len(p) > 0 else 0 for p in probs]
    
    # ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±å–å¾—
    topic_info = topic_model.get_topic_info()
    print(f"\nğŸ“Š Detected {len(topic_info) - 1} topics (excluding outliers)")
    print(topic_info.head(10))
    
    return df, topic_model, topic_info

# ==================== å›½åˆ¥ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ ====================
def analyze_country_topics(df, topic_model, topic_info):
    """å›½åˆ¥ã®ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒã‚’åˆ†æ"""
    print("\nğŸ“Š Analyzing country-specific topics...")
    
    # Outlier (-1) ã‚’é™¤å¤–
    df_valid = df[df['topic'] != -1].copy()
    
    # å›½åˆ¥ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ
    country_topic_dist = pd.crosstab(
        df_valid['country'],
        df_valid['topic'],
        normalize='index'
    ) * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸
    
    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(14, 8))
    
    # ä¸Šä½10ãƒˆãƒ”ãƒƒã‚¯ã®ã¿è¡¨ç¤º
    top_topics = topic_info[topic_info['Topic'] != -1].head(10)['Topic'].tolist()
    country_topic_dist_top = country_topic_dist[top_topics]
    
    country_topic_dist_top.plot(kind='bar', stacked=False, ax=ax, width=0.8)
    
    ax.set_xlabel('Country', fontsize=14, fontweight='bold')
    ax.set_ylabel('Topic Distribution (%)', fontsize=14, fontweight='bold')
    ax.set_title('Country-Specific Topic Distribution (Top 10 Topics)', 
                 fontsize=16, fontweight='bold', pad=20)
    ax.legend(title='Topic ID', bbox_to_anchor=(1.05, 1), loc='upper left')
    ax.grid(axis='y', alpha=0.3)
    plt.xticks(rotation=0)
    plt.tight_layout()
    
    plt.savefig(os.path.join(OUTPUT_DIR, 'country_topic_distribution.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: country_topic_distribution.png")
    plt.close()
    
    # CSVä¿å­˜
    country_topic_dist.to_csv(os.path.join(OUTPUT_DIR, 'country_topic_distribution.csv'))
    print(f"âœ… Saved: country_topic_distribution.csv")
    
    return country_topic_dist

# ==================== ãƒˆãƒ”ãƒƒã‚¯æ™‚ç³»åˆ—åˆ†æ ====================
def analyze_topic_timeline(df, topic_model, topic_info):
    """ãƒˆãƒ”ãƒƒã‚¯ã®æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
    print("\nğŸ“ˆ Analyzing topic timeline...")
    
    # Outlier (-1) ã‚’é™¤å¤–
    df_valid = df[df['topic'] != -1].copy()
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
    if df_valid['timestamp'].isna().all():
        print("âš ï¸  Warning: All timestamps are NaN, using row numbers instead")
        df_valid['timestamp'] = np.arange(len(df_valid))
    
    # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ãŒå°‘ãªã„å ´åˆã®å¯¾å‡¦
    unique_timestamps = df_valid['timestamp'].nunique()
    if unique_timestamps < 10:
        print(f"âš ï¸  Warning: Only {unique_timestamps} unique timestamps, adjusting bins")
        bins = max(2, unique_timestamps)
    else:
        bins = 10
    
    # æ™‚é–“ã‚’ç­‰åˆ†
    try:
        df_valid['time_bin'] = pd.cut(df_valid['timestamp'], bins=bins, labels=False, duplicates='drop')
    except Exception as e:
        print(f"âš ï¸  Warning: Could not create time bins ({e}), using quantile-based bins")
        df_valid['time_bin'] = pd.qcut(df_valid['timestamp'], q=min(10, unique_timestamps), labels=False, duplicates='drop')
    
    # ä¸Šä½5ãƒˆãƒ”ãƒƒã‚¯ã«çµã‚‹
    top_topics = topic_info[topic_info['Topic'] != -1].head(5)['Topic'].tolist()
    df_top = df_valid[df_valid['topic'].isin(top_topics)]
    
    # æ™‚é–“ãƒ“ãƒ³ã”ã¨ã®ãƒˆãƒ”ãƒƒã‚¯å‡ºç¾æ•°
    timeline = pd.crosstab(df_top['time_bin'], df_top['topic'])
    
    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(14, 6))
    
    for topic_id in top_topics:
        if topic_id in timeline.columns:
            # ãƒˆãƒ”ãƒƒã‚¯ãƒ©ãƒ™ãƒ«å–å¾—
            topic_words = topic_model.get_topic(topic_id)
            if topic_words:
                label = f"Topic {topic_id}: {', '.join([w[0] for w in topic_words[:3]])}"
            else:
                label = f"Topic {topic_id}"
            
            ax.plot(timeline.index, timeline[topic_id], marker='o', 
                   linewidth=2, label=label)
    
    ax.set_xlabel('Time Bin (Match Progress)', fontsize=14, fontweight='bold')
    ax.set_ylabel('Comment Count', fontsize=14, fontweight='bold')
    ax.set_title('Topic Timeline During Match', fontsize=16, fontweight='bold', pad=20)
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    
    plt.savefig(os.path.join(OUTPUT_DIR, 'topic_timeline.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: topic_timeline.png")
    plt.close()
    
    # CSVä¿å­˜
    timeline.to_csv(os.path.join(OUTPUT_DIR, 'topic_timeline.csv'))
    print(f"âœ… Saved: topic_timeline.csv")
    
    return timeline

# ==================== ãƒˆãƒ”ãƒƒã‚¯è©³ç´°æƒ…å ± ====================
def save_topic_details(topic_model, topic_info):
    """ãƒˆãƒ”ãƒƒã‚¯ã®è©³ç´°æƒ…å ±ã‚’ä¿å­˜"""
    print("\nğŸ’¾ Saving topic details...")
    
    # ãƒˆãƒ”ãƒƒã‚¯å˜èªãƒªã‚¹ãƒˆ
    topic_details = []
    
    for topic_id in topic_info[topic_info['Topic'] != -1]['Topic'].tolist():
        topic_words = topic_model.get_topic(topic_id)
        if topic_words:
            words = ', '.join([f"{w[0]}({w[1]:.3f})" for w in topic_words[:10]])
            topic_details.append({
                'Topic_ID': topic_id,
                'Count': topic_info[topic_info['Topic'] == topic_id]['Count'].values[0],
                'Top_Words': words
            })
    
    topic_df = pd.DataFrame(topic_details)
    topic_df.to_csv(os.path.join(OUTPUT_DIR, 'topic_details.csv'), index=False)
    print(f"âœ… Saved: topic_details.csv")
    
    return topic_df

# ==================== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ====================
def main():
    print("="*80)
    print("ğŸ† BERTopic Analysis - Football-Only (9 Streams, 4 Countries)")
    print("="*80)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_football_comments()
    
    # 2. BERTopicãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    topic_model = build_bertopic_model()
    
    # 3. ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
    df, topic_model, topic_info = extract_topics(df, topic_model)
    
    # 4. å›½åˆ¥ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ
    country_topic_dist = analyze_country_topics(df, topic_model, topic_info)
    
    # 5. ãƒˆãƒ”ãƒƒã‚¯æ™‚ç³»åˆ—
    timeline = analyze_topic_timeline(df, topic_model, topic_info)
    
    # 6. ãƒˆãƒ”ãƒƒã‚¯è©³ç´°ä¿å­˜
    topic_details = save_topic_details(topic_model, topic_info)
    
    # 7. ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
    print("\n" + "="*80)
    print("ğŸ“Š ANALYSIS SUMMARY")
    print("="*80)
    print(f"Total comments analyzed: {len(df)}")
    print(f"Valid topics detected: {len(topic_info) - 1}")
    print(f"Outlier comments: {len(df[df['topic'] == -1])}")
    print(f"\nCountry breakdown:")
    print(df['country'].value_counts())
    print(f"\nTop 5 topics:")
    print(topic_info[topic_info['Topic'] != -1].head(5)[['Topic', 'Count', 'Name']])
    
    print("\n" + "="*80)
    print("âœ… BERTopic Analysis Complete!")
    print(f"ğŸ“ Output saved to: {OUTPUT_DIR}/")
    print("="*80)

if __name__ == '__main__':
    main()
