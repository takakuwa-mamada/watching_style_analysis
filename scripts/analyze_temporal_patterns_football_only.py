"""
è©³ç´°ãªæ™‚ç³»åˆ—åˆ†æ (Football-Onlyç‰ˆ)
ç ”ç©¶è¨ˆç”»æ›¸ 4ç¯€ã€Œç››ã‚Šä¸ŠãŒã‚Šã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã«æ³¨ç›®ã€ã«å¯¾å¿œ

è©¦åˆé€²è¡Œã«æ²¿ã£ãŸæ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¯è¦–åŒ–:
1. ã‚³ãƒ¡ãƒ³ãƒˆå¯†åº¦ã®æ™‚ç³»åˆ—å¤‰åŒ–
2. å›½åˆ¥ã®æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³
3. ãƒãƒ¼ã‚¹ãƒˆè©³ç´°åˆ†æ
4. æ„Ÿæƒ…è¡¨ç¾ã®æ™‚ç³»åˆ—æ¨ç§»
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.signal import find_peaks
import warnings
warnings.filterwarnings('ignore')

# UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šï¼ˆæ–‡å­—åŒ–ã‘é˜²æ­¢ï¼‰
import sys
import io
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆå„ªå…ˆé †ä½ã‚’å¤‰æ›´ï¼‰
import matplotlib
matplotlib.rcParams['font.family'] = 'sans-serif'
matplotlib.rcParams['font.sans-serif'] = ['Yu Gothic', 'Meiryo', 'MS Gothic', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Yu Gothic', 'Meiryo', 'MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ==================== ãƒ‡ãƒ¼ã‚¿è¨­å®š ====================
FOOTBALL_STREAMS = {
    # El Clasico streams (10é…ä¿¡)
    'â±ï¸ MINUTO A MINUTO _ Real Madrid vs Barcelona _ El ClÃ¡sico_chat_log.csv': {
        'country': 'Spain', 'name': 'Spain_1'
    },
    'âš½ï¸ REAL MADRID vs FC BARCELONA _ #LaLiga 25_26 - Jornada 10 _ \'EL CLÃSICO\' EN DIRECTO_chat_log.csv': {
        'country': 'Spain', 'name': 'Spain_2'
    },
    'REAL MADRID VS FC BARCELONA EN DIRECTO _ EL CLÃSICO _ LALIGA _ Tiempo de Juego COPE _ EN VIVO_chat_log.csv': {
        'country': 'Spain', 'name': 'Spain_3'
    },
    'ã€ã‚¨ãƒ«ã‚¯ãƒ©ã‚·ã‚³ã€‘ãƒ¬ã‚¢ãƒ«ãƒãƒ‰ãƒªãƒ¼ãƒ‰Ã—ãƒãƒ«ã‚»ãƒ­ãƒŠ 0_15ã‚­ãƒƒã‚¯ã‚ªãƒ• ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æˆ¦è¡“åˆ†æ_chat_log.csv': {
        'country': 'Japan', 'name': 'Japan_1'
    },
    'ã€LIVEåˆ†æã€‘ãƒ¬ã‚¢ãƒ«ãƒãƒ‰ãƒªãƒ¼ãƒ‰vsãƒãƒ«ã‚»ãƒ­ãƒŠã€€â–·ãƒ©ãƒ»ãƒªãƒ¼ã‚¬ï½œç¬¬10ç¯€ã€€ã‚¨ãƒ«ã‚¯ãƒ©ã‚·ã‚³_chat_log.csv': {
        'country': 'Japan', 'name': 'Japan_2'
    },
    'Real Madrid vs Barcelona _EL CLASICO_ Laliga 2025 Live Reaction_chat_log.csv': {
        'country': 'UK', 'name': 'UK_1'
    },
    'Real Madrid vs Barcelona _ La Liga LIVE WATCHALONG_chat_log.csv': {
        'country': 'UK', 'name': 'UK_2'
    },
    'REAL MADRID VS BARCELONA _ EL CLASICO LIVE REACTION!_chat_log.csv': {
        'country': 'UK', 'name': 'UK_3'
    },
    'Real Madrid vs Barcelona El Clasico Watchalong LaLiga LIVE _ TFHD_chat_log.csv': {
        'country': 'UK', 'name': 'UK_4'
    },
    'ğŸ”´ REAL MADRID - BARCELONE LIVE _ ğŸš¨LE CLASICO POUR LA 1ERE PLACE ! _ ğŸ”¥PLACE AU SPECTACLE ! _ LIGA_chat_log.csv': {
        'country': 'France', 'name': 'France'
    }
}

DATA_DIR = 'data/football/ãƒ¬ã‚¢ãƒ«ãƒãƒ‰ãƒªãƒ¼ãƒ‰vsãƒãƒ«ã‚»ãƒ­ãƒŠ'
OUTPUT_DIR = 'output/temporal_analysis_el_clasico'
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ==================== ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ====================
def load_football_data():
    """Football-Only 9é…ä¿¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    all_data = []
    
    for stream_file, meta in FOOTBALL_STREAMS.items():
        filepath = os.path.join(DATA_DIR, stream_file)
        if not os.path.exists(filepath):
            print(f"âš ï¸  Warning: {filepath} not found, skipping...")
            continue
        
        try:
            df = pd.read_csv(filepath, encoding='utf-8')
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ 
            text_col = None
            for col in ['message', 'text', 'comment', 'body']:
                if col in df.columns:
                    text_col = col
                    break
            
            if text_col is None:
                print(f"âš ï¸  Warning: No text column in {stream_file}")
                continue
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚«ãƒ©ãƒ 
            time_col = None
            for col in ['timestamp', 'time', 'time_seconds', 'elapsed_time']:
                if col in df.columns:
                    time_col = col
                    break
            
            # ãƒ‡ãƒ¼ã‚¿æ•´å½¢
            df_clean = df[[text_col]].copy()
            df_clean['comment'] = df_clean[text_col].astype(str)
            df_clean['country'] = meta['country']
            df_clean['stream'] = meta['name']
            
            if time_col:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’datetimeã«å¤‰æ›ã—ã¦ã‹ã‚‰æ•°å€¤åŒ–
                try:
                    df_clean['timestamp'] = pd.to_datetime(df[time_col], errors='coerce')
                    # æœ€åˆã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰ã®çµŒéç§’æ•°ã«å¤‰æ›
                    first_time = df_clean['timestamp'].min()
                    df_clean['timestamp'] = (df_clean['timestamp'] - first_time).dt.total_seconds()
                except:
                    # å¤‰æ›å¤±æ•—æ™‚ã¯è¡Œç•ªå·ã‚’ä½¿ç”¨
                    df_clean['timestamp'] = np.arange(len(df))
            else:
                # ç–‘ä¼¼ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— (è¡Œç•ªå·ãƒ™ãƒ¼ã‚¹)
                df_clean['timestamp'] = np.arange(len(df))
            
            # NaNé™¤å¤–
            df_clean = df_clean[df_clean['comment'].notna()]
            df_clean = df_clean[df_clean['comment'].astype(str).str.strip() != '']
            df_clean = df_clean.dropna(subset=['timestamp'])
            
            all_data.append(df_clean)
            print(f"âœ… Loaded {len(df_clean)} comments from {meta['name']} ({meta['country']})")
            
        except Exception as e:
            print(f"âŒ Error loading {stream_file}: {e}")
    
    if not all_data:
        raise ValueError("No data loaded!")
    
    combined = pd.concat(all_data, ignore_index=True)
    print(f"\nğŸ“Š Total: {len(combined)} comments from {len(combined['stream'].unique())} streams")
    
    return combined

# ==================== æ„Ÿæƒ…è¡¨ç¾ã®æŠ½å‡º ====================
def extract_emotional_features(text):
    """æ„Ÿæƒ…è¡¨ç¾ã‚’æŠ½å‡º"""
    text = str(text).lower()
    
    # Emoji count (ç°¡æ˜“ç‰ˆ)
    emoji_count = len([c for c in text if ord(c) > 0x1F300])
    
    # Exclamation
    exclamation_count = text.count('!')
    
    # Laugh (w, lol, haha, jaja)
    laugh_patterns = ['w', 'lol', 'haha', 'jaja', 'kkkk']
    laugh_count = sum([text.count(p) for p in laugh_patterns])
    
    return {
        'emoji_count': emoji_count,
        'exclamation_count': exclamation_count,
        'laugh_count': laugh_count
    }

# ==================== æ™‚ç³»åˆ—å¯†åº¦åˆ†æ ====================
def analyze_comment_density(df):
    """ã‚³ãƒ¡ãƒ³ãƒˆå¯†åº¦ã®æ™‚ç³»åˆ—åˆ†æ"""
    print("\nğŸ“ˆ Analyzing comment density over time...")
    
    # æ™‚é–“ã‚’æ­£è¦åŒ– (0-100%)
    df['time_normalized'] = (df['timestamp'] - df['timestamp'].min()) / \
                            (df['timestamp'].max() - df['timestamp'].min()) * 100
    
    # æ™‚é–“ãƒ“ãƒ³ (1%åˆ»ã¿)
    df['time_bin'] = pd.cut(df['time_normalized'], bins=100, labels=False)
    
    # å…¨ä½“å¯†åº¦
    overall_density = df.groupby('time_bin').size()
    
    # å›½åˆ¥å¯†åº¦
    country_density = df.groupby(['time_bin', 'country']).size().unstack(fill_value=0)
    
    # å¯è¦–åŒ–: å…¨ä½“
    fig, ax = plt.subplots(figsize=(16, 5))
    ax.plot(overall_density.index, overall_density.values, 
            linewidth=2, color='#2E86AB', alpha=0.8)
    ax.fill_between(overall_density.index, overall_density.values, 
                     alpha=0.3, color='#2E86AB')
    ax.set_xlabel('Match Progress (%)', fontsize=14, fontweight='bold')
    ax.set_ylabel('Comment Count per 1% Time Bin', fontsize=14, fontweight='bold')
    ax.set_title('Comment Density Timeline - All Streams', 
                 fontsize=16, fontweight='bold', pad=20)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'comment_density_overall.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: comment_density_overall.png")
    plt.close()
    
    # å¯è¦–åŒ–: å›½åˆ¥
    fig, ax = plt.subplots(figsize=(16, 6))
    for country in country_density.columns:
        ax.plot(country_density.index, country_density[country], 
                linewidth=2.5, label=country, marker='', alpha=0.8)
    ax.set_xlabel('Match Progress (%)', fontsize=14, fontweight='bold')
    ax.set_ylabel('Comment Count per 1% Time Bin', fontsize=14, fontweight='bold')
    ax.set_title('Comment Density Timeline - Country Comparison', 
                 fontsize=16, fontweight='bold', pad=20)
    ax.legend(fontsize=12, loc='upper left')
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'comment_density_by_country.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: comment_density_by_country.png")
    plt.close()
    
    # CSVä¿å­˜
    overall_density.to_csv(os.path.join(OUTPUT_DIR, 'comment_density_overall.csv'))
    country_density.to_csv(os.path.join(OUTPUT_DIR, 'comment_density_by_country.csv'))
    
    return overall_density, country_density

# ==================== ãƒãƒ¼ã‚¹ãƒˆæ¤œå‡º ====================
def detect_bursts(df):
    """ã‚³ãƒ¡ãƒ³ãƒˆãƒãƒ¼ã‚¹ãƒˆã‚’è©³ç´°ã«æ¤œå‡º"""
    print("\nğŸ’¥ Detecting comment bursts...")
    
    # æ™‚é–“ãƒ“ãƒ³ (30ç§’é–“éš”ã¾ãŸã¯100ãƒ“ãƒ³)
    df['time_bin_fine'] = pd.cut(df['timestamp'], bins=100, labels=False)
    
    # æ™‚é–“ãƒ“ãƒ³ã”ã¨ã®ã‚³ãƒ¡ãƒ³ãƒˆæ•°
    bin_counts = df.groupby('time_bin_fine').size()
    
    # ãƒ”ãƒ¼ã‚¯æ¤œå‡º (é«˜ã•: å¹³å‡ã®1.5å€ä»¥ä¸Š)
    threshold = bin_counts.mean() + 1.5 * bin_counts.std()
    peaks, properties = find_peaks(bin_counts.values, height=threshold, distance=3)
    
    # ãƒãƒ¼ã‚¹ãƒˆæƒ…å ±
    burst_info = []
    for i, peak_idx in enumerate(peaks):
        peak_time = bin_counts.index[peak_idx]
        peak_height = bin_counts.iloc[peak_idx]
        
        # ãã®ãƒãƒ¼ã‚¹ãƒˆã®ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—
        burst_comments = df[df['time_bin_fine'] == peak_time]['comment'].tolist()
        
        burst_info.append({
            'Burst_ID': i + 1,
            'Time_Bin': peak_time,
            'Time_Percent': (peak_time / 100) * 100,
            'Peak_Height': peak_height,
            'Sample_Comments': ' | '.join(burst_comments[:3])
        })
    
    burst_df = pd.DataFrame(burst_info)
    print(f"âœ… Detected {len(burst_df)} bursts")
    
    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(16, 6))
    ax.plot(bin_counts.index, bin_counts.values, linewidth=2, 
            color='#2E86AB', alpha=0.7, label='Comment Count')
    ax.scatter(peaks, bin_counts.iloc[peaks], color='red', s=100, 
               zorder=5, label=f'Bursts (n={len(peaks)})')
    ax.axhline(threshold, color='orange', linestyle='--', linewidth=2, 
               label='Threshold', alpha=0.7)
    ax.set_xlabel('Time Bin', fontsize=14, fontweight='bold')
    ax.set_ylabel('Comment Count', fontsize=14, fontweight='bold')
    ax.set_title('Comment Burst Detection', fontsize=16, fontweight='bold', pad=20)
    ax.legend(fontsize=12)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'burst_detection.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: burst_detection.png")
    plt.close()
    
    # CSVä¿å­˜
    burst_df.to_csv(os.path.join(OUTPUT_DIR, 'burst_details.csv'), index=False)
    print(f"âœ… Saved: burst_details.csv")
    
    return burst_df

# ==================== æ„Ÿæƒ…è¡¨ç¾ã®æ™‚ç³»åˆ— ====================
def analyze_emotion_timeline(df):
    """æ„Ÿæƒ…è¡¨ç¾ã®æ™‚ç³»åˆ—æ¨ç§»"""
    print("\nğŸ˜Š Analyzing emotion timeline...")
    
    # æ„Ÿæƒ…è¡¨ç¾ã‚’æŠ½å‡º
    emotion_features = df['comment'].apply(extract_emotional_features)
    df['emoji_count'] = emotion_features.apply(lambda x: x['emoji_count'])
    df['exclamation_count'] = emotion_features.apply(lambda x: x['exclamation_count'])
    df['laugh_count'] = emotion_features.apply(lambda x: x['laugh_count'])
    
    # æ™‚é–“ã‚’æ­£è¦åŒ–
    df['time_normalized'] = (df['timestamp'] - df['timestamp'].min()) / \
                            (df['timestamp'].max() - df['timestamp'].min()) * 100
    df['time_bin'] = pd.cut(df['time_normalized'], bins=20, labels=False)
    
    # æ™‚é–“ãƒ“ãƒ³ã”ã¨ã®æ„Ÿæƒ…è¡¨ç¾ç‡
    emotion_timeline = df.groupby('time_bin').agg({
        'emoji_count': 'sum',
        'exclamation_count': 'sum',
        'laugh_count': 'sum',
        'comment': 'count'
    })
    
    emotion_timeline['emoji_rate'] = emotion_timeline['emoji_count'] / emotion_timeline['comment']
    emotion_timeline['exclamation_rate'] = emotion_timeline['exclamation_count'] / emotion_timeline['comment']
    emotion_timeline['laugh_rate'] = emotion_timeline['laugh_count'] / emotion_timeline['comment']
    
    # å¯è¦–åŒ–
    fig, ax = plt.subplots(figsize=(14, 6))
    
    ax.plot(emotion_timeline.index, emotion_timeline['emoji_rate'], 
            marker='o', linewidth=2.5, label='Emoji Rate', color='#FF6B6B')
    ax.plot(emotion_timeline.index, emotion_timeline['exclamation_rate'], 
            marker='s', linewidth=2.5, label='Exclamation Rate', color='#4ECDC4')
    ax.plot(emotion_timeline.index, emotion_timeline['laugh_rate'], 
            marker='^', linewidth=2.5, label='Laugh Rate', color='#FFE66D')
    
    ax.set_xlabel('Match Progress (20 Time Bins)', fontsize=14, fontweight='bold')
    ax.set_ylabel('Emotion Expression Rate', fontsize=14, fontweight='bold')
    ax.set_title('Emotion Expression Timeline', fontsize=16, fontweight='bold', pad=20)
    ax.legend(fontsize=12, loc='upper right')
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'emotion_timeline.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: emotion_timeline.png")
    plt.close()
    
    # CSVä¿å­˜
    emotion_timeline.to_csv(os.path.join(OUTPUT_DIR, 'emotion_timeline.csv'))
    print(f"âœ… Saved: emotion_timeline.csv")
    
    return emotion_timeline

# ==================== å›½åˆ¥æ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ ====================
def compare_country_temporal_patterns(df):
    """å›½åˆ¥ã®æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¯”è¼ƒ"""
    print("\nğŸŒ Comparing country temporal patterns...")
    
    # æ™‚é–“ã‚’æ­£è¦åŒ–
    df['time_normalized'] = (df['timestamp'] - df['timestamp'].min()) / \
                            (df['timestamp'].max() - df['timestamp'].min()) * 100
    df['time_bin'] = pd.cut(df['time_normalized'], bins=20, labels=False)
    
    # å›½åˆ¥ãƒ»æ™‚é–“ãƒ“ãƒ³åˆ¥ã®ã‚³ãƒ¡ãƒ³ãƒˆæ•°
    country_time = df.groupby(['country', 'time_bin']).size().unstack(fill_value=0)
    
    # æ­£è¦åŒ– (å„å›½ã‚’0-1ã«ã‚¹ã‚±ãƒ¼ãƒ«)
    country_time_norm = country_time.div(country_time.sum(axis=1), axis=0)
    
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    fig, ax = plt.subplots(figsize=(16, 6))
    sns.heatmap(country_time_norm, cmap='YlOrRd', annot=False, 
                fmt='.2f', cbar_kws={'label': 'Normalized Comment Density'},
                ax=ax, linewidths=0.5)
    ax.set_xlabel('Time Bin (Match Progress)', fontsize=14, fontweight='bold')
    ax.set_ylabel('Country', fontsize=14, fontweight='bold')
    ax.set_title('Country Temporal Pattern Heatmap', fontsize=16, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'country_temporal_heatmap.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: country_temporal_heatmap.png")
    plt.close()
    
    # CSVä¿å­˜
    country_time_norm.to_csv(os.path.join(OUTPUT_DIR, 'country_temporal_patterns.csv'))
    print(f"âœ… Saved: country_temporal_patterns.csv")
    
    return country_time_norm

# ==================== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ====================
def main():
    print("="*80)
    print("â±ï¸  Temporal Analysis - Football-Only (9 Streams, 4 Countries)")
    print("="*80)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_football_data()
    
    # 2. ã‚³ãƒ¡ãƒ³ãƒˆå¯†åº¦åˆ†æ
    overall_density, country_density = analyze_comment_density(df)
    
    # 3. ãƒãƒ¼ã‚¹ãƒˆæ¤œå‡º
    burst_df = detect_bursts(df)
    
    # 4. æ„Ÿæƒ…è¡¨ç¾ã®æ™‚ç³»åˆ—
    emotion_timeline = analyze_emotion_timeline(df)
    
    # 5. å›½åˆ¥æ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³
    country_patterns = compare_country_temporal_patterns(df)
    
    # ã‚µãƒãƒªãƒ¼
    print("\n" + "="*80)
    print("ğŸ“Š TEMPORAL ANALYSIS SUMMARY")
    print("="*80)
    print(f"Total comments: {len(df)}")
    print(f"Time range: {df['timestamp'].min():.1f} - {df['timestamp'].max():.1f}")
    print(f"Bursts detected: {len(burst_df)}")
    print(f"\nTop 3 bursts:")
    print(burst_df.nlargest(3, 'Peak_Height')[['Burst_ID', 'Time_Percent', 'Peak_Height']])
    
    print("\n" + "="*80)
    print("âœ… Temporal Analysis Complete!")
    print(f"ğŸ“ Output saved to: {OUTPUT_DIR}/")
    print("="*80)

if __name__ == '__main__':
    main()
