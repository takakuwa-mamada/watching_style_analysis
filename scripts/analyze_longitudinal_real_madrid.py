"""
ç¸¦æ–­çš„æ¯”è¼ƒåˆ†æž - ãƒ¬ã‚¢ãƒ«ãƒžãƒ‰ãƒªãƒ¼ãƒ‰3è©¦åˆ
Longitudinal Comparison Analysis - Real Madrid 3 Matches

ç›®çš„:
- åŒä¸€ãƒãƒ¼ãƒ ã®ç•°ãªã‚‹è©¦åˆã‚’æ¯”è¼ƒ
- è©¦åˆç‰¹æ€§ã®å¤‰åŒ–ã«ã‚ˆã‚‹è¦–è´ã‚¹ã‚¿ã‚¤ãƒ«ã®å¤‰åŒ–ã‚’æ¤œå‡º
- é…ä¿¡è€…ã‚’å›ºå®šã—ã¦ç´”ç²‹ãªè©¦åˆåŠ¹æžœã‚’æŠ½å‡º
"""

import pandas as pd
import numpy as np
from pathlib import Path
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import warnings
import sys
import io
warnings.filterwarnings('ignore')

# Windows PowerShellã®æ–‡å­—åŒ–ã‘å¯¾ç­–
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
DATA_DIR = Path(r"G:\ãƒžã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\å¤§å­¦\4å¹´\ã‚¼ãƒŸ\watching_style_analysis\data\football")
OUTPUT_DIR = Path(r"G:\ãƒžã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\å¤§å­¦\4å¹´\ã‚¼ãƒŸ\watching_style_analysis\output\longitudinal_real_madrid")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ãƒ¬ã‚¢ãƒ«ãƒžãƒ‰ãƒªãƒ¼ãƒ‰3è©¦åˆï¼ˆæ—¥æœ¬èªžãƒ•ã‚©ãƒ«ãƒ€åã«ãƒžãƒƒãƒ”ãƒ³ã‚°ï¼‰
REAL_MADRID_MATCHES = {
    "Real_Madrid_vs_Barcelona": {
        "folder": "ãƒ¬ã‚¢ãƒ«ãƒžãƒ‰ãƒªãƒ¼ãƒ‰vsãƒãƒ«ã‚»ãƒ­ãƒŠ",
        "opponent": "Barcelona",
        "tier": 1,
        "importance": "Ultra-High",
        "context": "El Clasico - æœ€é«˜å³°ã®ä¸€æˆ¦"
    },
    "Real_Sociedad_vs_Real_Madrid": {
        "folder": "ãƒ¬ã‚¢ãƒ«ã‚½ã‚·ã‚¨ãƒ€vsãƒ¬ã‚¢ãƒ«ãƒžãƒ‰ãƒªãƒ¼ãƒ‰",
        "opponent": "Real Sociedad",
        "tier": 3,
        "importance": "Medium",
        "context": "ãƒªãƒ¼ã‚°æˆ¦ä¸­ä½å¯¾æˆ¦"
    },
    "PSG_vs_Inter_Miami": {
        "folder": "ãƒ‘ãƒªã‚µãƒ³ã‚¸ã‚§ãƒ«ãƒžãƒ³vsã‚¤ãƒ³ãƒ†ãƒ«ãƒžã‚¤ã‚¢ãƒŸ",
        "opponent": "PSG (Real Madridé–¢é€£)",
        "tier": 4,
        "importance": "Low",
        "context": "è¦ªå–„è©¦åˆï¼ˆæ³¨: æ­£ç¢ºã«ã¯Messiçµ¡ã¿ï¼‰"
    }
}

def load_match_data(match_folder):
    """
    è©¦åˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    
    Parameters:
    -----------
    match_folder : str
        è©¦åˆãƒ•ã‚©ãƒ«ãƒ€å
        
    Returns:
    --------
    pd.DataFrame : èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
    """
    folder_path = DATA_DIR / match_folder
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    csv_files = list(folder_path.glob("*_chat_log.csv"))
    if not csv_files:
        csv_files = list(folder_path.glob("*.csv"))
    
    if not csv_files:
        print(f"âš ï¸ {match_folder}: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    all_data = []
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
            
            # ã‚«ãƒ©ãƒ åã‚’æ­£è¦åŒ–
            if 'message' in df.columns:
                df.rename(columns={'message': 'comment'}, inplace=True)
            
            if 'comment' not in df.columns:
                continue
            
            # é…ä¿¡è€…åã‚’è¿½åŠ 
            df['stream_source'] = csv_file.stem
            df['match'] = match_folder
            
            all_data.append(df)
        except Exception as e:
            print(f"âš ï¸ {csv_file.name}: èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")
            continue
    
    if not all_data:
        return None
    
    combined_df = pd.concat(all_data, ignore_index=True)
    return combined_df

def calculate_engagement_metrics(df):
    """
    ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™ã‚’è¨ˆç®—
    
    Parameters:
    -----------
    df : pd.DataFrame
        è©¦åˆãƒ‡ãƒ¼ã‚¿
        
    Returns:
    --------
    dict : å„ç¨®æŒ‡æ¨™
    """
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ datetime ã«å¤‰æ›
    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
    df = df.dropna(subset=['timestamp'])
    df = df.sort_values('timestamp')
    
    # é…ä¿¡æ™‚é–“
    duration_minutes = (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 60
    
    # åŸºæœ¬æŒ‡æ¨™
    total_comments = len(df)
    cpm = total_comments / duration_minutes if duration_minutes > 0 else 0
    
    # ã‚³ãƒ¡ãƒ³ãƒˆé•·
    avg_length = df['comment'].str.len().mean()
    
    # çµµæ–‡å­—çŽ‡
    emoji_pattern = r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U000024C2-\U0001F251]+'
    emoji_rate = df['comment'].str.contains(emoji_pattern, regex=True, na=False).sum() / total_comments * 100
    
    # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    word_freq = defaultdict(int)
    for comment in df['comment'].dropna():
        words = str(comment).split()
        for word in words:
            word_freq[word] += 1
    
    total_words = sum(word_freq.values())
    if total_words > 0:
        probs = np.array(list(word_freq.values())) / total_words
        entropy = -np.sum(probs * np.log2(probs + 1e-10))
    else:
        entropy = 0
    
    # æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ - ãƒãƒ¼ã‚¹ãƒˆæ¤œå‡º
    df['minute'] = ((df['timestamp'] - df['timestamp'].min()).dt.total_seconds() / 60).astype(int)
    comments_per_minute = df.groupby('minute').size()
    
    burst_threshold = comments_per_minute.mean() + comments_per_minute.std()
    burst_count = (comments_per_minute > burst_threshold).sum()
    burst_frequency = burst_count / len(comments_per_minute) if len(comments_per_minute) > 0 else 0
    
    # CPMã®å¤‰å‹•ä¿‚æ•°
    cpm_cv = comments_per_minute.std() / comments_per_minute.mean() if comments_per_minute.mean() > 0 else 0
    
    return {
        'total_comments': total_comments,
        'duration_minutes': duration_minutes,
        'cpm': cpm,
        'avg_comment_length': avg_length,
        'emoji_rate': emoji_rate,
        'entropy': entropy,
        'burst_count': burst_count,
        'burst_frequency': burst_frequency,
        'cpm_cv': cpm_cv,
        'stream_count': df['stream_source'].nunique()
    }

def analyze_streamer_consistency(all_data):
    """
    é…ä¿¡è€…ã®ä¸€è²«æ€§ã‚’åˆ†æžï¼ˆåŒã˜é…ä¿¡è€…ãŒè¤‡æ•°è©¦åˆã«å‡ºç¾ã™ã‚‹ã‹ï¼‰
    
    Parameters:
    -----------
    all_data : dict
        è©¦åˆå -> DataFrame ã®ãƒžãƒƒãƒ”ãƒ³ã‚°
        
    Returns:
    --------
    dict : é…ä¿¡è€…ã®å‡ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
    """
    streamer_appearances = defaultdict(list)
    
    for match, df in all_data.items():
        for streamer in df['stream_source'].unique():
            streamer_appearances[streamer].append(match)
    
    # è¤‡æ•°è©¦åˆã«å‡ºç¾ã™ã‚‹é…ä¿¡è€…
    consistent_streamers = {k: v for k, v in streamer_appearances.items() if len(v) > 1}
    
    return consistent_streamers

def perform_longitudinal_tests(metrics_df):
    """
    ç¸¦æ–­çš„æ¯”è¼ƒã®çµ±è¨ˆæ¤œå®š
    
    Parameters:
    -----------
    metrics_df : pd.DataFrame
        è©¦åˆåˆ¥æŒ‡æ¨™
        
    Returns:
    --------
    dict : çµ±è¨ˆæ¤œå®šçµæžœ
    """
    print("\n" + "="*80)
    print("ç¸¦æ–­çš„æ¯”è¼ƒã®çµ±è¨ˆæ¤œå®š...")
    print("="*80)
    
    results = {}
    
    test_metrics = ['cpm', 'avg_comment_length', 'emoji_rate', 'entropy', 'burst_frequency', 'cpm_cv']
    
    for metric in test_metrics:
        print(f"\nðŸ“Š {metric}:")
        
        # è©¦åˆåˆ¥ã®å¹³å‡å€¤
        for _, row in metrics_df.iterrows():
            print(f"  {row['match']}: {row[metric]:.2f}")
        
        # 3è©¦åˆé–“ã®å·®ã®æ¤œå®šï¼ˆKruskal-Wallisï¼‰
        groups = [metrics_df[metrics_df['match'] == match][metric].values 
                  for match in metrics_df['match'].unique()]
        
        if len(groups) >= 2 and all(len(g) > 0 for g in groups):
            h_stat, p_val = stats.kruskal(*groups)
            print(f"  Kruskal-Wallis: H={h_stat:.3f}, p={p_val:.4f}")
            
            results[metric] = {
                'test': 'Kruskal-Wallis',
                'statistic': h_stat,
                'p_value': p_val,
                'significant': p_val < 0.05
            }
    
    return results

def create_visualizations(metrics_df, all_data):
    """
    ç¸¦æ–­çš„æ¯”è¼ƒã®å¯è¦–åŒ–
    
    Parameters:
    -----------
    metrics_df : pd.DataFrame
        è©¦åˆåˆ¥æŒ‡æ¨™
    all_data : dict
        å…¨è©¦åˆãƒ‡ãƒ¼ã‚¿
    """
    print("\n" + "="*80)
    print("å¯è¦–åŒ–ä½œæˆä¸­...")
    print("="*80)
    
    # 1. è©¦åˆåˆ¥æŒ‡æ¨™ã®æ¯”è¼ƒ
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('ãƒ¬ã‚¢ãƒ«ãƒžãƒ‰ãƒªãƒ¼ãƒ‰ç¸¦æ–­çš„æ¯”è¼ƒ\nReal Madrid Longitudinal Comparison', 
                 fontsize=16, fontweight='bold')
    
    metrics_to_plot = [
        ('cpm', 'Comments Per Minute (CPM)', axes[0, 0]),
        ('avg_comment_length', 'Average Comment Length (characters)', axes[0, 1]),
        ('emoji_rate', 'Emoji Usage Rate (%)', axes[0, 2]),
        ('entropy', 'Comment Diversity (Entropy)', axes[1, 0]),
        ('burst_frequency', 'Burst Frequency', axes[1, 1]),
        ('cpm_cv', 'CPM Coefficient of Variation', axes[1, 2])
    ]
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    
    for metric, title, ax in metrics_to_plot:
        # è©¦åˆåˆ¥ã®å¹³å‡å€¤ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        match_names = metrics_df['match'].unique()
        match_means = [metrics_df[metrics_df['match'] == m][metric].mean() for m in match_names]
        match_stds = [metrics_df[metrics_df['match'] == m][metric].std() for m in match_names]
        
        x_pos = np.arange(len(match_names))
        ax.bar(x_pos, match_means, yerr=match_stds, capsize=5, color=colors, alpha=0.7, edgecolor='black')
        
        ax.set_title(title, fontsize=11, fontweight='bold')
        ax.set_xticks(x_pos)
        ax.set_xticklabels([m.replace('_', ' ') for m in match_names], rotation=15, ha='right', fontsize=9)
        ax.set_ylabel(title.split('(')[0].strip(), fontsize=10)
        ax.grid(True, alpha=0.3, axis='y')
        
        # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’è¡¨ç¤º
        for i, match in enumerate(match_names):
            n = len(metrics_df[metrics_df['match'] == match])
            ax.text(i, ax.get_ylim()[0], f'N={n}', ha='center', va='top', fontsize=8)
    
    plt.tight_layout()
    output_path = OUTPUT_DIR / "longitudinal_metrics_comparison.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"  âœ“ æŒ‡æ¨™æ¯”è¼ƒä¿å­˜: {output_path.name}")
    plt.close()
    
    # 2. æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–
    fig, axes = plt.subplots(len(all_data), 1, figsize=(14, 4 * len(all_data)))
    if len(all_data) == 1:
        axes = [axes]
    
    fig.suptitle('è©¦åˆã”ã¨ã®ã‚³ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³æŽ¨ç§»\nComment Pattern Over Time by Match', 
                 fontsize=14, fontweight='bold')
    
    for idx, (match, df) in enumerate(all_data.items()):
        ax = axes[idx]
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ datetime ã«å¤‰æ›
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
        df = df.dropna(subset=['timestamp'])
        df = df.sort_values('timestamp')
        
        # åˆ†å˜ä½ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        df['minute'] = ((df['timestamp'] - df['timestamp'].min()).dt.total_seconds() / 60).astype(int)
        comments_per_minute = df.groupby('minute').size()
        
        # ãƒ—ãƒ­ãƒƒãƒˆ
        ax.plot(comments_per_minute.index, comments_per_minute.values, color=colors[idx % len(colors)], linewidth=1.5)
        ax.fill_between(comments_per_minute.index, comments_per_minute.values, alpha=0.3, color=colors[idx % len(colors)])
        
        # å¹³å‡ç·š
        mean_cpm = comments_per_minute.mean()
        ax.axhline(mean_cpm, color='red', linestyle='--', linewidth=1, alpha=0.7, label=f'Mean: {mean_cpm:.1f}')
        
        ax.set_title(f"{match.replace('_', ' ')}", fontsize=11, fontweight='bold')
        ax.set_xlabel('Time (minutes)', fontsize=10)
        ax.set_ylabel('Comments per Minute', fontsize=10)
        ax.legend(loc='upper right', fontsize=9)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_path = OUTPUT_DIR / "longitudinal_time_series.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"  âœ“ æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜: {output_path.name}")
    plt.close()

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("="*80)
    print("ç¸¦æ–­çš„æ¯”è¼ƒåˆ†æž - ãƒ¬ã‚¢ãƒ«ãƒžãƒ‰ãƒªãƒ¼ãƒ‰3è©¦åˆ")
    print("Longitudinal Comparison Analysis - Real Madrid 3 Matches")
    print("="*80)
    
    print("\nðŸš€ åˆ†æžé–‹å§‹\n")
    
    # å„è©¦åˆã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    all_data = {}
    all_metrics = []
    
    for match, info in REAL_MADRID_MATCHES.items():
        print(f"\n{'='*80}")
        print(f"ðŸ“‚ {match}")
        print(f"   å¯¾æˆ¦ç›¸æ‰‹: {info['opponent']}")
        print(f"   é‡è¦åº¦: Tier {info['tier']} ({info['importance']})")
        print(f"   æ–‡è„ˆ: {info['context']}")
        print(f"{'='*80}")
        
        df = load_match_data(info['folder'])
        
        if df is None:
            print(f"âš ï¸ {match}: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—")
            continue
        
        print(f"âœ“ {len(df):,} ã‚³ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿å®Œäº†")
        print(f"  é…ä¿¡æ•°: {df['stream_source'].nunique()}")
        
        all_data[match] = df
        
        # é…ä¿¡ã”ã¨ã«æŒ‡æ¨™ã‚’è¨ˆç®—
        for stream in df['stream_source'].unique():
            stream_df = df[df['stream_source'] == stream]
            metrics = calculate_engagement_metrics(stream_df)
            metrics['match'] = match
            metrics['stream_source'] = stream
            metrics['tier'] = info['tier']
            metrics['importance'] = info['importance']
            all_metrics.append(metrics)
    
    if not all_metrics:
        print("\nâŒ ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    metrics_df = pd.DataFrame(all_metrics)
    
    # çµæžœã‚’ä¿å­˜
    output_csv = OUTPUT_DIR / "longitudinal_real_madrid_metrics.csv"
    metrics_df.to_csv(output_csv, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ ç¸¦æ–­çš„æŒ‡æ¨™ä¿å­˜: {output_csv}")
    
    # é…ä¿¡è€…ã®ä¸€è²«æ€§ã‚’ç¢ºèª
    print("\n" + "="*80)
    print("é…ä¿¡è€…ã®å‡ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³...")
    print("="*80)
    
    consistent_streamers = analyze_streamer_consistency(all_data)
    
    if consistent_streamers:
        print(f"\nâœ… è¤‡æ•°è©¦åˆã«å‡ºç¾ã™ã‚‹é…ä¿¡è€…: {len(consistent_streamers)}å")
        for streamer, matches in consistent_streamers.items():
            print(f"  - {streamer}: {', '.join(matches)}")
    else:
        print("\nâš ï¸ è¤‡æ•°è©¦åˆã«å‡ºç¾ã™ã‚‹é…ä¿¡è€…ãªã—ï¼ˆè©¦åˆé–“ã®ç›´æŽ¥æ¯”è¼ƒãŒå›°é›£ï¼‰")
    
    # çµ±è¨ˆæ¤œå®š
    stats_results = perform_longitudinal_tests(metrics_df)
    
    # å¯è¦–åŒ–
    create_visualizations(metrics_df, all_data)
    
    # ã‚µãƒžãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    create_summary_report(metrics_df, stats_results, consistent_streamers)
    
    print("\n" + "="*80)
    print("âœ… ç¸¦æ–­çš„æ¯”è¼ƒåˆ†æž å®Œäº†")
    print("="*80)

def create_summary_report(metrics_df, stats_results, consistent_streamers):
    """ã‚µãƒžãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
    report = []
    report.append("# ç¸¦æ–­çš„æ¯”è¼ƒåˆ†æž - Longitudinal Comparison Report\n")
    report.append(f"**åˆ†æžæ—¥æ™‚**: {pd.Timestamp.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n")
    report.append(f"**å¯¾è±¡**: ãƒ¬ã‚¢ãƒ«ãƒžãƒ‰ãƒªãƒ¼ãƒ‰é–¢é€£3è©¦åˆ\n")
    report.append("---\n\n")
    
    # ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
    report.append("## ðŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦\n\n")
    
    for match in metrics_df['match'].unique():
        match_data = metrics_df[metrics_df['match'] == match]
        report.append(f"### {match.replace('_', ' ')}\n")
        report.append(f"- é…ä¿¡æ•°: {match_data['stream_count'].iloc[0]}\n")
        report.append(f"- ç·ã‚³ãƒ¡ãƒ³ãƒˆæ•°: {match_data['total_comments'].sum():,}\n")
        report.append(f"- é‡è¦åº¦: {match_data['importance'].iloc[0]}\n\n")
    
    # è©¦åˆåˆ¥æŒ‡æ¨™
    report.append("## ðŸŽ¯ è©¦åˆåˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™\n\n")
    
    summary_metrics = metrics_df.groupby('match').agg({
        'cpm': ['mean', 'std'],
        'avg_comment_length': ['mean', 'std'],
        'emoji_rate': ['mean', 'std'],
        'entropy': ['mean', 'std'],
        'burst_frequency': ['mean', 'std'],
        'cpm_cv': ['mean', 'std']
    }).round(2)
    
    report.append("| è©¦åˆ | CPM | ã‚³ãƒ¡ãƒ³ãƒˆé•· | çµµæ–‡å­—çŽ‡(%) | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | ãƒãƒ¼ã‚¹ãƒˆé »åº¦ | CPMå¤‰å‹•ä¿‚æ•° |\n")
    report.append("|------|-----|-----------|------------|-------------|-------------|-------------|\n")
    
    for match in metrics_df['match'].unique():
        match_data = metrics_df[metrics_df['match'] == match]
        report.append(f"| {match.replace('_', ' ')} | "
                     f"{match_data['cpm'].mean():.1f}Â±{match_data['cpm'].std():.1f} | "
                     f"{match_data['avg_comment_length'].mean():.1f}Â±{match_data['avg_comment_length'].std():.1f} | "
                     f"{match_data['emoji_rate'].mean():.1f}Â±{match_data['emoji_rate'].std():.1f} | "
                     f"{match_data['entropy'].mean():.2f}Â±{match_data['entropy'].std():.2f} | "
                     f"{match_data['burst_frequency'].mean():.3f}Â±{match_data['burst_frequency'].std():.3f} | "
                     f"{match_data['cpm_cv'].mean():.2f}Â±{match_data['cpm_cv'].std():.2f} |\n")
    
    report.append("\n")
    
    # çµ±è¨ˆçš„æ¤œå®š
    report.append("## ðŸ“ˆ çµ±è¨ˆçš„æ¤œå®šçµæžœ\n\n")
    
    if stats_results:
        for metric, result in stats_results.items():
            sig_mark = "âœ… **æœ‰æ„**" if result['significant'] else "âŒ éžæœ‰æ„"
            report.append(f"### {metric}\n\n")
            report.append(f"- æ¤œå®š: {result['test']}\n")
            report.append(f"- çµ±è¨ˆé‡: {result['statistic']:.3f}\n")
            report.append(f"- på€¤: {result['p_value']:.4f}\n")
            report.append(f"- çµæžœ: {sig_mark}\n\n")
    
    # é…ä¿¡è€…ã®ä¸€è²«æ€§
    report.append("## ðŸ‘¥ é…ä¿¡è€…ã®å‡ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³\n\n")
    
    if consistent_streamers:
        report.append(f"**è¤‡æ•°è©¦åˆã«å‡ºç¾ã™ã‚‹é…ä¿¡è€…**: {len(consistent_streamers)}å\n\n")
        for streamer, matches in consistent_streamers.items():
            report.append(f"- `{streamer}`: {', '.join(matches)}\n")
        report.append("\nðŸ’¡ ã“ã‚Œã‚‰ã®é…ä¿¡è€…ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠåŽ³å¯†ãªè©¦åˆé–“æ¯”è¼ƒãŒå¯èƒ½\n\n")
    else:
        report.append("âš ï¸ è¤‡æ•°è©¦åˆã«å‡ºç¾ã™ã‚‹é…ä¿¡è€…ãªã—\n")
        report.append("â†’ è©¦åˆé–“ã®ç›´æŽ¥æ¯”è¼ƒã«ã¯é™ç•Œã‚ã‚Šï¼ˆé…ä¿¡è€…åŠ¹æžœãŒæ··äº¤ï¼‰\n\n")
    
    # ç™ºè¦‹ã¨ç¤ºå”†
    report.append("## ðŸ” ä¸»è¦ãªç™ºè¦‹\n\n")
    
    # CPMã®å¤‰åŒ–
    cpm_by_match = metrics_df.groupby('match')['cpm'].mean().sort_values(ascending=False)
    report.append("### CPMï¼ˆã‚³ãƒ¡ãƒ³ãƒˆå¯†åº¦ï¼‰ã®å¤‰åŒ–\n\n")
    for match, cpm in cpm_by_match.items():
        tier = metrics_df[metrics_df['match'] == match]['tier'].iloc[0]
        report.append(f"- **{match.replace('_', ' ')}** (Tier {tier}): {cpm:.1f} CPM\n")
    report.append("\n")
    
    # æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å¾´
    report.append("### æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å¾´\n\n")
    burst_by_match = metrics_df.groupby('match')['burst_frequency'].mean().sort_values(ascending=False)
    for match, freq in burst_by_match.items():
        report.append(f"- **{match.replace('_', ' ')}**: ãƒãƒ¼ã‚¹ãƒˆé »åº¦ {freq:.3f}\n")
    report.append("\n")
    
    # ä»Šå¾Œã®å±•é–‹
    report.append("## ðŸš€ ä»Šå¾Œã®å±•é–‹\n\n")
    report.append("1. **åŒä¸€é…ä¿¡è€…ã§ã®è©¦åˆé–“æ¯”è¼ƒ**\n")
    report.append("   - è¤‡æ•°è©¦åˆã«å‡ºç¾ã™ã‚‹é…ä¿¡è€…ã«çµžã£ãŸåˆ†æž\n")
    report.append("   - é…ä¿¡è€…åŠ¹æžœã‚’åˆ¶å¾¡ã—ãŸç´”ç²‹ãªè©¦åˆåŠ¹æžœã®æŠ½å‡º\n\n")
    
    report.append("2. **è©¦åˆå±•é–‹ã¨ã®å¯¾å¿œ**\n")
    report.append("   - å¾—ç‚¹ã‚·ãƒ¼ãƒ³ã¨ã‚³ãƒ¡ãƒ³ãƒˆãƒãƒ¼ã‚¹ãƒˆã®å¯¾å¿œåˆ†æž\n")
    report.append("   - è©¦åˆã®ç·Šè¿«åº¦ã¨ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã®é–¢ä¿‚\n\n")
    
    report.append("3. **ä»–ãƒãƒ¼ãƒ ã¨ã®æ¯”è¼ƒ**\n")
    report.append("   - ãƒ¬ã‚¢ãƒ«ãƒžãƒ‰ãƒªãƒ¼ãƒ‰ã¨ä»–ãƒãƒ¼ãƒ ã®ç¸¦æ–­çš„ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ\n")
    report.append("   - ãƒãƒ¼ãƒ ç‰¹æ€§ï¼ˆæ”»æ’ƒçš„ vs å®ˆå‚™çš„ï¼‰ã®å½±éŸ¿\n\n")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    report_path = OUTPUT_DIR / "LONGITUDINAL_ANALYSIS_SUMMARY.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.writelines(report)
    
    print(f"\nâœ“ ã‚µãƒžãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")

if __name__ == "__main__":
    main()
