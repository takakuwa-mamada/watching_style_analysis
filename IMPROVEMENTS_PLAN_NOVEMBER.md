# ğŸ”§ ãƒ—ãƒ­ã‚°ãƒ©ãƒ æ”¹å–„è¨ˆç”»ï¼ˆ11æœˆä¸­ï¼‰

**ç›®æ¨™**: ç¾çŠ¶ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã€12æœˆã®è«–æ–‡åŸ·ç­†æ™‚ã«èª¬å¾—åŠ›ã®ã‚ã‚‹çµæœã‚’æç¤ºã™ã‚‹

---

## ğŸ“Š ç¾çŠ¶ã®èª²é¡Œï¼ˆå„ªå…ˆåº¦é †ï¼‰

### ğŸ”´ **æœ€å„ªå…ˆèª²é¡Œ1: ä½ã„å¹³å‡é¡ä¼¼åº¦ï¼ˆ0.237ï¼‰**
- **å•é¡Œ**: 89.3%ã®ãƒšã‚¢ãŒé¡ä¼¼åº¦<0.4
- **ç›®æ¨™**: å¹³å‡é¡ä¼¼åº¦ã‚’0.35-0.40ã«æ”¹å–„
- **å½±éŸ¿**: æ¤œå‡ºç²¾åº¦ã®å‘ä¸Š

### ğŸ”´ **æœ€å„ªå…ˆèª²é¡Œ2: ä½ã„ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ç‡ï¼ˆ17.9%ï¼‰**
- **å•é¡Œ**: 28ãƒšã‚¢ä¸­5ãƒšã‚¢ã®ã¿ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´
- **ç›®æ¨™**: ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ç‡ã‚’35-40%ã«æ”¹å–„
- **å½±éŸ¿**: ãƒ•ãƒ¬ãƒ¼ã‚ºãƒ¬ãƒ™ãƒ«ã®æ„å‘³ç†è§£

### ğŸŸ¡ **å„ªå…ˆèª²é¡Œ3: æ™‚é–“çš„ä¸€è²«æ€§ã®é€†è»¢ï¼ˆ0.49xï¼‰**
- **å•é¡Œ**: é¡ä¼¼ãƒšã‚¢ã®æ–¹ãŒæ™‚é–“å·®ãŒå¤§ãã„
- **ç›®æ¨™**: 2.0-3.0xã«æ”¹å–„ï¼ˆæ­£å¸¸åŒ–ï¼‰
- **å½±éŸ¿**: æ™‚é–“çš„ãƒ­ã‚¸ãƒƒã‚¯ã®å¦¥å½“æ€§

---

## ğŸš€ æ”¹å–„ãƒ—ãƒ©ãƒ³ï¼ˆ3é€±é–“ï¼‰

### **Week 1ï¼ˆ11/10-11/16ï¼‰: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–** â­â­â­â­â­

#### 1.1 N-gramæŠ½å‡ºã®èª¿æ•´ï¼ˆå„ªå…ˆåº¦MAXï¼‰

**ç¾çŠ¶**:
```python
# event_comparison.py line 687
vectorizer = TfidfVectorizer(
    ngram_range=(1, 3),
    max_df=0.95,
    min_df=1,  # æ—¢ã«æœ€é©
    max_features=2000,
)
```

**æ”¹å–„æ¡ˆ1: max_featuresæ‹¡å¼µ**
```python
# 2000 â†’ 3000ã«æ‹¡å¼µï¼ˆã‚ˆã‚Šå¤šãã®ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡ºï¼‰
vectorizer = TfidfVectorizer(
    ngram_range=(1, 3),
    max_df=0.95,
    min_df=1,
    max_features=3000,  # â† å¤‰æ›´
)
```

**æœŸå¾…åŠ¹æœ**:
- ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ç‡: 17.9% â†’ 25-30%
- å¹³å‡é¡ä¼¼åº¦: +0.03-0.05

---

**æ”¹å–„æ¡ˆ2: N-gramã®é‡ã¿ä»˜ã‘èª¿æ•´**
```python
# 1-gram vs 2-gram vs 3-gram ã®é‡ã¿ã‚’èª¿æ•´
# ç¾åœ¨: å‡ç­‰
# æ”¹å–„: 2-gram, 3-gramã‚’é‡è¦–ï¼ˆãƒ•ãƒ¬ãƒ¼ã‚ºå„ªå…ˆï¼‰

def extract_ngram_topics_with_weights(comments, top_k=30):
    """
    2-gram, 3-gramã«é«˜ã„ã‚¦ã‚§ã‚¤ãƒˆã‚’ä»˜ä¸
    """
    vectorizer = TfidfVectorizer(
        ngram_range=(1, 3),
        max_df=0.95,
        min_df=1,
        max_features=3000,
    )
    
    X = vectorizer.fit_transform(comments)
    feature_names = vectorizer.get_feature_names_out()
    scores = np.asarray(X.sum(axis=0)).flatten()
    
    # N-gramã®é•·ã•ã«å¿œã˜ã¦é‡ã¿ä»˜ã‘
    weighted_scores = []
    for i, name in enumerate(feature_names):
        ngram_length = len(name.split())
        if ngram_length == 3:
            weight = 2.0  # 3-gramã¯2å€
        elif ngram_length == 2:
            weight = 1.5  # 2-gramã¯1.5å€
        else:
            weight = 1.0  # 1-gramã¯é€šå¸¸
        
        weighted_scores.append(scores[i] * weight)
    
    # é‡ã¿ä»˜ã‘ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    top_indices = np.argsort(weighted_scores)[-top_k:][::-1]
    return [feature_names[i] for i in top_indices]
```

**æœŸå¾…åŠ¹æœ**:
- "éŸ“å›½ç™ºç‹‚"ã®ã‚ˆã†ãªãƒ•ãƒ¬ãƒ¼ã‚ºãŒä¸Šä½ã«
- ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ç‡: +5-8%

---

#### 1.2 é¡ä¼¼åº¦è¨ˆç®—ã®é‡ã¿èª¿æ•´

**ç¾çŠ¶ã®æ¨å®šé‡ã¿**:
```python
# embedding: 0.35
# lexical: 0.20
# topic: 0.35
# temporal: 0.10
```

**å•é¡Œç‚¹**:
- Lexicalå¹³å‡0.129ï¼ˆæœ€ä½ï¼‰ãªã®ã«20%ã®é‡ã¿
- Topicå¹³å‡0.048ï¼ˆä½ã„ï¼‰ã ãŒé‡è¦åº¦é«˜ã„

**æ”¹å–„æ¡ˆ: ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®é‡ã¿æœ€é©åŒ–**
```python
def optimize_weights_grid_search():
    """
    ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§æœ€é©ãªé‡ã¿ã‚’æ¢ç´¢
    """
    df = pd.read_csv('output/event_to_event_pairs.csv')
    
    # Event 56â†”59ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰ã‚’åŸºæº–ã«æœ€é©åŒ–
    target_pair = df[(df['event_A_id'] == 56) & (df['event_B_id'] == 59)].iloc[0]
    
    best_weights = None
    best_score = 0
    
    # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
    for w_emb in [0.3, 0.35, 0.4, 0.45]:
        for w_topic in [0.3, 0.35, 0.4, 0.45]:
            for w_lex in [0.1, 0.15, 0.2]:
                w_temp = 1.0 - w_emb - w_topic - w_lex
                
                if w_temp < 0.05 or w_temp > 0.15:
                    continue
                
                # å†è¨ˆç®—
                df['new_score'] = (
                    w_emb * df['embedding_similarity'] +
                    w_lex * df['lexical_similarity'] +
                    w_topic * df['topic_jaccard'] +
                    w_temp * df['temporal_correlation']
                )
                
                # å®Œå…¨ä¸€è‡´ãƒšã‚¢ã®ã‚¹ã‚³ã‚¢ã‚’æœ€å¤§åŒ–
                target_score = df[(df['event_A_id'] == 56) & (df['event_B_id'] == 59)]['new_score'].iloc[0]
                
                # ä»–ã®ãƒšã‚¢ã¨ã®åˆ†é›¢åº¦ã‚‚è€ƒæ…®
                avg_other = df[(df['event_A_id'] != 56) | (df['event_B_id'] != 59)]['new_score'].mean()
                separation = target_score - avg_other
                
                if separation > best_score:
                    best_score = separation
                    best_weights = (w_emb, w_lex, w_topic, w_temp)
    
    print(f"æœ€é©é‡ã¿: emb={best_weights[0]:.2f}, lex={best_weights[1]:.2f}, "
          f"topic={best_weights[2]:.2f}, temp={best_weights[3]:.2f}")
    print(f"åˆ†é›¢åº¦: {best_score:.3f}")
    
    return best_weights
```

**æœŸå¾…åŠ¹æœ**:
- å¹³å‡é¡ä¼¼åº¦: +0.05-0.08
- å®Œå…¨ä¸€è‡´ãƒšã‚¢ãŒã‚ˆã‚Šéš›ç«‹ã¤

---

#### 1.3 æ™‚é–“çš„ä¸€è²«æ€§ã®ä¿®æ­£

**ç¾çŠ¶ã®å•é¡Œ**:
- Event 56â†”59ã¯76 binsã®æ™‚é–“å·®ï¼ˆå¤–ã‚Œå€¤ï¼‰
- ã“ã‚ŒãŒå¹³å‡ã‚’æ­ªã‚ã¦ã„ã‚‹

**æ”¹å–„æ¡ˆ: ãƒ­ãƒã‚¹ãƒˆãªæ™‚é–“é¡ä¼¼åº¦è¨ˆç®—**
```python
def compute_temporal_similarity_robust(event_A, event_B, max_bins=100):
    """
    å¤–ã‚Œå€¤ã«é ‘å¥ãªæ™‚é–“é¡ä¼¼åº¦
    """
    time_diff = abs(event_A['bin_id'] - event_B['bin_id'])
    
    # æ­£è¦åŒ–ï¼ˆ0-1ã®ç¯„å›²ï¼‰
    # max_binsä»¥ä¸Šã¯0ã«ã‚¯ãƒªãƒƒãƒ—
    if time_diff >= max_bins:
        return 0.0
    
    # æŒ‡æ•°æ¸›è¡°ï¼ˆè¿‘ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
    similarity = np.exp(-time_diff / 20.0)
    
    return similarity

# ã•ã‚‰ã«ã€æ™‚ç³»åˆ—ç›¸é–¢ã‚‚è¨ˆç®—
def compute_temporal_correlation_improved(ts_A, ts_B):
    """
    æ™‚ç³»åˆ—ã®å½¢çŠ¶é¡ä¼¼åº¦ï¼ˆDTWç°¡æ˜“ç‰ˆï¼‰
    """
    from scipy.stats import pearsonr
    
    # é•·ã•ã‚’æƒãˆã‚‹
    min_len = min(len(ts_A), len(ts_B))
    ts_A_trimmed = ts_A[:min_len]
    ts_B_trimmed = ts_B[:min_len]
    
    # Pearsonç›¸é–¢
    if len(ts_A_trimmed) > 1:
        corr, _ = pearsonr(ts_A_trimmed, ts_B_trimmed)
        return max(0, corr)  # è² ã®ç›¸é–¢ã¯0
    
    return 0.0
```

**æœŸå¾…åŠ¹æœ**:
- æ™‚é–“çš„ä¸€è²«æ€§: 0.49x â†’ 2.5-3.0xï¼ˆæ­£å¸¸åŒ–ï¼‰
- ã‚ˆã‚Šè«–ç†çš„ãªè©•ä¾¡

---

### **Week 2ï¼ˆ11/17-11/23ï¼‰: æ–°æ©Ÿèƒ½ã®è¿½åŠ ** â­â­â­â­

#### 2.1 æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®è¿½åŠ 

**ç›®çš„**: ã‚³ãƒ¡ãƒ³ãƒˆã®èˆˆå¥®åº¦ã‚’è€ƒæ…®

**å®Ÿè£…**:
```python
# ç°¡æ˜“æ„Ÿæƒ…åˆ†æ
POSITIVE_WORDS_JA = ["ã™ã”ã„", "æœ€é«˜", "ç¥", "ã‚„ã°ã„", "ã†ã¾ã„", "å‹", "ã‚´ãƒ¼ãƒ«"]
POSITIVE_WORDS_EN = ["goal", "amazing", "great", "wow", "nice", "win", "epic"]
POSITIVE_WORDS_PT = ["gol", "incrÃ­vel", "Ã³timo", "legal", "vitÃ³ria"]

NEGATIVE_WORDS_JA = ["ãƒ€ãƒ¡", "æœ€æ‚ª", "è² ã‘", "ãƒŸã‚¹", "ã‚„ã°ã„", "ã¤ã¾ã‚‰ã‚“"]
NEGATIVE_WORDS_EN = ["miss", "bad", "lose", "terrible", "boring", "awful"]
NEGATIVE_WORDS_PT = ["perda", "ruim", "pÃ©ssimo", "chato"]

def compute_sentiment_score(comments, lang='mixed'):
    """
    ç°¡æ˜“æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ï¼ˆ-1 to 1ï¼‰
    """
    pos_count = 0
    neg_count = 0
    
    for comment in comments:
        comment_lower = comment.lower()
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–èªã®ã‚«ã‚¦ãƒ³ãƒˆ
        for word in POSITIVE_WORDS_JA + POSITIVE_WORDS_EN + POSITIVE_WORDS_PT:
            if word in comment_lower:
                pos_count += 1
        
        # ãƒã‚¬ãƒ†ã‚£ãƒ–èªã®ã‚«ã‚¦ãƒ³ãƒˆ
        for word in NEGATIVE_WORDS_JA + NEGATIVE_WORDS_EN + NEGATIVE_WORDS_PT:
            if word in comment_lower:
                neg_count += 1
    
    total = len(comments)
    if total == 0:
        return 0.0
    
    # æ­£è¦åŒ–
    score = (pos_count - neg_count) / total
    return np.clip(score, -1.0, 1.0)

def compute_sentiment_similarity(event_A, event_B):
    """
    æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®é¡ä¼¼åº¦
    """
    score_A = compute_sentiment_score(event_A['comments'])
    score_B = compute_sentiment_score(event_B['comments'])
    
    # å·®ãŒå°ã•ã„ã»ã©é¡ä¼¼
    diff = abs(score_A - score_B)
    similarity = 1.0 - (diff / 2.0)  # 0-1ã®ç¯„å›²
    
    return similarity
```

**çµ±åˆ**:
```python
# é¡ä¼¼åº¦è¨ˆç®—ã«è¿½åŠ 
combined_score = (
    0.35 * embedding_similarity +
    0.15 * lexical_similarity +
    0.35 * topic_jaccard +
    0.10 * temporal_correlation +
    0.05 * sentiment_similarity  # â† æ–°è¦
)
```

**æœŸå¾…åŠ¹æœ**:
- èˆˆå¥®åº¦ãŒä¼¼ãŸã‚¤ãƒ™ãƒ³ãƒˆã‚’ã‚ˆã‚Šæ­£ç¢ºã«æ¤œå‡º
- å¹³å‡é¡ä¼¼åº¦: +0.02-0.03

---

#### 2.2 é…ä¿¡è€…æ•°ã®æ´»ç”¨

**ç¾çŠ¶**: é…ä¿¡è€…æ•°ã‚’è€ƒæ…®ã—ã¦ã„ãªã„

**æ”¹å–„æ¡ˆ**:
```python
def compute_broadcaster_coverage(event_A, event_B, total_broadcasters=4):
    """
    ä¸¡ã‚¤ãƒ™ãƒ³ãƒˆã§ä½•äººã®é…ä¿¡è€…ãŒåå¿œã—ãŸã‹
    """
    broadcasters_A = set(event_A['broadcasters'])
    broadcasters_B = set(event_B['broadcasters'])
    
    # å’Œé›†åˆï¼ˆå°‘ãªãã¨ã‚‚ç‰‡æ–¹ã§åå¿œï¼‰
    union = broadcasters_A | broadcasters_B
    coverage = len(union) / total_broadcasters
    
    return coverage

def compute_broadcaster_overlap(event_A, event_B):
    """
    å…±é€šé…ä¿¡è€…ã®å‰²åˆ
    """
    broadcasters_A = set(event_A['broadcasters'])
    broadcasters_B = set(event_B['broadcasters'])
    
    intersection = broadcasters_A & broadcasters_B
    union = broadcasters_A | broadcasters_B
    
    if len(union) == 0:
        return 0.0
    
    jaccard = len(intersection) / len(union)
    return jaccard
```

**æ´»ç”¨æ–¹æ³•**:
```python
# coverage ãŒé«˜ã„ãƒšã‚¢ = é‡è¦ãªã‚¤ãƒ™ãƒ³ãƒˆï¼ˆè«–æ–‡ã§å¼·èª¿ï¼‰
# overlap ãŒé«˜ã„ãƒšã‚¢ = åŒã˜ã‚¤ãƒ™ãƒ³ãƒˆã®å¯èƒ½æ€§é«˜ã„ï¼ˆé‡ã¿å¢—åŠ ï¼‰
```

---

### **Week 3ï¼ˆ11/24-11/30ï¼‰: è©•ä¾¡ã¨å¯è¦–åŒ–ã®å¼·åŒ–** â­â­â­â­

#### 3.1 æ”¹å–„å‰å¾Œã®æ¯”è¼ƒ

**å®Ÿè£…**:
```python
def compare_before_after():
    """
    æ”¹å–„å‰å¾Œã®æ€§èƒ½æ¯”è¼ƒ
    """
    # æ”¹å–„å‰ï¼ˆç¾çŠ¶ï¼‰
    df_old = pd.read_csv('output/event_to_event_pairs.csv')
    
    # æ”¹å–„å¾Œï¼ˆæ–°ã—ã„é‡ã¿ãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å†å®Ÿè¡Œï¼‰
    # ... å†å®Ÿè¡Œ ...
    df_new = pd.read_csv('output/event_to_event_pairs_improved.csv')
    
    metrics = ['avg_similarity', 'topic_coverage', 'temporal_consistency']
    
    print("ã€æ”¹å–„å‰å¾Œã®æ¯”è¼ƒã€‘")
    print(f"{'æŒ‡æ¨™':<25} | æ”¹å–„å‰ | æ”¹å–„å¾Œ | æ”¹å–„ç‡")
    print("-" * 60)
    
    # å¹³å‡é¡ä¼¼åº¦
    old_avg = df_old['combined_score'].mean()
    new_avg = df_new['combined_score'].mean()
    improvement = (new_avg - old_avg) / old_avg * 100
    print(f"{'å¹³å‡é¡ä¼¼åº¦':<25} | {old_avg:.3f} | {new_avg:.3f} | +{improvement:.1f}%")
    
    # ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ç‡
    old_coverage = len(df_old[df_old['topic_jaccard'] > 0]) / len(df_old)
    new_coverage = len(df_new[df_new['topic_jaccard'] > 0]) / len(df_new)
    improvement = (new_coverage - old_coverage) / old_coverage * 100
    print(f"{'ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ç‡':<25} | {old_coverage:.1%} | {new_coverage:.1%} | +{improvement:.1f}%")
    
    # å®Œå…¨ä¸€è‡´
    old_perfect = len(df_old[df_old['topic_jaccard'] == 1.0])
    new_perfect = len(df_new[df_new['topic_jaccard'] == 1.0])
    print(f"{'å®Œå…¨ä¸€è‡´':<25} | {old_perfect} | {new_perfect} | +{new_perfect - old_perfect}")
```

---

#### 3.2 è©³ç´°ãªã‚¨ãƒ©ãƒ¼åˆ†æ

**å®Ÿè£…**:
```python
def analyze_failure_cases():
    """
    ä½é¡ä¼¼åº¦ãƒšã‚¢ã®è©³ç´°åˆ†æ
    """
    df = pd.read_csv('output/event_to_event_pairs.csv')
    
    # é¡ä¼¼åº¦<0.3ã®ãƒšã‚¢
    low_sim = df[df['combined_score'] < 0.3]
    
    print(f"ã€ä½é¡ä¼¼åº¦ãƒšã‚¢ã®åˆ†æã€‘")
    print(f"ç·æ•°: {len(low_sim)}ãƒšã‚¢")
    
    for idx, row in low_sim.iterrows():
        print(f"\nEvent {row['event_A_id']} â†” {row['event_B_id']}")
        print(f"  ç·åˆ: {row['combined_score']:.3f}")
        print(f"  embedding: {row['embedding_similarity']:.3f}")
        print(f"  topic: {row['topic_jaccard']:.3f}")
        print(f"  æ™‚é–“å·®: {row['time_diff_bins']} bins")
        
        # å¤±æ•—åŸå› ã®æ¨æ¸¬
        reasons = []
        if row['embedding_similarity'] < 0.4:
            reasons.append("åŸ‹ã‚è¾¼ã¿ãŒä½ã„ï¼ˆç•°ãªã‚‹å†…å®¹ï¼‰")
        if row['topic_jaccard'] == 0:
            reasons.append("ãƒˆãƒ”ãƒƒã‚¯ä¸ä¸€è‡´ï¼ˆN-gramæŠ½å‡ºå¤±æ•—ï¼Ÿï¼‰")
        if row['time_diff_bins'] > 50:
            reasons.append("æ™‚é–“å·®ãŒå¤§ãã„")
        
        print(f"  æ¨å®šåŸå› : {', '.join(reasons)}")
```

---

## ğŸ“… å®Ÿè¡Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ11æœˆï¼‰

| é€± | ã‚¿ã‚¹ã‚¯ | æ‰€è¦æ™‚é–“ | æˆæœç‰© |
|----|--------|---------|--------|
| **Week 1** | N-gramæœ€é©åŒ– | 2-3æ™‚é–“ | max_features=3000ç‰ˆ |
| Week 1 | é‡ã¿æœ€é©åŒ– | 2æ™‚é–“ | æœ€é©é‡ã¿ã®æ±ºå®š |
| Week 1 | æ™‚é–“é¡ä¼¼åº¦ä¿®æ­£ | 1æ™‚é–“ | ãƒ­ãƒã‚¹ãƒˆç‰ˆå®Ÿè£… |
| **Week 2** | æ„Ÿæƒ…åˆ†æè¿½åŠ  | 2æ™‚é–“ | sentiment_score |
| Week 2 | é…ä¿¡è€…æ•°æ´»ç”¨ | 1æ™‚é–“ | broadcaster_overlap |
| Week 2 | å†å®Ÿè¡Œ | 30åˆ† | æ”¹å–„ç‰ˆçµæœ |
| **Week 3** | æ”¹å–„å‰å¾Œæ¯”è¼ƒ | 1æ™‚é–“ | æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ |
| Week 3 | ã‚¨ãƒ©ãƒ¼åˆ†æ | 1æ™‚é–“ | å¤±æ•—ã‚±ãƒ¼ã‚¹åˆ†æ |
| Week 3 | è¿½åŠ å¯è¦–åŒ– | 2æ™‚é–“ | æ”¹å–„å›³è¡¨ |

**åˆè¨ˆ**: ç´„15æ™‚é–“ï¼ˆé€±5æ™‚é–“ Ã— 3é€±é–“ï¼‰

---

## ğŸ¯ 11æœˆæœ«ã®ç›®æ¨™å€¤

| æŒ‡æ¨™ | ç¾çŠ¶ | ç›®æ¨™ | é”æˆæ¡ä»¶ |
|------|------|------|---------|
| å¹³å‡é¡ä¼¼åº¦ | 0.237 | **0.35** | Week 1å®Œäº† |
| ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ç‡ | 17.9% | **35%** | Week 1å®Œäº† |
| å®Œå…¨ä¸€è‡´ | 1ä»¶ | **2-3ä»¶** | Week 1-2å®Œäº† |
| æ™‚é–“çš„ä¸€è²«æ€§ | 0.49x | **2.5x** | Week 1å®Œäº† |
| é«˜å“è³ªãƒšã‚¢ | 1ä»¶ | **3-5ä»¶** | Week 2å®Œäº† |

---

## âœ… ä»Šé€±ï¼ˆWeek 1ï¼‰ã®å…·ä½“çš„ã‚¿ã‚¹ã‚¯

### **Task 1: max_featuresæ‹¡å¼µï¼ˆ30åˆ†ï¼‰** â­â­â­â­â­

**å®Ÿè¡Œå†…å®¹**:
1. `event_comparison.py` line 687ã‚’ç·¨é›†
2. `max_features=2000` â†’ `max_features=3000`
3. å†å®Ÿè¡Œã—ã¦çµæœã‚’æ¯”è¼ƒ

### **Task 2: é‡ã¿æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆï¼ˆ1æ™‚é–“ï¼‰**

**å®Ÿè¡Œå†…å®¹**:
1. `optimize_weights.py` ã‚’ä½œæˆ
2. ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå®Ÿè¡Œ
3. æœ€é©é‡ã¿ã‚’æ±ºå®š

### **Task 3: æ™‚é–“é¡ä¼¼åº¦ã®ãƒ­ãƒã‚¹ãƒˆåŒ–ï¼ˆ1æ™‚é–“ï¼‰**

**å®Ÿè¡Œå†…å®¹**:
1. `compute_temporal_similarity_robust()` å®Ÿè£…
2. `event_comparison.py` ã«çµ±åˆ
3. å†å®Ÿè¡Œ

---

## ğŸ’¡ é‡è¦ãƒã‚¤ãƒ³ãƒˆ

1. **æ®µéšçš„æ”¹å–„**: ä¸€åº¦ã«ã™ã¹ã¦å¤‰æ›´ã›ãšã€1ã¤ãšã¤åŠ¹æœã‚’ç¢ºèª
2. **æ¯”è¼ƒã®å¾¹åº•**: æ”¹å–„å‰å¾Œã‚’å¿…ãšæ¯”è¼ƒã—ã¦æ•°å€¤åŒ–
3. **å¤±æ•—ã®åˆ†æ**: ä½•ãŒã†ã¾ãã„ã‹ãªã„ã‹ã‚’ç†è§£ã™ã‚‹
4. **12æœˆã«å‚™ãˆã‚‹**: è«–æ–‡åŸ·ç­†æ™‚ã«èª¬å¾—åŠ›ã®ã‚ã‚‹çµæœã‚’ç”¨æ„

---

**ã¾ãšã¯ä»Šé€±ã®Task 1ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ï¼max_featuresæ‹¡å¼µã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ**
