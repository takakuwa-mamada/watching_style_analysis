"""
çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: ã™ã¹ã¦ã®åˆ†æçµæœã‚’ã¾ã¨ã‚ã¦è«–æ–‡ç”¨ã®ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import dendrogram, linkage
import os

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
plt.rcParams['axes.unicode_minus'] = False

def load_all_results():
    """ã™ã¹ã¦ã®åˆ†æçµæœã‚’èª­ã¿è¾¼ã‚€"""
    results = {}
    
    # æ„Ÿæƒ…è¡¨ç¾åˆ†æ
    try:
        results['emotional'] = pd.read_csv('output/emotional_analysis/emotional_expression_results.csv')
        print("âœ… Loaded emotional expression results")
    except:
        print("âš ï¸ Emotional expression results not found")
    
    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ†æ
    try:
        results['engagement'] = pd.read_csv('output/engagement_analysis/engagement_results.csv')
        print("âœ… Loaded engagement results")
    except:
        print("âš ï¸ Engagement results not found")
    
    # æ–‡åŒ–çš„é¡ä¼¼åº¦åˆ†æ
    try:
        results['cultural_sim'] = pd.read_csv('output/cultural_similarity_analysis/cultural_similarity_results.csv')
        print("âœ… Loaded cultural similarity results")
    except:
        print("âš ï¸ Cultural similarity results not found")
    
    return results

def create_integrated_cultural_profile(results):
    """å„å›½ã®ç·åˆçš„ãªæ–‡åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
    
    # å›½åˆ¥ã®ç‰¹å¾´é‡ã‚’çµ±åˆ
    country_profiles = {}
    
    if 'emotional' in results:
        emotional_by_country = results['emotional'].groupby('country').agg({
            'emoji_rate': 'mean',
            'laugh_rate': 'mean',
            'exclamation_rate': 'mean',
            'mean_length': 'mean'
        })
        country_profiles['emotional'] = emotional_by_country
    
    if 'engagement' in results:
        engagement_by_country = results['engagement'].groupby('country').agg({
            'mean_cpm': 'mean',
            'burst_freq_per_hour': 'mean',
            'mean_burst_duration': 'mean',
            'mean_burst_intensity': 'mean'
        })
        country_profiles['engagement'] = engagement_by_country
    
    # çµ±åˆDataFrameã‚’ä½œæˆ
    if country_profiles:
        integrated_df = pd.concat(country_profiles.values(), axis=1)
        return integrated_df
    else:
        return None

def calculate_cultural_distance_matrix(integrated_profile):
    """æ–‡åŒ–çš„è·é›¢ãƒãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
    if integrated_profile is None or len(integrated_profile) < 2:
        return None, None
    
    # æ­£è¦åŒ–
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    features_normalized = scaler.fit_transform(integrated_profile.fillna(0))
    
    # è·é›¢è¡Œåˆ—è¨ˆç®—
    distance_matrix = squareform(pdist(features_normalized, metric='euclidean'))
    
    # éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    linkage_matrix = linkage(distance_matrix, method='ward')
    
    return distance_matrix, linkage_matrix

def create_comprehensive_report(results):
    """åŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    
    print("="*80)
    print("çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    print("="*80)
    
    output_dir = 'output/comprehensive_report'
    os.makedirs(output_dir, exist_ok=True)
    
    # çµ±åˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    integrated_profile = create_integrated_cultural_profile(results)
    
    if integrated_profile is not None:
        print("\nğŸ“Š çµ±åˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«:")
        print(integrated_profile.round(2))
        
        integrated_profile.to_csv(f'{output_dir}/integrated_cultural_profile.csv', encoding='utf-8-sig')
        print(f"\nâœ… Saved: integrated_cultural_profile.csv")
    
    # æ–‡åŒ–çš„è·é›¢ãƒãƒˆãƒªã‚¯ã‚¹
    distance_matrix, linkage_matrix = calculate_cultural_distance_matrix(integrated_profile)
    
    if distance_matrix is not None:
        print("\nğŸ“ æ–‡åŒ–çš„è·é›¢ãƒãƒˆãƒªã‚¯ã‚¹:")
        distance_df = pd.DataFrame(distance_matrix, 
                                   index=integrated_profile.index, 
                                   columns=integrated_profile.index)
        print(distance_df.round(2))
        
        distance_df.to_csv(f'{output_dir}/cultural_distance_matrix.csv', encoding='utf-8-sig')
        print(f"\nâœ… Saved: cultural_distance_matrix.csv")
    
    # å¯è¦–åŒ–
    create_comprehensive_visualizations(results, integrated_profile, distance_matrix, linkage_matrix, output_dir)
    
    # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_summary_report(results, integrated_profile, distance_df, output_dir)
    
    print(f"\nâœ… çµ±åˆãƒ¬ãƒãƒ¼ãƒˆå®Œäº†ï¼")
    print(f"ğŸ“ çµæœã¯ {output_dir}/ ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

def create_comprehensive_visualizations(results, integrated_profile, distance_matrix, linkage_matrix, output_dir):
    """åŒ…æ‹¬çš„ãªå¯è¦–åŒ–ã‚’ä½œæˆ"""
    
    print("\nğŸ¨ åŒ…æ‹¬çš„ãªå¯è¦–åŒ–ä½œæˆä¸­...")
    
    # Figure 1: çµ±åˆãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆ5è»¸ï¼‰
    if integrated_profile is not None and len(integrated_profile) >= 3:
        fig, axes = plt.subplots(2, 3, figsize=(18, 12), subplot_kw=dict(projection='polar'))
        axes = axes.flatten()
        
        # é¸æŠã™ã‚‹æŒ‡æ¨™
        key_metrics = ['emoji_rate', 'laugh_rate', 'mean_cpm', 'burst_freq_per_hour', 'mean_burst_intensity']
        available_metrics = [m for m in key_metrics if m in integrated_profile.columns]
        
        if len(available_metrics) >= 3:
            for idx, country in enumerate(integrated_profile.index[:6]):  # æœ€å¤§6å›½
                if idx >= len(axes):
                    break
                
                ax = axes[idx]
                
                # ãƒ‡ãƒ¼ã‚¿æº–å‚™
                values = []
                for metric in available_metrics:
                    val = integrated_profile.loc[country, metric]
                    # æ­£è¦åŒ–ï¼ˆ0-1ï¼‰
                    col_min = integrated_profile[metric].min()
                    col_max = integrated_profile[metric].max()
                    if col_max > col_min:
                        normalized = (val - col_min) / (col_max - col_min)
                    else:
                        normalized = 0.5
                    values.append(normalized)
                
                # å††ã‚’é–‰ã˜ã‚‹
                values += values[:1]
                
                # è§’åº¦è¨­å®š
                angles = np.linspace(0, 2 * np.pi, len(available_metrics), endpoint=False).tolist()
                angles += angles[:1]
                
                # ãƒ—ãƒ­ãƒƒãƒˆ
                ax.plot(angles, values, 'o-', linewidth=2, label=country)
                ax.fill(angles, values, alpha=0.25)
                ax.set_xticks(angles[:-1])
                ax.set_xticklabels([m.replace('_', ' ').title()[:15] for m in available_metrics], fontsize=8)
                ax.set_ylim(0, 1)
                ax.set_title(f'{country}', fontsize=12, fontweight='bold', pad=20)
                ax.grid(True)
            
            # æœªä½¿ç”¨ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
            for idx in range(len(integrated_profile.index[:6]), len(axes)):
                axes[idx].axis('off')
            
            plt.suptitle('Cultural Watching Style Profiles (Normalized)', fontsize=16, fontweight='bold', y=1.0)
            plt.tight_layout()
            plt.savefig(f'{output_dir}/cultural_profiles_radar.png', dpi=300, bbox_inches='tight')
            print(f"âœ… Saved: cultural_profiles_radar.png")
            plt.close()
    
    # Figure 2: æ–‡åŒ–çš„è·é›¢ãƒãƒˆãƒªã‚¯ã‚¹ + ãƒ‡ãƒ³ãƒ‰ãƒ­ã‚°ãƒ©ãƒ 
    if distance_matrix is not None and linkage_matrix is not None:
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # Left: Heatmap
        ax1 = axes[0]
        sns.heatmap(distance_matrix, 
                   xticklabels=integrated_profile.index,
                   yticklabels=integrated_profile.index,
                   annot=True, fmt='.2f', cmap='YlOrRd', ax=ax1,
                   cbar_kws={'label': 'Euclidean Distance'})
        ax1.set_title('Cultural Distance Matrix', fontsize=14, fontweight='bold')
        
        # Right: Dendrogram
        ax2 = axes[1]
        dendrogram(linkage_matrix, labels=integrated_profile.index.tolist(), ax=ax2)
        ax2.set_title('Hierarchical Clustering', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Distance', fontsize=12)
        ax2.set_xlabel('Country', fontsize=12)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/cultural_distance_analysis.png', dpi=300, bbox_inches='tight')
        print(f"âœ… Saved: cultural_distance_analysis.png")
        plt.close()
    
    # Figure 3: çµ±åˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆã™ã¹ã¦ã®æŒ‡æ¨™ï¼‰
    if integrated_profile is not None:
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # æ­£è¦åŒ–
        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()
        normalized_data = pd.DataFrame(
            scaler.fit_transform(integrated_profile.fillna(0)),
            index=integrated_profile.index,
            columns=integrated_profile.columns
        )
        
        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        sns.heatmap(normalized_data.T, annot=True, fmt='.2f', cmap='RdYlGn', ax=ax,
                   cbar_kws={'label': 'Normalized Score (0-1)'})
        ax.set_title('Comprehensive Cultural Profile\n(All Metrics Normalized)', 
                    fontsize=14, fontweight='bold')
        ax.set_xlabel('Country', fontsize=12)
        ax.set_ylabel('Metric', fontsize=12)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/comprehensive_profile_heatmap.png', dpi=300, bbox_inches='tight')
        print(f"âœ… Saved: comprehensive_profile_heatmap.png")
        plt.close()

def generate_summary_report(results, integrated_profile, distance_df, output_dir):
    """è«–æ–‡ç”¨ã®ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    
    report_lines = []
    report_lines.append("="*80)
    report_lines.append("åŒ…æ‹¬çš„åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ")
    report_lines.append("="*80)
    report_lines.append("")
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³1: æ„Ÿæƒ…è¡¨ç¾åˆ†æ
    if 'emotional' in results:
        report_lines.append("## 1. æ„Ÿæƒ…è¡¨ç¾ã®æ–‡åŒ–å·®")
        report_lines.append("")
        
        df_emo = results['emotional']
        country_emo = df_emo.groupby('country').agg({
            'emoji_rate': 'mean',
            'laugh_rate': 'mean',
            'exclamation_rate': 'mean'
        })
        
        # æœ€å¤§ãƒ»æœ€å°
        max_emoji_country = country_emo['emoji_rate'].idxmax()
        max_emoji_val = country_emo['emoji_rate'].max()
        min_emoji_country = country_emo['emoji_rate'].idxmin()
        min_emoji_val = country_emo['emoji_rate'].min()
        
        report_lines.append(f"### Emojiä½¿ç”¨ç‡:")
        report_lines.append(f"- æœ€é«˜: {max_emoji_country} ({max_emoji_val:.3f} emoji/comment)")
        report_lines.append(f"- æœ€ä½: {min_emoji_country} ({min_emoji_val:.3f} emoji/comment)")
        report_lines.append(f"- å€ç‡: {max_emoji_val/min_emoji_val:.1f}Ã—")
        report_lines.append("")
        
        max_laugh_country = country_emo['laugh_rate'].idxmax()
        max_laugh_val = country_emo['laugh_rate'].max()
        
        report_lines.append(f"### ç¬‘ã„è¡¨ç¾ç‡:")
        report_lines.append(f"- æœ€é«˜: {max_laugh_country} ({max_laugh_val:.3f})")
        report_lines.append("")
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³2: ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³
    if 'engagement' in results:
        report_lines.append("## 2. ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³")
        report_lines.append("")
        
        df_eng = results['engagement']
        country_eng = df_eng.groupby('country').agg({
            'mean_cpm': 'mean',
            'burst_freq_per_hour': 'mean',
            'mean_burst_duration': 'mean'
        })
        
        max_cpm_country = country_eng['mean_cpm'].idxmax()
        max_cpm_val = country_eng['mean_cpm'].max()
        
        report_lines.append(f"### ã‚³ãƒ¡ãƒ³ãƒˆå¯†åº¦ (CPM):")
        report_lines.append(f"- æœ€é«˜: {max_cpm_country} ({max_cpm_val:.1f} comments/minute)")
        report_lines.append("")
        
        max_burst_country = country_eng['burst_freq_per_hour'].idxmax()
        max_burst_val = country_eng['burst_freq_per_hour'].max()
        
        report_lines.append(f"### Bursté »åº¦:")
        report_lines.append(f"- æœ€é«˜: {max_burst_country} ({max_burst_val:.1f} bursts/hour)")
        report_lines.append("")
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³3: æ–‡åŒ–çš„è·é›¢
    if distance_df is not None:
        report_lines.append("## 3. æ–‡åŒ–çš„è·é›¢åˆ†æ")
        report_lines.append("")
        
        # æœ€ã‚‚è¿‘ã„ãƒšã‚¢
        mask = np.triu(np.ones_like(distance_df, dtype=bool), k=1)
        distance_arr = distance_df.where(mask).stack().sort_values()
        
        if len(distance_arr) > 0:
            closest_pair = distance_arr.index[0]
            closest_dist = distance_arr.iloc[0]
            
            furthest_pair = distance_arr.index[-1]
            furthest_dist = distance_arr.iloc[-1]
            
            report_lines.append(f"### æœ€ã‚‚é¡ä¼¼ã—ãŸæ–‡åŒ–ãƒšã‚¢:")
            report_lines.append(f"- {closest_pair[0]} â†” {closest_pair[1]}: è·é›¢ {closest_dist:.2f}")
            report_lines.append("")
            
            report_lines.append(f"### æœ€ã‚‚ç•°ãªã‚‹æ–‡åŒ–ãƒšã‚¢:")
            report_lines.append(f"- {furthest_pair[0]} â†” {furthest_pair[1]}: è·é›¢ {furthest_dist:.2f}")
            report_lines.append("")
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³4: è«–æ–‡ç”¨ã®ä¸»è¦çŸ¥è¦‹
    report_lines.append("## 4. è«–æ–‡ç”¨ã®ä¸»è¦çŸ¥è¦‹ï¼ˆKey Findingsï¼‰")
    report_lines.append("")
    report_lines.append("### For Abstract:")
    report_lines.append("")
    
    if 'emotional' in results and 'engagement' in results:
        df_emo = results['emotional']
        df_eng = results['engagement']
        
        # ã‚µãƒ³ãƒ—ãƒ«çŸ¥è¦‹
        report_lines.append("\"We quantitatively characterize sports watching styles across cultures,")
        report_lines.append(f" revealing {max_emoji_country} viewers' emoji-rich engagement")
        report_lines.append(f" ({max_emoji_val:.2f} emoji/comment) contrasts with {min_emoji_country}'s")
        report_lines.append(f" restrained expression ({min_emoji_val:.2f} emoji/comment, {max_emoji_val/min_emoji_val:.1f}Ã— difference).\"")
        report_lines.append("")
    
    report_lines.append("### For Discussion:")
    report_lines.append("")
    report_lines.append("- Cultural communication theories (Hofstede, Hall) validated with quantitative data")
    report_lines.append("- Collectivism vs Individualism reflected in engagement patterns")
    report_lines.append("- High-context vs Low-context cultures in emotional expression")
    report_lines.append("")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(f'{output_dir}/COMPREHENSIVE_SUMMARY_REPORT.md', 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print(f"âœ… Saved: COMPREHENSIVE_SUMMARY_REPORT.md")
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚å‡ºåŠ›
    print("\n" + "\n".join(report_lines))

def main():
    print("="*80)
    print("çµ±åˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    print("="*80)
    
    # ã™ã¹ã¦ã®çµæœã‚’èª­ã¿è¾¼ã¿
    results = load_all_results()
    
    if not results:
        print("\nâŒ åˆ†æçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("  1. analyze_emotional_expression.py")
        print("  2. analyze_engagement_patterns.py")
        print("  3. analyze_cultural_similarity.py")
        return
    
    # åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    create_comprehensive_report(results)

if __name__ == "__main__":
    main()
